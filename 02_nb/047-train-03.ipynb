{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"047-train-03.ipynb","provenance":[{"file_id":"17V-_oEoc7G-20Mk78qNMJWU1Of9PfFTH","timestamp":1626822715425},{"file_id":"17a4F4aC9L0QBqU8BRTrdqPn0WwJ0b08b","timestamp":1626746992716},{"file_id":"1G_W9irFTrEmDeHR0S6_u0bjpk8nxipXW","timestamp":1626689695352},{"file_id":"1bhhkorT--y8XXaVLM8hibVgC-tLqZ16P","timestamp":1626358153868},{"file_id":"1WtT2hX6O9Qbt_hb9sF50nM2QmDXFi-XA","timestamp":1626338366006},{"file_id":"1k_p5wftcUeo711Xho1-T5an2Xkneau-J","timestamp":1626323813472},{"file_id":"1Vz2GB2BNTWuefEFkCSh3TBPEIel7KG1t","timestamp":1626317426487},{"file_id":"1djoMWojeaIPopG5tS1jNMohn8ineblRh","timestamp":1626306831897},{"file_id":"1-6tlDO8158Pi6TpptIF884oFaEiT4Uxb","timestamp":1626276420047},{"file_id":"1js8eA3mDNS8mwSpCiHuzPeARFlUPAVrg","timestamp":1626272452526},{"file_id":"1yhcPgulwJtjJKUK9IuRKmNMhJ-4YXGol","timestamp":1626267205517},{"file_id":"1mnnSv0Pofn1QxArywV81VYqnZPB8uUWN","timestamp":1626180468522},{"file_id":"1RRdjt_UAeHmr5QQBAMyC82Fq1s31OWdK","timestamp":1625833136005},{"file_id":"1JPgg44HFemzwk8VSCXih3PejL0idy-C4","timestamp":1625825483466},{"file_id":"1Ye6wqVX71xAAAhmjXkw9IpRvTqeUyJDA","timestamp":1625812137500}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":154},"id":"Z6yRwt-PXtbP","executionInfo":{"status":"ok","timestamp":1626828856346,"user_tz":-540,"elapsed":269,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"d91141e9-30fb-41be-f145-639a7c2b6826"},"source":["\"\"\"\n","if 'google.colab' in sys.modules:  # colab環境特有の処理_初回のみ\n","  # Google Driveのマウント\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules' \\\n","   -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/requirements.txt' \\\n","   --ignore-installed\n","\n","  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules' \\\n","   transformers -U\n","  !pip install gensim==4.0.1 --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\n","  !pip install pytorch_memlab --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\n","\"\"\""],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nif 'google.colab' in sys.modules:  # colab環境特有の処理_初回のみ\\n  # Google Driveのマウント\\n  from google.colab import drive\\n  drive.mount('/content/drive')\\n\\n  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'    -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/requirements.txt'    --ignore-installed\\n\\n  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'    transformers -U\\n  !pip install gensim==4.0.1 --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\\n  !pip install pytorch_memlab --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\\n\""]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucCbvGD1XvG7","executionInfo":{"status":"ok","timestamp":1626829165331,"user_tz":-540,"elapsed":308641,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"d266bdd4-6121-4831-cbb2-d9290b486f26"},"source":["import sys\n","if 'google.colab' in sys.modules:  # colab特有の処理_2回目以降\n","  # Google Driveのマウント\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  # データセットをDriveから取得\n","  !mkdir -p 'input'\n","  !cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input/commonlitreadabilityprize/' '/content/input'\n","  !cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch2' '/content/clrp-roberta-large'\n","  # ライブラリのパス指定\n","  sys.path.append('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules')\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"S-WYDSfvMboK","executionInfo":{"status":"ok","timestamp":1626829165333,"user_tz":-540,"elapsed":29,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"a64b2621-b189-4aef-9768-76aa797d71b1"},"source":["\"\"\"!pip install --upgrade --force-reinstall --no-deps kaggle\"\"\""],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!pip install --upgrade --force-reinstall --no-deps kaggle'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"hcMyFJY0Mbkd","executionInfo":{"status":"ok","timestamp":1626829165334,"user_tz":-540,"elapsed":22,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"N6kqvVoPrnpj","executionInfo":{"status":"ok","timestamp":1626829165334,"user_tz":-540,"elapsed":20,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"d111fd15-32fc-4320-8e91-9011e94b686c"},"source":["\n","\"\"\"%cd\n","!mkdir .kaggle\n","!mkdir /content/model\n","!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\n","\"\"\""],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'%cd\\n!mkdir .kaggle\\n!mkdir /content/model\\n!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\\n'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"Axihy1acGjie","executionInfo":{"status":"ok","timestamp":1626829165335,"user_tz":-540,"elapsed":19,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"841abdb3-c9c2-4f5a-f1f7-9a9172693190"},"source":["\"\"\"!mkdir /content/pre-trained-roberta\n","!kaggle datasets download -p /content/pre-trained-roberta -d calclrprobertalargeep2 --unzip\n","!cp -r /content/pre-trained-roberta/ /content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch2\"\"\""],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!mkdir /content/pre-trained-roberta\\n!kaggle datasets download -p /content/pre-trained-roberta -d calclrprobertalargeep2 --unzip\\n!cp -r /content/pre-trained-roberta/ /content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch2'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"RV9-VwbpZLZ9","executionInfo":{"status":"ok","timestamp":1626829165336,"user_tz":-540,"elapsed":19,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["from pathlib import Path\n","\n","# input\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    DATA_DIR = Path('../input/commonlitreadabilityprize/')\n","\n","elif 'google.colab' in sys.modules: # Colab環境\n","    DATA_DIR = Path('/content/input/commonlitreadabilityprize')\n","\n","else:\n","    DATA_DIR = Path('../00_input/commonlitreadabilityprize/')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5difyXe00UV","executionInfo":{"status":"ok","timestamp":1626829165682,"user_tz":-540,"elapsed":364,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["from pathlib import Path\n","\n","# tokenizer\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    TOKENIZER_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    !mkdir /content/pre-trained-roberta\n","    #!cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch2' '/content/clrp-roberta-large'\n","    TOKENIZER_DIR = '/content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    TOKENIZER_DIR = 'roberta-large'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKjsUxnOeDYl","executionInfo":{"status":"ok","timestamp":1626829165683,"user_tz":-540,"elapsed":5,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["from pathlib import Path\n","\n","# pre-trained model\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    PRE_TRAINED_MODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    PRE_TRAINED_MODEL_DIR = '/content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    PRE_TRAINED_MODEL_DIR = 'roberta-large'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLaT2V0ReoAZ","executionInfo":{"status":"ok","timestamp":1626829165683,"user_tz":-540,"elapsed":5,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["UPLOAD_DIR = Path('/content/model')\n","EX_NO = '047-train-03'  # 実験番号などを入れる、folderのpathにする\n","USERID = 'calpis10000'"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOGjAb4pAJ0F","executionInfo":{"status":"ok","timestamp":1626829165684,"user_tz":-540,"elapsed":5,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["import subprocess\n","import shlex\n","\n","def gpuinfo():\n","    \"\"\"\n","    Returns size of total GPU RAM and used GPU RAM.\n","\n","    Parameters\n","    ----------\n","    None\n","\n","    Returns\n","    -------\n","    info : dict\n","        Total GPU RAM in integer for key 'total_MiB'.\n","        Used GPU RAM in integer for key 'used_MiB'.\n","    \"\"\"\n","\n","    command = 'nvidia-smi -q -d MEMORY | sed -n \"/FB Memory Usage/,/Free/p\" | sed -e \"1d\" -e \"4d\" -e \"s/ MiB//g\" | cut -d \":\" -f 2 | cut -c2-'\n","    commands = [shlex.split(part) for part in command.split(' | ')]\n","    for i, cmd in enumerate(commands):\n","        if i==0:\n","            res = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n","        else:\n","            res = subprocess.Popen(cmd, stdin=res.stdout, stdout=subprocess.PIPE)\n","    total, used = map(int, res.communicate()[0].decode('utf-8').strip().split('\\n'))\n","    info = {'total_MiB':total, 'used_MiB':used}\n","    return info\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g3-6m5MKXecB"},"source":["# Overview\n","This nb is based on copy from https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch .\n","\n","Acknowledgments(from base nb): \n","some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T06:26:32.834365Z","iopub.execute_input":"2021-07-04T06:26:32.834903Z","iopub.status.idle":"2021-07-04T06:26:40.143740Z","shell.execute_reply.started":"2021-07-04T06:26:32.834785Z","shell.execute_reply":"2021-07-04T06:26:40.142864Z"},"trusted":true,"id":"HRsRZ06WXecD","executionInfo":{"status":"ok","timestamp":1626829325327,"user_tz":-540,"elapsed":159648,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["import os\n","import math\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from transformers import AdamW # optimizer\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoConfig\n","from transformers import get_cosine_schedule_with_warmup # scheduler\n","#from pytorch_memlab import profile\n","#import pytorch_memlab\n","#from pytorch_memlab import MemReporter\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gc\n","gc.enable()"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.145217Z","iopub.execute_input":"2021-07-04T06:26:40.145539Z","iopub.status.idle":"2021-07-04T06:26:40.201326Z","shell.execute_reply.started":"2021-07-04T06:26:40.145504Z","shell.execute_reply":"2021-07-04T06:26:40.200136Z"},"trusted":true,"id":"omBfwshTXecE","executionInfo":{"status":"ok","timestamp":1626829325328,"user_tz":-540,"elapsed":9,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["NUM_FOLDS = 5 # K Fold\n","NUM_EPOCHS = 5 # Epochs\n","BATCH_SIZE = 12 # Batch Size\n","MAX_LEN = 248 # ベクトル長\n","EVAL_SCHEDULE = [(0.55, 64), (0.50, 32), (0.49, 16), (0.48, 8), (0.47, 4), (0.46, 2), (-1., 1)] # schedulerの何らかの設定？\n","ROBERTA_PATH = PRE_TRAINED_MODEL_DIR # roberta pre-trainedモデル(モデルとして指定)\n","TOKENIZER_PATH = TOKENIZER_DIR # roberta pre-trainedモデル(Tokenizerとして指定)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # cudaがなければcpuを使えばいいじゃない"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.203398Z","iopub.execute_input":"2021-07-04T06:26:40.204055Z","iopub.status.idle":"2021-07-04T06:26:40.211572Z","shell.execute_reply.started":"2021-07-04T06:26:40.204015Z","shell.execute_reply":"2021-07-04T06:26:40.210762Z"},"trusted":true,"id":"4qcuXqwtXecF","executionInfo":{"status":"ok","timestamp":1626829325329,"user_tz":-540,"elapsed":8,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["def set_random_seed(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n","\n","    torch.manual_seed(random_seed)\n","    torch.cuda.manual_seed(random_seed)\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","    torch.backends.cudnn.deterministic = True# cudnnによる最適化で結果が変わらないためのおまじない "],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.214188Z","iopub.execute_input":"2021-07-04T06:26:40.214809Z","iopub.status.idle":"2021-07-04T06:26:40.309744Z","shell.execute_reply.started":"2021-07-04T06:26:40.214769Z","shell.execute_reply":"2021-07-04T06:26:40.308926Z"},"trusted":true,"id":"70PyLsJTXecF","executionInfo":{"status":"ok","timestamp":1626829325680,"user_tz":-540,"elapsed":358,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# train, testを読む\n","train_df = pd.read_csv(DATA_DIR/\"train.csv\")\n","\n","# Remove incomplete entries if any.\n","train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n","              inplace=True)\n","train_df.reset_index(drop=True, inplace=True)\n","\n","test_df = pd.read_csv(DATA_DIR/\"test.csv\")\n","submission_df = pd.read_csv(DATA_DIR/\"sample_submission.csv\")"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"9ZYOB59L8qtA","executionInfo":{"status":"ok","timestamp":1626829325681,"user_tz":-540,"elapsed":9,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"85510000-7295-489a-9594-43d851f54903"},"source":["train_df.head()\n"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>url_legal</th>\n","      <th>license</th>\n","      <th>excerpt</th>\n","      <th>target</th>\n","      <th>standard_error</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>c12129c31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>When the young people returned to the ballroom...</td>\n","      <td>-0.340259</td>\n","      <td>0.464009</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>85aa80a4c</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n","      <td>-0.315372</td>\n","      <td>0.480805</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>b69ac6792</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>As Roger had predicted, the snow departed as q...</td>\n","      <td>-0.580118</td>\n","      <td>0.476676</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>dd1000b26</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>And outside before the palace a great garden w...</td>\n","      <td>-1.054013</td>\n","      <td>0.450007</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>37c1b32fb</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Once upon a time there were Three Bears who li...</td>\n","      <td>0.247197</td>\n","      <td>0.510845</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id url_legal  ...    target standard_error\n","0  c12129c31       NaN  ... -0.340259       0.464009\n","1  85aa80a4c       NaN  ... -0.315372       0.480805\n","2  b69ac6792       NaN  ... -0.580118       0.476676\n","3  dd1000b26       NaN  ... -1.054013       0.450007\n","4  37c1b32fb       NaN  ...  0.247197       0.510845\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.311021Z","iopub.execute_input":"2021-07-04T06:26:40.311347Z","iopub.status.idle":"2021-07-04T06:26:40.624393Z","shell.execute_reply.started":"2021-07-04T06:26:40.311314Z","shell.execute_reply":"2021-07-04T06:26:40.623347Z"},"trusted":true,"id":"xf0662k4XecF","executionInfo":{"status":"ok","timestamp":1626829326035,"user_tz":-540,"elapsed":361,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# tokenizerを指定\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6aaghNkXecG"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.628883Z","iopub.execute_input":"2021-07-04T06:26:40.629347Z","iopub.status.idle":"2021-07-04T06:26:40.644338Z","shell.execute_reply.started":"2021-07-04T06:26:40.629309Z","shell.execute_reply":"2021-07-04T06:26:40.643336Z"},"trusted":true,"id":"zkopT0U1XecG","executionInfo":{"status":"ok","timestamp":1626829326035,"user_tz":-540,"elapsed":7,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# Dataset用のClass。おそらく、trainとtestでインスタンスを生成し、DataFrameと同じように扱えるような思想。\n","class LitDataset(Dataset):\n","    def __init__(self, df, inference_only=False):\n","        super().__init__()\n","\n","        self.df = df        \n","        self.inference_only = inference_only # Testデータ用フラグ\n","        self.text = df.excerpt.tolist() # 分析対象カラムをlistにする。(分かち書きではなく、Seriesをlistへ変換するような処理)\n","        #self.text = [text.replace(\"\\n\", \" \") for text in self.text] # 単語単位で分かち書きする場合\n","        \n","        if not self.inference_only:\n","            self.target = torch.tensor(df.target.values, dtype=torch.float32) # trainのみ、targetをtensorに変換\n","            self.standard_error = torch.tensor(df.standard_error.values, dtype=torch.float32) \n","\n","        self.encoded = tokenizer.batch_encode_plus( # textをtokenize\n","            self.text,\n","            padding = 'max_length',            \n","            max_length = MAX_LEN,\n","            truncation = True, # 最大長を超える文字は切り捨て\n","            return_attention_mask=True\n","        )        \n"," \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, index): # 変換結果を返す\n","        input_ids = torch.tensor(self.encoded['input_ids'][index])\n","        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n","        \n","        if self.inference_only:\n","            return (input_ids, attention_mask)            \n","        else:\n","            target = self.target[index]\n","            standard_error = self.standard_error[index]\n","            return (input_ids, attention_mask, target, standard_error)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKtdy32wXecG"},"source":["# Model\n","The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.649629Z","iopub.execute_input":"2021-07-04T06:26:40.650066Z","iopub.status.idle":"2021-07-04T06:26:40.666374Z","shell.execute_reply.started":"2021-07-04T06:26:40.650002Z","shell.execute_reply":"2021-07-04T06:26:40.665211Z"},"trusted":true,"id":"BpkxjXEUXecH","executionInfo":{"status":"ok","timestamp":1626829326036,"user_tz":-540,"elapsed":7,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["class LitModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(ROBERTA_PATH) # pretrainedからconfigを読み込み\n","        config.update({\"output_hidden_states\":True, # config更新: embedding層を抽出\n","                       \"hidden_dropout_prob\": 0.0, # config更新: dropoutしない\n","                       \"layer_norm_eps\": 1e-7}) # config更新: layer normalizationのepsilon                      \n","        \n","        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config) # cpuで処理する\n","            \n","        self.attention = nn.Sequential(# attentionレイヤー            \n","            nn.Linear(config.hidden_size, 512),      \n","            nn.Tanh(),                       \n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","\n","        self.regressor = nn.Sequential( # 出力レイヤー                    \n","            nn.Linear(config.hidden_size, 2)                        \n","        )\n","\n","    def forward(self, input_ids, attention_mask):\n","        roberta_output = self.roberta(input_ids=input_ids, # robertaに入力データを流し、出力としてrobertaモデル(layerの複合体)を得る\n","                                      attention_mask=attention_mask)     \n","\n","        last_hidden_state = roberta_output.hidden_states[-1] # robertaモデルの最後のlayerを得る\n","        weights = self.attention(last_hidden_state) # robertaの最後のlayerをattentionへ入力し、出力として重みを得る                \n","        context_vector = torch.sum(weights * last_hidden_state, dim=1) # 重み×最後の層を足し合わせて文書ベクトルとする。\n","        return self.regressor(context_vector) # 文書ベクトルを線形層に入力し、targetを出力する\n","\n","        # https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently\n","        #last_hidden_state = roberta_output[0]\n","        #input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        #sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        #sum_mask = input_mask_expanded.sum(1)\n","        #sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        #mean_embeddings = sum_embeddings / sum_mask\n","\n","        \n","        # Now we reduce the context vector to the prediction score.\n","        #return self.regressor(mean_embeddings) # 文書ベクトルを線形層に入力し、targetを出力する"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.672515Z","iopub.execute_input":"2021-07-04T06:26:40.672944Z","iopub.status.idle":"2021-07-04T06:26:40.684593Z","shell.execute_reply.started":"2021-07-04T06:26:40.672908Z","shell.execute_reply":"2021-07-04T06:26:40.683569Z"},"trusted":true,"id":"bB4jvQTxXecH","executionInfo":{"status":"ok","timestamp":1626829326036,"user_tz":-540,"elapsed":6,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# 評価指標(MSE)の計算。最終的に、ルートしてRMSEにすると思われる。\n","def eval_mse(model, data_loader):\n","    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモードを選択。Batch Normとかdropoutをしなくなる           \n","    mse_mean_sum = 0\n","    mse_std_sum = 0\n","\n","    with torch.no_grad(): # 勾配の計算をしないBlock\n","        for batch_num, (input_ids, attention_mask, target, standard_error) in enumerate(data_loader): # data_loaderからinput, attentin_mask, targetをbatchごとに取り出す\n","            input_ids = input_ids.to(DEVICE)   \n","            attention_mask = attention_mask.to(DEVICE)   \n","            target = target.to(DEVICE)      \n","            standard_error = standard_error.to(DEVICE) \n","            \n","            output = model(input_ids, attention_mask) # 取得した値をモデルへ入力し、出力として予測値を得る。\n","\n","            mse_mean_sum += nn.MSELoss(reduction=\"sum\")(output[:,0].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","            mse_std_sum += nn.MSELoss(reduction=\"sum\")(output[:,1].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","\n","    del input_ids\n","    del attention_mask\n","    del target\n","\n","    mse_mean_result = mse_mean_sum / len(data_loader.dataset)\n","    mse_std_result = mse_std_sum / len(data_loader.dataset)\n","  \n","    return mse_mean_result, mse_std_result # 誤差の合計をdataset長で除し、mseを取得＆返す"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.690155Z","iopub.execute_input":"2021-07-04T06:26:40.692530Z","iopub.status.idle":"2021-07-04T06:26:40.703425Z","shell.execute_reply.started":"2021-07-04T06:26:40.692488Z","shell.execute_reply":"2021-07-04T06:26:40.702366Z"},"trusted":true,"id":"47bDno_LXecI","executionInfo":{"status":"ok","timestamp":1626829326393,"user_tz":-540,"elapsed":362,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# 推論結果を返す\n","def predict(model, data_loader):\n","    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモード(dropout, batch_normしない)\n","\n","    result = np.zeros(len(data_loader.dataset)) # 結果をdataset長のzero配列として用意\n","    index = 0\n","    \n","    with torch.no_grad(): # 勾配の計算をしないblock(inputすると、現状の重みによる推論結果を返す)\n","        for batch_num, (input_ids, attention_mask) in enumerate(data_loader): # data_loaderからbatchごとにinputを得る\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)\n","                        \n","            output = model(input_ids, attention_mask) # modelにinputを入力し、予測結果を得る。\n","\n","            result[index : index + output[:,0].shape[0]] = output[:,0].flatten().to(\"cpu\") # result[index ~ predの長さ]へ、予測結果を格納\n","            index += pred.shape[0] # indexを更新\n","\n","    return result # 全batchで推論が終わったら、結果を返す"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.708605Z","iopub.execute_input":"2021-07-04T06:26:40.709024Z","iopub.status.idle":"2021-07-04T06:26:40.730675Z","shell.execute_reply.started":"2021-07-04T06:26:40.708983Z","shell.execute_reply":"2021-07-04T06:26:40.729705Z"},"trusted":true,"id":"oInneuAmXecI","executionInfo":{"status":"ok","timestamp":1626829326394,"user_tz":-540,"elapsed":15,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# 学習\n","def train(model, # モデル\n","          model_path, # モデルのアウトプット先\n","          train_loader, # train-setのdata_loader\n","          val_loader, # valid-setのdata_loader\n","          optimizer, # optimizer\n","          scheduler=None, # scheduler, デフォルトはNone\n","          num_epochs=NUM_EPOCHS # epoch数、notebook冒頭で指定した値\n","         ):    \n","    \n","    best_val_rmse = None\n","    best_epoch = 0\n","    step = 0\n","    last_eval_step = 0\n","    eval_period = EVAL_SCHEDULE[0][1] # eval期間(って何？) 冒頭で決めたEVAL_SCHEDULEの最初のtupleの[1]を取得\n","\n","    start = time.time() # 時間計測用\n","\n","    for epoch in range(num_epochs): # 指定したEpoch数だけ繰り返し\n","        val_rmse = None         \n","\n","        for batch_num, (input_ids, attention_mask, target, standard_error) in enumerate(train_loader): # train_loaderからinput, targetを取得\n","            input_ids = input_ids.to(DEVICE) # inputをDEVICEへ突っ込む\n","            attention_mask = attention_mask.to(DEVICE)       \n","            target = target.to(DEVICE)\n","            standard_error = standard_error.to(DEVICE)  \n","\n","            optimizer.zero_grad() # 勾配を初期化            \n","            model.train() # 学習モード開始\n","\n","            # https://www.kaggle.com/c/commonlitreadabilityprize/discussion/239421\n","            output = model(input_ids, attention_mask) # input,attention_maskを入力し、予測結果を得る\n","            p = torch.distributions.Normal(output[:,0], torch.sqrt(output[:,1]**2))\n","            q = torch.distributions.Normal(target, standard_error)\n","            kl_vector = torch.distributions.kl_divergence(p, q)\n","            loss = kl_vector.mean()\n","\n","            loss.backward() # 誤差逆伝播法により勾配を得る\n","            optimizer.step() # 重みを更新する\n","\n","            if scheduler:\n","                scheduler.step() # schedulerが与えられた場合は、schedulerの学習率更新\n","            \n","            if step >= last_eval_step + eval_period: # batchを回すごとにstepを増やしていって、「前回evalしたstep + eval_period(16)」を超えたら実行。\n","                # Evaluate the model on val_loader.\n","                elapsed_seconds = time.time() - start # 経過時間\n","                num_steps = step - last_eval_step # 経過ステップ数\n","                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n","                last_eval_step = step # 前回stepの更新\n","                \n","                # valid-setによるrmse計算\n","                train_mean_mse = nn.MSELoss(reduction=\"mean\")(output[:,0].flatten(), target) \n","                train_std_mse = nn.MSELoss(reduction=\"mean\")(torch.sqrt(output[:,1]**2).flatten(), standard_error) \n","\n","                train_mean_rmse = math.sqrt(train_mean_mse)\n","                train_std_rmse = math.sqrt(train_std_mse)\n","\n","                val_mean_mse, val_std_mse = eval_mse(model, val_loader)\n","                val_mean_rmse = math.sqrt(val_mean_mse)                            \n","                val_std_rmse = math.sqrt(val_std_mse)                            \n","\n","                print(f\"Epoch: {epoch} batch_num: {batch_num}\")\n","                print(f\"train_rmse_target: {train_mean_rmse:0.4}\",\n","                      f\"train_rmse_stderror: {train_std_rmse:0.4}\",\n","                      f\"train_kl_div: {loss:0.4}\",\n","                      )\n","                print(f\"val_rmse_target: {val_mean_rmse:0.4}\",\n","                      f\"val_rmse_stderror: {val_std_rmse:0.4}\"\n","                      )\n","\n","                for rmse, period in EVAL_SCHEDULE: # eval_periodをvalid-rmseで切り替える処理\n","                    if val_mean_rmse >= rmse: # valid rmseをEVAL_SCHEDULEと比較し、0項 > valid rmseとなるまで回す : EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n","                        eval_period = period # eval_periodを更新\n","                        break                               \n","\n","                if not best_val_rmse or val_mean_rmse < best_val_rmse: # 初回(best_val_rmse==None), またはbest_val_rmseを更新したらモデルを保存する\n","                    best_val_rmse = val_mean_rmse\n","                    best_epoch = epoch\n","                    torch.save(model.state_dict(), model_path) # 最高の自分を保存\n","                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n","                else:       \n","                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\", # 更新されない場合は、元のスコアを表示\n","                          f\"(from epoch {best_epoch})\")      \n","                                                  \n","                start = time.time()\n","            \n","            # batchごとにメモリ解放\n","            del input_ids\n","            del attention_mask\n","            del target\n","            torch.cuda.empty_cache()                                            \n","            step += 1\n","    \n","    return best_val_rmse"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.735798Z","iopub.execute_input":"2021-07-04T06:26:40.738398Z","iopub.status.idle":"2021-07-04T06:26:40.750876Z","shell.execute_reply.started":"2021-07-04T06:26:40.738356Z","shell.execute_reply":"2021-07-04T06:26:40.749635Z"},"trusted":true,"id":"rMY0fjXwXecJ","executionInfo":{"status":"ok","timestamp":1626829326395,"user_tz":-540,"elapsed":16,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# optimizerの作成\n","def create_optimizer(model):\n","    named_parameters = list(model.named_parameters()) # モデルパラメータの取得\n","    \n","    roberta_parameters = list(model.roberta.named_parameters())[:-2] # パラメータをroberta用、attention用、regressor用に格納。(直接引っ張ってくる形式に変更)\n","    attention_parameters = list(model.attention.named_parameters())\n","    regressor_parameters = list(model.regressor.named_parameters())\n","        \n","    attention_group = [params for (name, params) in attention_parameters] # attention用パラメータをリストとして取得\n","    regressor_group = [params for (name, params) in regressor_parameters] # reg用パラメータをリストとして取得\n","\n","    parameters = []\n","    parameters.append({\"params\": attention_group}) # パラメータをリストに辞書として格納していく\n","    parameters.append({\"params\": regressor_group})\n","\n","    for layer_num, (name, params) in enumerate(roberta_parameters): # レイヤーごとにname, paramsを取得していろんな処理\n","        weight_decay = 0.0 if \"bias\" in name else 0.01\n","\n","        lr = 8e-6\n","\n","        if layer_num >= 69:        \n","            lr = 2e-5\n","\n","        if layer_num >= 133:\n","            lr = 4e-5\n","\n","        parameters.append({\"params\": params,\n","                           \"weight_decay\": weight_decay,\n","                           \"lr\": lr})\n","\n","    return AdamW(parameters) # 最終的に、AdamWにパラメータを入力する。\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbaJojz0Zjif","executionInfo":{"status":"ok","timestamp":1626829326396,"user_tz":-540,"elapsed":16,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# https://www.kaggle.com/abhishek/step-1-create-folds\n","def create_folds(data, num_splits, SEED, return_df=False):\n","    # we create a new column called kfold and fill it with -1\n","    data[\"kfold\"] = -1\n","    \n","    # the next step is to randomize the rows of the data\n","    data = data.sample(frac=1).reset_index(drop=True)\n","\n","    # calculate number of bins by Sturge's rule\n","    # I take the floor of the value, you can also\n","    # just round it\n","    num_bins = int(np.floor(1 + np.log2(len(data))))\n","    \n","    # bin targets\n","    data.loc[:, \"bins_tg\"] = pd.cut(\n","        data[\"target\"], bins=num_bins, labels=False\n","    ).map(lambda x: str(x))\n","\n","    # bin standard_error\n","    data.loc[:, \"bins_std\"] = pd.cut(\n","        data[\"standard_error\"], bins=num_bins, labels=False\n","    )\n","\n","    # bins\n","    data.loc[:, \"bins\"] = data['bins_tg'].map(lambda x: str(x)) + data['bins_std'].map(lambda x: str(x))\n","\n","    # initiate the kfold class from model_selection module\n","    kf = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n","\n","    # note that, instead of targets, we use bins!\n","    if return_df:\n","      for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n","        data.loc[v_, 'kfold'] = f\n","      return data\n","    else:\n","      return kf.split(X=data, y=data.bins.values)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"id":"vAmhaYaylMk5","executionInfo":{"status":"ok","timestamp":1626829326397,"user_tz":-540,"elapsed":17,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"348ab9a2-daaf-4298-cc39-40da38d6cca6"},"source":["# 検証用\n","SEED = 1000\n","st_kfold_bins_df = create_folds(train_df, num_splits=5, SEED=SEED, return_df=True)\n","st_kfold_bins_df['bins_tg'] = st_kfold_bins_df['bins_tg'].map(lambda x: float(x))\n","st_kfold_bins_df['bins_std'] = st_kfold_bins_df['bins_std'].map(lambda x: float(x))\n","st_kfold_bins_df.groupby('kfold').agg({'bins_tg': ['min', 'max', 'mean'],\n","                                    'bins_std': ['min', 'max', 'mean']})"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n","  % (min_groups, self.n_splits)), UserWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"3\" halign=\"left\">bins_tg</th>\n","      <th colspan=\"3\" halign=\"left\">bins_std</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>min</th>\n","      <th>max</th>\n","      <th>mean</th>\n","      <th>min</th>\n","      <th>max</th>\n","      <th>mean</th>\n","    </tr>\n","    <tr>\n","      <th>kfold</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.534392</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.902998</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.562610</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.911817</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.536155</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.943563</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.553004</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.932862</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.563604</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.954064</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      bins_tg                 bins_std                \n","          min   max      mean      min   max      mean\n","kfold                                                 \n","0         0.0  11.0  5.534392      0.0  11.0  2.902998\n","1         0.0  11.0  5.562610      0.0  11.0  2.911817\n","2         0.0  11.0  5.536155      0.0  11.0  2.943563\n","3         0.0  11.0  5.553004      0.0  11.0  2.932862\n","4         0.0  11.0  5.563604      0.0  11.0  2.954064"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"TyjgRCu3mmqG","executionInfo":{"status":"ok","timestamp":1626829326398,"user_tz":-540,"elapsed":14,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PLKHwvKtNBn","executionInfo":{"status":"ok","timestamp":1626829326398,"user_tz":-540,"elapsed":12,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["def train_and_save_model(train_indices, val_indices, model_path):\n","    train_dataset = LitDataset(train_df.loc[train_indices]) # train, validのDataset\n","    val_dataset = LitDataset(train_df.loc[val_indices])\n","        \n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              drop_last=True, shuffle=True, num_workers=2) # train, validのDataLoader\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                            drop_last=False, shuffle=False, num_workers=2)    \n","\n","    model = LitModel().to(DEVICE) # modelをDEVICEへぶち込む\n","    optimizer = create_optimizer(model) # optimizerをモデルから作成\n","    scheduler = get_cosine_schedule_with_warmup( # schedulerを作成\n","        optimizer,\n","        num_training_steps=NUM_EPOCHS * len(train_loader),\n","        num_warmup_steps=50)    \n","    rmse = train(model, model_path, train_loader, val_loader, optimizer, scheduler=scheduler)\n","\n","    del train_dataset\n","    del val_dataset\n","    del train_loader\n","    del val_loader\n","    del model\n","    del optimizer\n","    del scheduler\n","    gc.collect() \n","    torch.cuda.empty_cache()\n","    return rmse"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.755813Z","iopub.execute_input":"2021-07-04T06:26:40.758373Z","iopub.status.idle":"2021-07-04T06:27:12.493221Z","shell.execute_reply.started":"2021-07-04T06:26:40.758265Z","shell.execute_reply":"2021-07-04T06:27:12.490139Z"},"trusted":true,"id":"k2LGJD3XXecK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626839345559,"user_tz":-540,"elapsed":10019172,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"e7df2dc9-a3ec-49d8-da17-768e11df228a"},"source":["# 実行処理。 KFold & 学習\n","SEED = 1000\n","list_val_rmse = []\n","\n","#kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n","kfold = create_folds(train_df, 5, SEED=SEED, return_df=False) # binsで切る場合\n","\n","for fold, (train_indices, val_indices) in enumerate(kfold):    \n","    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n","    #print(gpuinfo())\n","    model_path = f\"model_{fold + 1}.pth\" # model_fold数_.pth\n","    set_random_seed(SEED + fold) # SEEDはfold別に変わるようにする\n","    list_val_rmse.append(train_and_save_model(train_indices, val_indices, model_path))\n","\n","    print(\"\\nPerformance estimates:\")\n","    print(list_val_rmse)\n","    print(\"Mean:\", np.array(list_val_rmse).mean())\n","    #print(gpuinfo())"],"execution_count":26,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n","  % (min_groups, self.n_splits)), UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Fold 1/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 82.5 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.6077 train_rmse_stderror: 0.0455 train_kl_div: 0.8339\n","val_rmse_target: 0.6523 val_rmse_stderror: 1.752\n","New best_val_rmse: 0.6523\n","\n","64 steps took 81.0 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.4919 train_rmse_stderror: 0.02783 train_kl_div: 0.5227\n","val_rmse_target: 0.5971 val_rmse_stderror: 1.756\n","New best_val_rmse: 0.5971\n","\n","64 steps took 81.1 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.4433 train_rmse_stderror: 0.04053 train_kl_div: 0.3768\n","val_rmse_target: 0.5854 val_rmse_stderror: 1.784\n","New best_val_rmse: 0.5854\n","\n","64 steps took 81.0 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.2119 train_rmse_stderror: 0.03297 train_kl_div: 0.08982\n","val_rmse_target: 0.5106 val_rmse_stderror: 1.758\n","New best_val_rmse: 0.5106\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.5532 train_rmse_stderror: 0.03888 train_kl_div: 0.5445\n","val_rmse_target: 0.5766 val_rmse_stderror: 1.757\n","Still best_val_rmse: 0.5106 (from epoch 1)\n","\n","64 steps took 81.0 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.3731 train_rmse_stderror: 0.0286 train_kl_div: 0.3166\n","val_rmse_target: 0.4951 val_rmse_stderror: 1.767\n","New best_val_rmse: 0.4951\n","\n","16 steps took 20.2 seconds\n","Epoch: 1 batch_num: 180\n","train_rmse_target: 0.5 train_rmse_stderror: 0.03103 train_kl_div: 0.505\n","val_rmse_target: 0.545 val_rmse_stderror: 1.769\n","Still best_val_rmse: 0.4951 (from epoch 1)\n","\n","32 steps took 40.6 seconds\n","Epoch: 2 batch_num: 24\n","train_rmse_target: 0.252 train_rmse_stderror: 0.0395 train_kl_div: 0.137\n","val_rmse_target: 0.5047 val_rmse_stderror: 1.752\n","Still best_val_rmse: 0.4951 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 56\n","train_rmse_target: 0.2921 train_rmse_stderror: 0.03497 train_kl_div: 0.1651\n","val_rmse_target: 0.4813 val_rmse_stderror: 1.775\n","New best_val_rmse: 0.4813\n","\n","8 steps took 10.1 seconds\n","Epoch: 2 batch_num: 64\n","train_rmse_target: 0.2459 train_rmse_stderror: 0.01518 train_kl_div: 0.1256\n","val_rmse_target: 0.4827 val_rmse_stderror: 1.749\n","Still best_val_rmse: 0.4813 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.3059 train_rmse_stderror: 0.03158 train_kl_div: 0.1911\n","val_rmse_target: 0.5118 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4813 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.2951 train_rmse_stderror: 0.0337 train_kl_div: 0.1873\n","val_rmse_target: 0.4752 val_rmse_stderror: 1.754\n","New best_val_rmse: 0.4752\n","\n","4 steps took 5.07 seconds\n","Epoch: 2 batch_num: 108\n","train_rmse_target: 0.2251 train_rmse_stderror: 0.02375 train_kl_div: 0.1137\n","val_rmse_target: 0.5243 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 140\n","train_rmse_target: 0.3551 train_rmse_stderror: 0.03548 train_kl_div: 0.2578\n","val_rmse_target: 0.5007 val_rmse_stderror: 1.748\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 172\n","train_rmse_target: 0.2398 train_rmse_stderror: 0.01689 train_kl_div: 0.1125\n","val_rmse_target: 0.4852 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 2 batch_num: 180\n","train_rmse_target: 0.2457 train_rmse_stderror: 0.01974 train_kl_div: 0.132\n","val_rmse_target: 0.5152 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","32 steps took 40.6 seconds\n","Epoch: 3 batch_num: 24\n","train_rmse_target: 0.1803 train_rmse_stderror: 0.03147 train_kl_div: 0.06544\n","val_rmse_target: 0.4753 val_rmse_stderror: 1.765\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","4 steps took 5.05 seconds\n","Epoch: 3 batch_num: 28\n","train_rmse_target: 0.1928 train_rmse_stderror: 0.0305 train_kl_div: 0.08631\n","val_rmse_target: 0.483 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 36\n","train_rmse_target: 0.1334 train_rmse_stderror: 0.02533 train_kl_div: 0.03963\n","val_rmse_target: 0.4826 val_rmse_stderror: 1.754\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.1585 train_rmse_stderror: 0.03017 train_kl_div: 0.05288\n","val_rmse_target: 0.4785 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","4 steps took 5.05 seconds\n","Epoch: 3 batch_num: 48\n","train_rmse_target: 0.1308 train_rmse_stderror: 0.02714 train_kl_div: 0.03756\n","val_rmse_target: 0.4754 val_rmse_stderror: 1.756\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","4 steps took 5.04 seconds\n","Epoch: 3 batch_num: 52\n","train_rmse_target: 0.2152 train_rmse_stderror: 0.03699 train_kl_div: 0.08456\n","val_rmse_target: 0.4803 val_rmse_stderror: 1.752\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 60\n","train_rmse_target: 0.2193 train_rmse_stderror: 0.0275 train_kl_div: 0.09984\n","val_rmse_target: 0.5036 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 92\n","train_rmse_target: 0.17 train_rmse_stderror: 0.02411 train_kl_div: 0.05716\n","val_rmse_target: 0.4833 val_rmse_stderror: 1.763\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 100\n","train_rmse_target: 0.1939 train_rmse_stderror: 0.0278 train_kl_div: 0.06823\n","val_rmse_target: 0.4811 val_rmse_stderror: 1.768\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1143 train_rmse_stderror: 0.03676 train_kl_div: 0.03126\n","val_rmse_target: 0.489 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 116\n","train_rmse_target: 0.1099 train_rmse_stderror: 0.0274 train_kl_div: 0.02757\n","val_rmse_target: 0.4822 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 124\n","train_rmse_target: 0.2085 train_rmse_stderror: 0.0228 train_kl_div: 0.08052\n","val_rmse_target: 0.4785 val_rmse_stderror: 1.757\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","4 steps took 5.04 seconds\n","Epoch: 3 batch_num: 128\n","train_rmse_target: 0.1529 train_rmse_stderror: 0.01436 train_kl_div: 0.0456\n","val_rmse_target: 0.4817 val_rmse_stderror: 1.764\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 136\n","train_rmse_target: 0.1393 train_rmse_stderror: 0.02 train_kl_div: 0.03902\n","val_rmse_target: 0.4828 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 144\n","train_rmse_target: 0.1041 train_rmse_stderror: 0.02236 train_kl_div: 0.0247\n","val_rmse_target: 0.4802 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 152\n","train_rmse_target: 0.1312 train_rmse_stderror: 0.02153 train_kl_div: 0.03542\n","val_rmse_target: 0.4753 val_rmse_stderror: 1.763\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","4 steps took 5.04 seconds\n","Epoch: 3 batch_num: 156\n","train_rmse_target: 0.1829 train_rmse_stderror: 0.0261 train_kl_div: 0.07194\n","val_rmse_target: 0.4756 val_rmse_stderror: 1.764\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","4 steps took 5.05 seconds\n","Epoch: 3 batch_num: 160\n","train_rmse_target: 0.1139 train_rmse_stderror: 0.02652 train_kl_div: 0.02707\n","val_rmse_target: 0.4778 val_rmse_stderror: 1.763\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","4 steps took 5.04 seconds\n","Epoch: 3 batch_num: 164\n","train_rmse_target: 0.1416 train_rmse_stderror: 0.01679 train_kl_div: 0.04459\n","val_rmse_target: 0.4805 val_rmse_stderror: 1.764\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.1637 train_rmse_stderror: 0.02493 train_kl_div: 0.05403\n","val_rmse_target: 0.4771 val_rmse_stderror: 1.765\n","Still best_val_rmse: 0.4752 (from epoch 2)\n","\n","4 steps took 5.05 seconds\n","Epoch: 3 batch_num: 176\n","train_rmse_target: 0.1627 train_rmse_stderror: 0.01709 train_kl_div: 0.05475\n","val_rmse_target: 0.4733 val_rmse_stderror: 1.763\n","New best_val_rmse: 0.4733\n","\n","4 steps took 5.07 seconds\n","Epoch: 3 batch_num: 180\n","train_rmse_target: 0.1905 train_rmse_stderror: 0.03286 train_kl_div: 0.06285\n","val_rmse_target: 0.4743 val_rmse_stderror: 1.762\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.05 seconds\n","Epoch: 3 batch_num: 184\n","train_rmse_target: 0.1515 train_rmse_stderror: 0.02582 train_kl_div: 0.05165\n","val_rmse_target: 0.477 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.23 seconds\n","Epoch: 4 batch_num: 0\n","train_rmse_target: 0.1332 train_rmse_stderror: 0.02161 train_kl_div: 0.03799\n","val_rmse_target: 0.4812 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 8\n","train_rmse_target: 0.1603 train_rmse_stderror: 0.02118 train_kl_div: 0.05384\n","val_rmse_target: 0.4858 val_rmse_stderror: 1.763\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1251 train_rmse_stderror: 0.01736 train_kl_div: 0.03357\n","val_rmse_target: 0.4821 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 24\n","train_rmse_target: 0.0846 train_rmse_stderror: 0.02008 train_kl_div: 0.01666\n","val_rmse_target: 0.4797 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.05 seconds\n","Epoch: 4 batch_num: 28\n","train_rmse_target: 0.1548 train_rmse_stderror: 0.02013 train_kl_div: 0.04854\n","val_rmse_target: 0.4781 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.05 seconds\n","Epoch: 4 batch_num: 32\n","train_rmse_target: 0.08564 train_rmse_stderror: 0.02607 train_kl_div: 0.01643\n","val_rmse_target: 0.479 val_rmse_stderror: 1.757\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.04 seconds\n","Epoch: 4 batch_num: 36\n","train_rmse_target: 0.1436 train_rmse_stderror: 0.03721 train_kl_div: 0.04045\n","val_rmse_target: 0.4782 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.05 seconds\n","Epoch: 4 batch_num: 40\n","train_rmse_target: 0.1114 train_rmse_stderror: 0.02162 train_kl_div: 0.0276\n","val_rmse_target: 0.4772 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.05 seconds\n","Epoch: 4 batch_num: 44\n","train_rmse_target: 0.1868 train_rmse_stderror: 0.027 train_kl_div: 0.05725\n","val_rmse_target: 0.478 val_rmse_stderror: 1.762\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.04 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.09365 train_rmse_stderror: 0.0177 train_kl_div: 0.01963\n","val_rmse_target: 0.4808 val_rmse_stderror: 1.762\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 56\n","train_rmse_target: 0.1508 train_rmse_stderror: 0.02934 train_kl_div: 0.04437\n","val_rmse_target: 0.4815 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 64\n","train_rmse_target: 0.1294 train_rmse_stderror: 0.01793 train_kl_div: 0.037\n","val_rmse_target: 0.4838 val_rmse_stderror: 1.757\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 72\n","train_rmse_target: 0.1386 train_rmse_stderror: 0.02722 train_kl_div: 0.03119\n","val_rmse_target: 0.4842 val_rmse_stderror: 1.763\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.1082 train_rmse_stderror: 0.02346 train_kl_div: 0.02576\n","val_rmse_target: 0.4826 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 88\n","train_rmse_target: 0.09161 train_rmse_stderror: 0.02011 train_kl_div: 0.01894\n","val_rmse_target: 0.4822 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 96\n","train_rmse_target: 0.1215 train_rmse_stderror: 0.01979 train_kl_div: 0.03246\n","val_rmse_target: 0.4802 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 104\n","train_rmse_target: 0.1729 train_rmse_stderror: 0.02112 train_kl_div: 0.05367\n","val_rmse_target: 0.4792 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.05 seconds\n","Epoch: 4 batch_num: 108\n","train_rmse_target: 0.1274 train_rmse_stderror: 0.026 train_kl_div: 0.03708\n","val_rmse_target: 0.4793 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.05 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.07619 train_rmse_stderror: 0.02907 train_kl_div: 0.01512\n","val_rmse_target: 0.4798 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.04 seconds\n","Epoch: 4 batch_num: 116\n","train_rmse_target: 0.07637 train_rmse_stderror: 0.02207 train_kl_div: 0.01537\n","val_rmse_target: 0.4801 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 124\n","train_rmse_target: 0.1316 train_rmse_stderror: 0.0284 train_kl_div: 0.0284\n","val_rmse_target: 0.4795 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.06 seconds\n","Epoch: 4 batch_num: 128\n","train_rmse_target: 0.11 train_rmse_stderror: 0.02784 train_kl_div: 0.0285\n","val_rmse_target: 0.4796 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.04 seconds\n","Epoch: 4 batch_num: 132\n","train_rmse_target: 0.09237 train_rmse_stderror: 0.01555 train_kl_div: 0.01879\n","val_rmse_target: 0.48 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","4 steps took 5.05 seconds\n","Epoch: 4 batch_num: 136\n","train_rmse_target: 0.08672 train_rmse_stderror: 0.02162 train_kl_div: 0.01826\n","val_rmse_target: 0.4802 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.06208 train_rmse_stderror: 0.02012 train_kl_div: 0.009388\n","val_rmse_target: 0.4803 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 152\n","train_rmse_target: 0.09954 train_rmse_stderror: 0.02238 train_kl_div: 0.02307\n","val_rmse_target: 0.4804 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 160\n","train_rmse_target: 0.1256 train_rmse_stderror: 0.0351 train_kl_div: 0.03694\n","val_rmse_target: 0.4803 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 168\n","train_rmse_target: 0.1038 train_rmse_stderror: 0.02391 train_kl_div: 0.02399\n","val_rmse_target: 0.4802 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1013 train_rmse_stderror: 0.02415 train_kl_div: 0.02253\n","val_rmse_target: 0.4802 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 184\n","train_rmse_target: 0.08523 train_rmse_stderror: 0.02482 train_kl_div: 0.01965\n","val_rmse_target: 0.4802 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4733 (from epoch 3)\n","\n","Performance estimates:\n","[0.47334423773166556]\n","Mean: 0.47334423773166556\n","\n","Fold 2/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 81.9 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.3855 train_rmse_stderror: 0.07937 train_kl_div: 0.3071\n","val_rmse_target: 0.6773 val_rmse_stderror: 1.109\n","New best_val_rmse: 0.6773\n","\n","64 steps took 81.0 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.7952 train_rmse_stderror: 0.08246 train_kl_div: 1.307\n","val_rmse_target: 0.6645 val_rmse_stderror: 1.172\n","New best_val_rmse: 0.6645\n","\n","64 steps took 81.1 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.552 train_rmse_stderror: 0.06294 train_kl_div: 0.5156\n","val_rmse_target: 0.5724 val_rmse_stderror: 1.162\n","New best_val_rmse: 0.5724\n","\n","64 steps took 81.0 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.284 train_rmse_stderror: 0.02332 train_kl_div: 0.1816\n","val_rmse_target: 0.5099 val_rmse_stderror: 1.176\n","New best_val_rmse: 0.5099\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.3495 train_rmse_stderror: 0.02729 train_kl_div: 0.27\n","val_rmse_target: 0.5475 val_rmse_stderror: 1.162\n","Still best_val_rmse: 0.5099 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.4499 train_rmse_stderror: 0.0391 train_kl_div: 0.4278\n","val_rmse_target: 0.5652 val_rmse_stderror: 1.178\n","Still best_val_rmse: 0.5099 (from epoch 1)\n","\n","64 steps took 81.1 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.2094 train_rmse_stderror: 0.0211 train_kl_div: 0.1009\n","val_rmse_target: 0.5085 val_rmse_stderror: 1.167\n","New best_val_rmse: 0.5085\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2614 train_rmse_stderror: 0.02476 train_kl_div: 0.1424\n","val_rmse_target: 0.5081 val_rmse_stderror: 1.164\n","New best_val_rmse: 0.5081\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2888 train_rmse_stderror: 0.02749 train_kl_div: 0.189\n","val_rmse_target: 0.5074 val_rmse_stderror: 1.174\n","New best_val_rmse: 0.5074\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.3329 train_rmse_stderror: 0.04419 train_kl_div: 0.1792\n","val_rmse_target: 0.5237 val_rmse_stderror: 1.158\n","Still best_val_rmse: 0.5074 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.3209 train_rmse_stderror: 0.03144 train_kl_div: 0.2123\n","val_rmse_target: 0.5141 val_rmse_stderror: 1.162\n","Still best_val_rmse: 0.5074 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.3153 train_rmse_stderror: 0.03102 train_kl_div: 0.2161\n","val_rmse_target: 0.4943 val_rmse_stderror: 1.159\n","New best_val_rmse: 0.4943\n","\n","16 steps took 20.2 seconds\n","Epoch: 2 batch_num: 184\n","train_rmse_target: 0.1898 train_rmse_stderror: 0.02172 train_kl_div: 0.07778\n","val_rmse_target: 0.5114 val_rmse_stderror: 1.175\n","Still best_val_rmse: 0.4943 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 28\n","train_rmse_target: 0.2057 train_rmse_stderror: 0.03488 train_kl_div: 0.08762\n","val_rmse_target: 0.5056 val_rmse_stderror: 1.174\n","Still best_val_rmse: 0.4943 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 60\n","train_rmse_target: 0.1451 train_rmse_stderror: 0.02681 train_kl_div: 0.0521\n","val_rmse_target: 0.4995 val_rmse_stderror: 1.166\n","Still best_val_rmse: 0.4943 (from epoch 2)\n","\n","16 steps took 20.2 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.2186 train_rmse_stderror: 0.03495 train_kl_div: 0.08156\n","val_rmse_target: 0.5006 val_rmse_stderror: 1.168\n","Still best_val_rmse: 0.4943 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1247 train_rmse_stderror: 0.02616 train_kl_div: 0.03254\n","val_rmse_target: 0.5022 val_rmse_stderror: 1.173\n","Still best_val_rmse: 0.4943 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1769 train_rmse_stderror: 0.02602 train_kl_div: 0.07339\n","val_rmse_target: 0.4967 val_rmse_stderror: 1.163\n","Still best_val_rmse: 0.4943 (from epoch 2)\n","\n","16 steps took 20.2 seconds\n","Epoch: 3 batch_num: 156\n","train_rmse_target: 0.1708 train_rmse_stderror: 0.02513 train_kl_div: 0.06109\n","val_rmse_target: 0.4944 val_rmse_stderror: 1.162\n","Still best_val_rmse: 0.4943 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.1487 train_rmse_stderror: 0.04025 train_kl_div: 0.04996\n","val_rmse_target: 0.4974 val_rmse_stderror: 1.164\n","Still best_val_rmse: 0.4943 (from epoch 2)\n","\n","16 steps took 20.4 seconds\n","Epoch: 4 batch_num: 0\n","train_rmse_target: 0.09403 train_rmse_stderror: 0.02132 train_kl_div: 0.0222\n","val_rmse_target: 0.494 val_rmse_stderror: 1.169\n","New best_val_rmse: 0.494\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.09394 train_rmse_stderror: 0.01815 train_kl_div: 0.01788\n","val_rmse_target: 0.4993 val_rmse_stderror: 1.163\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 32\n","train_rmse_target: 0.1263 train_rmse_stderror: 0.02108 train_kl_div: 0.03662\n","val_rmse_target: 0.4971 val_rmse_stderror: 1.169\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1051 train_rmse_stderror: 0.01611 train_kl_div: 0.02178\n","val_rmse_target: 0.4992 val_rmse_stderror: 1.167\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 64\n","train_rmse_target: 0.04671 train_rmse_stderror: 0.01801 train_kl_div: 0.006196\n","val_rmse_target: 0.496 val_rmse_stderror: 1.168\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.1228 train_rmse_stderror: 0.01716 train_kl_div: 0.03227\n","val_rmse_target: 0.4975 val_rmse_stderror: 1.165\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 96\n","train_rmse_target: 0.1055 train_rmse_stderror: 0.01517 train_kl_div: 0.02631\n","val_rmse_target: 0.4961 val_rmse_stderror: 1.164\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.07666 train_rmse_stderror: 0.0217 train_kl_div: 0.01431\n","val_rmse_target: 0.496 val_rmse_stderror: 1.165\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 128\n","train_rmse_target: 0.1293 train_rmse_stderror: 0.01698 train_kl_div: 0.03669\n","val_rmse_target: 0.4958 val_rmse_stderror: 1.166\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.09098 train_rmse_stderror: 0.02089 train_kl_div: 0.01967\n","val_rmse_target: 0.4961 val_rmse_stderror: 1.166\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 160\n","train_rmse_target: 0.1016 train_rmse_stderror: 0.0243 train_kl_div: 0.0218\n","val_rmse_target: 0.4961 val_rmse_stderror: 1.166\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.09931 train_rmse_stderror: 0.02026 train_kl_div: 0.02298\n","val_rmse_target: 0.4961 val_rmse_stderror: 1.166\n","Still best_val_rmse: 0.494 (from epoch 4)\n","\n","Performance estimates:\n","[0.47334423773166556, 0.4939554494572308]\n","Mean: 0.4836498435944482\n","\n","Fold 3/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 81.9 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.7115 train_rmse_stderror: 0.04113 train_kl_div: 0.9814\n","val_rmse_target: 0.6776 val_rmse_stderror: 1.772\n","New best_val_rmse: 0.6776\n","\n","64 steps took 81.0 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.6928 train_rmse_stderror: 0.06911 train_kl_div: 0.8434\n","val_rmse_target: 0.5506 val_rmse_stderror: 1.794\n","New best_val_rmse: 0.5506\n","\n","64 steps took 81.2 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.6082 train_rmse_stderror: 0.02707 train_kl_div: 0.73\n","val_rmse_target: 0.6573 val_rmse_stderror: 1.791\n","Still best_val_rmse: 0.5506 (from epoch 0)\n","\n","64 steps took 80.9 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.489 train_rmse_stderror: 0.06114 train_kl_div: 0.4256\n","val_rmse_target: 0.5472 val_rmse_stderror: 1.805\n","New best_val_rmse: 0.5472\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.3874 train_rmse_stderror: 0.03265 train_kl_div: 0.2906\n","val_rmse_target: 0.5262 val_rmse_stderror: 1.795\n","New best_val_rmse: 0.5262\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.3528 train_rmse_stderror: 0.03727 train_kl_div: 0.3009\n","val_rmse_target: 0.5222 val_rmse_stderror: 1.779\n","New best_val_rmse: 0.5222\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.3537 train_rmse_stderror: 0.03823 train_kl_div: 0.2601\n","val_rmse_target: 0.5202 val_rmse_stderror: 1.768\n","New best_val_rmse: 0.5202\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.3119 train_rmse_stderror: 0.02275 train_kl_div: 0.2029\n","val_rmse_target: 0.502 val_rmse_stderror: 1.798\n","New best_val_rmse: 0.502\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2464 train_rmse_stderror: 0.04002 train_kl_div: 0.151\n","val_rmse_target: 0.5065 val_rmse_stderror: 1.793\n","Still best_val_rmse: 0.502 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.1898 train_rmse_stderror: 0.03337 train_kl_div: 0.08415\n","val_rmse_target: 0.4928 val_rmse_stderror: 1.802\n","New best_val_rmse: 0.4928\n","\n","16 steps took 20.2 seconds\n","Epoch: 2 batch_num: 88\n","train_rmse_target: 0.3445 train_rmse_stderror: 0.03906 train_kl_div: 0.2186\n","val_rmse_target: 0.4947 val_rmse_stderror: 1.789\n","Still best_val_rmse: 0.4928 (from epoch 2)\n","\n","16 steps took 20.2 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.175 train_rmse_stderror: 0.02277 train_kl_div: 0.06192\n","val_rmse_target: 0.5026 val_rmse_stderror: 1.787\n","Still best_val_rmse: 0.4928 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.2609 train_rmse_stderror: 0.0318 train_kl_div: 0.1112\n","val_rmse_target: 0.4888 val_rmse_stderror: 1.784\n","New best_val_rmse: 0.4888\n","\n","8 steps took 10.1 seconds\n","Epoch: 2 batch_num: 144\n","train_rmse_target: 0.1883 train_rmse_stderror: 0.02893 train_kl_div: 0.07658\n","val_rmse_target: 0.486 val_rmse_stderror: 1.791\n","New best_val_rmse: 0.486\n","\n","8 steps took 10.1 seconds\n","Epoch: 2 batch_num: 152\n","train_rmse_target: 0.1374 train_rmse_stderror: 0.03994 train_kl_div: 0.03888\n","val_rmse_target: 0.4912 val_rmse_stderror: 1.777\n","Still best_val_rmse: 0.486 (from epoch 2)\n","\n","16 steps took 20.2 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.2465 train_rmse_stderror: 0.02503 train_kl_div: 0.1166\n","val_rmse_target: 0.4884 val_rmse_stderror: 1.789\n","Still best_val_rmse: 0.486 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 2 batch_num: 176\n","train_rmse_target: 0.2666 train_rmse_stderror: 0.03064 train_kl_div: 0.1621\n","val_rmse_target: 0.5082 val_rmse_stderror: 1.782\n","Still best_val_rmse: 0.486 (from epoch 2)\n","\n","32 steps took 40.6 seconds\n","Epoch: 3 batch_num: 20\n","train_rmse_target: 0.1251 train_rmse_stderror: 0.01718 train_kl_div: 0.03902\n","val_rmse_target: 0.4867 val_rmse_stderror: 1.79\n","Still best_val_rmse: 0.486 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 28\n","train_rmse_target: 0.1896 train_rmse_stderror: 0.02419 train_kl_div: 0.08204\n","val_rmse_target: 0.4944 val_rmse_stderror: 1.785\n","Still best_val_rmse: 0.486 (from epoch 2)\n","\n","16 steps took 20.2 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.1419 train_rmse_stderror: 0.03011 train_kl_div: 0.04307\n","val_rmse_target: 0.4892 val_rmse_stderror: 1.787\n","Still best_val_rmse: 0.486 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 52\n","train_rmse_target: 0.1352 train_rmse_stderror: 0.02988 train_kl_div: 0.04152\n","val_rmse_target: 0.4837 val_rmse_stderror: 1.793\n","New best_val_rmse: 0.4837\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 60\n","train_rmse_target: 0.07785 train_rmse_stderror: 0.01924 train_kl_div: 0.01387\n","val_rmse_target: 0.4838 val_rmse_stderror: 1.78\n","Still best_val_rmse: 0.4837 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 68\n","train_rmse_target: 0.06374 train_rmse_stderror: 0.02756 train_kl_div: 0.01238\n","val_rmse_target: 0.4868 val_rmse_stderror: 1.794\n","Still best_val_rmse: 0.4837 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1461 train_rmse_stderror: 0.02802 train_kl_div: 0.04729\n","val_rmse_target: 0.4829 val_rmse_stderror: 1.782\n","New best_val_rmse: 0.4829\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 84\n","train_rmse_target: 0.08013 train_rmse_stderror: 0.0205 train_kl_div: 0.015\n","val_rmse_target: 0.481 val_rmse_stderror: 1.791\n","New best_val_rmse: 0.481\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 92\n","train_rmse_target: 0.1357 train_rmse_stderror: 0.02994 train_kl_div: 0.04219\n","val_rmse_target: 0.487 val_rmse_stderror: 1.779\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 100\n","train_rmse_target: 0.213 train_rmse_stderror: 0.02899 train_kl_div: 0.08337\n","val_rmse_target: 0.4826 val_rmse_stderror: 1.791\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1425 train_rmse_stderror: 0.02772 train_kl_div: 0.04436\n","val_rmse_target: 0.4819 val_rmse_stderror: 1.779\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 116\n","train_rmse_target: 0.16 train_rmse_stderror: 0.01994 train_kl_div: 0.06506\n","val_rmse_target: 0.4833 val_rmse_stderror: 1.781\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 124\n","train_rmse_target: 0.1508 train_rmse_stderror: 0.02487 train_kl_div: 0.042\n","val_rmse_target: 0.4859 val_rmse_stderror: 1.799\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 132\n","train_rmse_target: 0.1127 train_rmse_stderror: 0.02421 train_kl_div: 0.02722\n","val_rmse_target: 0.482 val_rmse_stderror: 1.779\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1031 train_rmse_stderror: 0.01792 train_kl_div: 0.0225\n","val_rmse_target: 0.4817 val_rmse_stderror: 1.781\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 148\n","train_rmse_target: 0.08307 train_rmse_stderror: 0.02078 train_kl_div: 0.0143\n","val_rmse_target: 0.4878 val_rmse_stderror: 1.79\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 156\n","train_rmse_target: 0.1266 train_rmse_stderror: 0.01816 train_kl_div: 0.03464\n","val_rmse_target: 0.4848 val_rmse_stderror: 1.779\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 164\n","train_rmse_target: 0.1204 train_rmse_stderror: 0.02909 train_kl_div: 0.03038\n","val_rmse_target: 0.4822 val_rmse_stderror: 1.784\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.1291 train_rmse_stderror: 0.01956 train_kl_div: 0.03158\n","val_rmse_target: 0.4832 val_rmse_stderror: 1.788\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 180\n","train_rmse_target: 0.1245 train_rmse_stderror: 0.03309 train_kl_div: 0.03402\n","val_rmse_target: 0.4866 val_rmse_stderror: 1.779\n","Still best_val_rmse: 0.481 (from epoch 3)\n","\n","8 steps took 10.3 seconds\n","Epoch: 4 batch_num: 0\n","train_rmse_target: 0.08049 train_rmse_stderror: 0.02165 train_kl_div: 0.01535\n","val_rmse_target: 0.4808 val_rmse_stderror: 1.787\n","New best_val_rmse: 0.4808\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 8\n","train_rmse_target: 0.1498 train_rmse_stderror: 0.0214 train_kl_div: 0.04248\n","val_rmse_target: 0.4819 val_rmse_stderror: 1.784\n","Still best_val_rmse: 0.4808 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.06106 train_rmse_stderror: 0.02141 train_kl_div: 0.009739\n","val_rmse_target: 0.4838 val_rmse_stderror: 1.785\n","Still best_val_rmse: 0.4808 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 24\n","train_rmse_target: 0.09703 train_rmse_stderror: 0.0245 train_kl_div: 0.01854\n","val_rmse_target: 0.4807 val_rmse_stderror: 1.791\n","New best_val_rmse: 0.4807\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 32\n","train_rmse_target: 0.1036 train_rmse_stderror: 0.02324 train_kl_div: 0.02319\n","val_rmse_target: 0.4818 val_rmse_stderror: 1.783\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 40\n","train_rmse_target: 0.05626 train_rmse_stderror: 0.02575 train_kl_div: 0.009937\n","val_rmse_target: 0.4822 val_rmse_stderror: 1.786\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1044 train_rmse_stderror: 0.02648 train_kl_div: 0.0251\n","val_rmse_target: 0.4838 val_rmse_stderror: 1.788\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 56\n","train_rmse_target: 0.06488 train_rmse_stderror: 0.02198 train_kl_div: 0.01057\n","val_rmse_target: 0.4843 val_rmse_stderror: 1.787\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 64\n","train_rmse_target: 0.09875 train_rmse_stderror: 0.01913 train_kl_div: 0.01988\n","val_rmse_target: 0.4832 val_rmse_stderror: 1.787\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 72\n","train_rmse_target: 0.06522 train_rmse_stderror: 0.02497 train_kl_div: 0.01208\n","val_rmse_target: 0.4835 val_rmse_stderror: 1.787\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.08435 train_rmse_stderror: 0.02787 train_kl_div: 0.01775\n","val_rmse_target: 0.4823 val_rmse_stderror: 1.787\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 88\n","train_rmse_target: 0.1172 train_rmse_stderror: 0.02758 train_kl_div: 0.02945\n","val_rmse_target: 0.4823 val_rmse_stderror: 1.789\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 96\n","train_rmse_target: 0.09239 train_rmse_stderror: 0.02118 train_kl_div: 0.02118\n","val_rmse_target: 0.4827 val_rmse_stderror: 1.785\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 104\n","train_rmse_target: 0.1076 train_rmse_stderror: 0.02595 train_kl_div: 0.02537\n","val_rmse_target: 0.4824 val_rmse_stderror: 1.783\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.08463 train_rmse_stderror: 0.03105 train_kl_div: 0.01875\n","val_rmse_target: 0.4827 val_rmse_stderror: 1.783\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 120\n","train_rmse_target: 0.07644 train_rmse_stderror: 0.02635 train_kl_div: 0.0147\n","val_rmse_target: 0.4835 val_rmse_stderror: 1.785\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 128\n","train_rmse_target: 0.09197 train_rmse_stderror: 0.0211 train_kl_div: 0.01891\n","val_rmse_target: 0.4841 val_rmse_stderror: 1.786\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 136\n","train_rmse_target: 0.05862 train_rmse_stderror: 0.02541 train_kl_div: 0.01074\n","val_rmse_target: 0.4843 val_rmse_stderror: 1.785\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.06919 train_rmse_stderror: 0.03224 train_kl_div: 0.01322\n","val_rmse_target: 0.4836 val_rmse_stderror: 1.785\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 152\n","train_rmse_target: 0.04472 train_rmse_stderror: 0.01715 train_kl_div: 0.005403\n","val_rmse_target: 0.4832 val_rmse_stderror: 1.785\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 160\n","train_rmse_target: 0.08548 train_rmse_stderror: 0.02346 train_kl_div: 0.01788\n","val_rmse_target: 0.483 val_rmse_stderror: 1.784\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 168\n","train_rmse_target: 0.1045 train_rmse_stderror: 0.0225 train_kl_div: 0.02618\n","val_rmse_target: 0.4829 val_rmse_stderror: 1.784\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1491 train_rmse_stderror: 0.02615 train_kl_div: 0.03487\n","val_rmse_target: 0.4829 val_rmse_stderror: 1.784\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 184\n","train_rmse_target: 0.07033 train_rmse_stderror: 0.01492 train_kl_div: 0.01184\n","val_rmse_target: 0.4829 val_rmse_stderror: 1.784\n","Still best_val_rmse: 0.4807 (from epoch 4)\n","\n","Performance estimates:\n","[0.47334423773166556, 0.4939554494572308, 0.4807271314748883]\n","Mean: 0.4826756062212616\n","\n","Fold 4/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 81.9 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.7513 train_rmse_stderror: 0.05377 train_kl_div: 1.113\n","val_rmse_target: 0.7308 val_rmse_stderror: 1.809\n","New best_val_rmse: 0.7308\n","\n","64 steps took 81.0 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.6933 train_rmse_stderror: 0.03539 train_kl_div: 0.9387\n","val_rmse_target: 0.5415 val_rmse_stderror: 1.784\n","New best_val_rmse: 0.5415\n","\n","32 steps took 40.5 seconds\n","Epoch: 0 batch_num: 160\n","train_rmse_target: 0.5138 train_rmse_stderror: 0.0306 train_kl_div: 0.5689\n","val_rmse_target: 0.5254 val_rmse_stderror: 1.756\n","New best_val_rmse: 0.5254\n","\n","32 steps took 40.7 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.6004 train_rmse_stderror: 0.03453 train_kl_div: 0.66\n","val_rmse_target: 0.5792 val_rmse_stderror: 1.766\n","Still best_val_rmse: 0.5254 (from epoch 0)\n","\n","64 steps took 81.0 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.3655 train_rmse_stderror: 0.02887 train_kl_div: 0.262\n","val_rmse_target: 0.5709 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.5254 (from epoch 0)\n","\n","64 steps took 80.9 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.8317 train_rmse_stderror: 0.05651 train_kl_div: 1.214\n","val_rmse_target: 0.673 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.5254 (from epoch 0)\n","\n","64 steps took 81.2 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.4255 train_rmse_stderror: 0.01855 train_kl_div: 0.3322\n","val_rmse_target: 0.4936 val_rmse_stderror: 1.771\n","New best_val_rmse: 0.4936\n","\n","16 steps took 20.2 seconds\n","Epoch: 2 batch_num: 24\n","train_rmse_target: 0.2777 train_rmse_stderror: 0.0382 train_kl_div: 0.1533\n","val_rmse_target: 0.4938 val_rmse_stderror: 1.762\n","Still best_val_rmse: 0.4936 (from epoch 2)\n","\n","16 steps took 20.2 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.1749 train_rmse_stderror: 0.03578 train_kl_div: 0.07291\n","val_rmse_target: 0.5036 val_rmse_stderror: 1.778\n","Still best_val_rmse: 0.4936 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2198 train_rmse_stderror: 0.03627 train_kl_div: 0.1025\n","val_rmse_target: 0.5007 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4936 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.264 train_rmse_stderror: 0.02018 train_kl_div: 0.141\n","val_rmse_target: 0.4994 val_rmse_stderror: 1.775\n","Still best_val_rmse: 0.4936 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 2 batch_num: 120\n","train_rmse_target: 0.2544 train_rmse_stderror: 0.04049 train_kl_div: 0.1467\n","val_rmse_target: 0.486 val_rmse_stderror: 1.783\n","New best_val_rmse: 0.486\n","\n","8 steps took 10.1 seconds\n","Epoch: 2 batch_num: 128\n","train_rmse_target: 0.4869 train_rmse_stderror: 0.06724 train_kl_div: 0.3596\n","val_rmse_target: 0.4848 val_rmse_stderror: 1.76\n","New best_val_rmse: 0.4848\n","\n","8 steps took 10.1 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.1547 train_rmse_stderror: 0.02034 train_kl_div: 0.05237\n","val_rmse_target: 0.5068 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.4848 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.1382 train_rmse_stderror: 0.03407 train_kl_div: 0.04175\n","val_rmse_target: 0.4948 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4848 (from epoch 2)\n","\n","16 steps took 20.2 seconds\n","Epoch: 2 batch_num: 184\n","train_rmse_target: 0.2208 train_rmse_stderror: 0.03053 train_kl_div: 0.1058\n","val_rmse_target: 0.4876 val_rmse_stderror: 1.757\n","Still best_val_rmse: 0.4848 (from epoch 2)\n","\n","8 steps took 10.3 seconds\n","Epoch: 3 batch_num: 4\n","train_rmse_target: 0.1726 train_rmse_stderror: 0.02753 train_kl_div: 0.05865\n","val_rmse_target: 0.4898 val_rmse_stderror: 1.778\n","Still best_val_rmse: 0.4848 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.1843 train_rmse_stderror: 0.02172 train_kl_div: 0.07352\n","val_rmse_target: 0.488 val_rmse_stderror: 1.773\n","Still best_val_rmse: 0.4848 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 20\n","train_rmse_target: 0.1507 train_rmse_stderror: 0.01619 train_kl_div: 0.0424\n","val_rmse_target: 0.4852 val_rmse_stderror: 1.778\n","Still best_val_rmse: 0.4848 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 28\n","train_rmse_target: 0.2051 train_rmse_stderror: 0.03133 train_kl_div: 0.09236\n","val_rmse_target: 0.4818 val_rmse_stderror: 1.76\n","New best_val_rmse: 0.4818\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 36\n","train_rmse_target: 0.1688 train_rmse_stderror: 0.01717 train_kl_div: 0.05862\n","val_rmse_target: 0.4886 val_rmse_stderror: 1.778\n","Still best_val_rmse: 0.4818 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.1919 train_rmse_stderror: 0.0341 train_kl_div: 0.07983\n","val_rmse_target: 0.4817 val_rmse_stderror: 1.766\n","New best_val_rmse: 0.4817\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 52\n","train_rmse_target: 0.259 train_rmse_stderror: 0.0206 train_kl_div: 0.109\n","val_rmse_target: 0.4957 val_rmse_stderror: 1.776\n","Still best_val_rmse: 0.4817 (from epoch 3)\n","\n","16 steps took 20.2 seconds\n","Epoch: 3 batch_num: 68\n","train_rmse_target: 0.216 train_rmse_stderror: 0.02826 train_kl_div: 0.1003\n","val_rmse_target: 0.4894 val_rmse_stderror: 1.769\n","Still best_val_rmse: 0.4817 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1306 train_rmse_stderror: 0.02458 train_kl_div: 0.04207\n","val_rmse_target: 0.4796 val_rmse_stderror: 1.771\n","New best_val_rmse: 0.4796\n","\n","4 steps took 5.08 seconds\n","Epoch: 3 batch_num: 80\n","train_rmse_target: 0.165 train_rmse_stderror: 0.0243 train_kl_div: 0.05122\n","val_rmse_target: 0.4864 val_rmse_stderror: 1.775\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 88\n","train_rmse_target: 0.09867 train_rmse_stderror: 0.01654 train_kl_div: 0.02121\n","val_rmse_target: 0.4816 val_rmse_stderror: 1.776\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 96\n","train_rmse_target: 0.1856 train_rmse_stderror: 0.02368 train_kl_div: 0.07488\n","val_rmse_target: 0.4916 val_rmse_stderror: 1.763\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","16 steps took 20.2 seconds\n","Epoch: 3 batch_num: 112\n","train_rmse_target: 0.155 train_rmse_stderror: 0.02105 train_kl_div: 0.04968\n","val_rmse_target: 0.4893 val_rmse_stderror: 1.764\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 120\n","train_rmse_target: 0.1986 train_rmse_stderror: 0.02533 train_kl_div: 0.07654\n","val_rmse_target: 0.4817 val_rmse_stderror: 1.777\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 128\n","train_rmse_target: 0.1268 train_rmse_stderror: 0.02259 train_kl_div: 0.03413\n","val_rmse_target: 0.482 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 136\n","train_rmse_target: 0.2656 train_rmse_stderror: 0.03206 train_kl_div: 0.1201\n","val_rmse_target: 0.4888 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 144\n","train_rmse_target: 0.1024 train_rmse_stderror: 0.03495 train_kl_div: 0.02191\n","val_rmse_target: 0.4826 val_rmse_stderror: 1.775\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 152\n","train_rmse_target: 0.2347 train_rmse_stderror: 0.02316 train_kl_div: 0.1154\n","val_rmse_target: 0.4796 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","4 steps took 5.05 seconds\n","Epoch: 3 batch_num: 156\n","train_rmse_target: 0.1709 train_rmse_stderror: 0.02201 train_kl_div: 0.06268\n","val_rmse_target: 0.4853 val_rmse_stderror: 1.777\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 3 batch_num: 164\n","train_rmse_target: 0.1736 train_rmse_stderror: 0.02576 train_kl_div: 0.05485\n","val_rmse_target: 0.4912 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","16 steps took 20.2 seconds\n","Epoch: 3 batch_num: 180\n","train_rmse_target: 0.1327 train_rmse_stderror: 0.02859 train_kl_div: 0.0371\n","val_rmse_target: 0.4897 val_rmse_stderror: 1.775\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.3 seconds\n","Epoch: 4 batch_num: 0\n","train_rmse_target: 0.1166 train_rmse_stderror: 0.01613 train_kl_div: 0.02891\n","val_rmse_target: 0.4893 val_rmse_stderror: 1.771\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 8\n","train_rmse_target: 0.1135 train_rmse_stderror: 0.02534 train_kl_div: 0.02815\n","val_rmse_target: 0.4822 val_rmse_stderror: 1.771\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1382 train_rmse_stderror: 0.026 train_kl_div: 0.03773\n","val_rmse_target: 0.4825 val_rmse_stderror: 1.771\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 24\n","train_rmse_target: 0.1121 train_rmse_stderror: 0.02618 train_kl_div: 0.02898\n","val_rmse_target: 0.4852 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 32\n","train_rmse_target: 0.1435 train_rmse_stderror: 0.0196 train_kl_div: 0.04481\n","val_rmse_target: 0.4834 val_rmse_stderror: 1.771\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 40\n","train_rmse_target: 0.1188 train_rmse_stderror: 0.02605 train_kl_div: 0.02895\n","val_rmse_target: 0.4845 val_rmse_stderror: 1.774\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1106 train_rmse_stderror: 0.02395 train_kl_div: 0.02517\n","val_rmse_target: 0.4836 val_rmse_stderror: 1.774\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 56\n","train_rmse_target: 0.1131 train_rmse_stderror: 0.01653 train_kl_div: 0.02995\n","val_rmse_target: 0.4823 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 64\n","train_rmse_target: 0.09748 train_rmse_stderror: 0.02871 train_kl_div: 0.02064\n","val_rmse_target: 0.4827 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 72\n","train_rmse_target: 0.09152 train_rmse_stderror: 0.02259 train_kl_div: 0.02106\n","val_rmse_target: 0.4855 val_rmse_stderror: 1.769\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.06694 train_rmse_stderror: 0.03303 train_kl_div: 0.01395\n","val_rmse_target: 0.4853 val_rmse_stderror: 1.773\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 88\n","train_rmse_target: 0.1023 train_rmse_stderror: 0.02635 train_kl_div: 0.02438\n","val_rmse_target: 0.4844 val_rmse_stderror: 1.773\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 96\n","train_rmse_target: 0.1096 train_rmse_stderror: 0.01665 train_kl_div: 0.02491\n","val_rmse_target: 0.4839 val_rmse_stderror: 1.775\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 104\n","train_rmse_target: 0.08694 train_rmse_stderror: 0.02149 train_kl_div: 0.01856\n","val_rmse_target: 0.483 val_rmse_stderror: 1.775\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.092 train_rmse_stderror: 0.01355 train_kl_div: 0.01967\n","val_rmse_target: 0.4827 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 120\n","train_rmse_target: 0.1194 train_rmse_stderror: 0.01751 train_kl_div: 0.02886\n","val_rmse_target: 0.4837 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 128\n","train_rmse_target: 0.08864 train_rmse_stderror: 0.03549 train_kl_div: 0.01502\n","val_rmse_target: 0.4835 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 136\n","train_rmse_target: 0.1185 train_rmse_stderror: 0.02232 train_kl_div: 0.03291\n","val_rmse_target: 0.4828 val_rmse_stderror: 1.771\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.1541 train_rmse_stderror: 0.03028 train_kl_div: 0.04098\n","val_rmse_target: 0.4826 val_rmse_stderror: 1.771\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 152\n","train_rmse_target: 0.1065 train_rmse_stderror: 0.02483 train_kl_div: 0.02682\n","val_rmse_target: 0.4829 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 160\n","train_rmse_target: 0.09488 train_rmse_stderror: 0.0147 train_kl_div: 0.01906\n","val_rmse_target: 0.4831 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 168\n","train_rmse_target: 0.08459 train_rmse_stderror: 0.0183 train_kl_div: 0.01659\n","val_rmse_target: 0.4831 val_rmse_stderror: 1.772\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.08342 train_rmse_stderror: 0.02693 train_kl_div: 0.01759\n","val_rmse_target: 0.4831 val_rmse_stderror: 1.771\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 184\n","train_rmse_target: 0.1485 train_rmse_stderror: 0.01681 train_kl_div: 0.04644\n","val_rmse_target: 0.4831 val_rmse_stderror: 1.771\n","Still best_val_rmse: 0.4796 (from epoch 3)\n","\n","Performance estimates:\n","[0.47334423773166556, 0.4939554494572308, 0.4807271314748883, 0.47962187842547155]\n","Mean: 0.4819121742723141\n","\n","Fold 5/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 81.9 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.7705 train_rmse_stderror: 0.0358 train_kl_div: 1.182\n","val_rmse_target: 0.8206 val_rmse_stderror: 1.755\n","New best_val_rmse: 0.8206\n","\n","64 steps took 80.9 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.6536 train_rmse_stderror: 0.05638 train_kl_div: 0.8413\n","val_rmse_target: 0.5882 val_rmse_stderror: 1.759\n","New best_val_rmse: 0.5882\n","\n","64 steps took 81.2 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.4795 train_rmse_stderror: 0.02386 train_kl_div: 0.5008\n","val_rmse_target: 0.7031 val_rmse_stderror: 1.752\n","Still best_val_rmse: 0.5882 (from epoch 0)\n","\n","64 steps took 80.9 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.5323 train_rmse_stderror: 0.05134 train_kl_div: 0.5463\n","val_rmse_target: 0.5313 val_rmse_stderror: 1.759\n","New best_val_rmse: 0.5313\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.3921 train_rmse_stderror: 0.03418 train_kl_div: 0.3119\n","val_rmse_target: 0.5775 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.5313 (from epoch 1)\n","\n","64 steps took 80.9 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.5267 train_rmse_stderror: 0.02343 train_kl_div: 0.5746\n","val_rmse_target: 0.5431 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.5313 (from epoch 1)\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.3664 train_rmse_stderror: 0.04894 train_kl_div: 0.2393\n","val_rmse_target: 0.5268 val_rmse_stderror: 1.743\n","New best_val_rmse: 0.5268\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2827 train_rmse_stderror: 0.02685 train_kl_div: 0.1708\n","val_rmse_target: 0.5188 val_rmse_stderror: 1.756\n","New best_val_rmse: 0.5188\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2204 train_rmse_stderror: 0.0339 train_kl_div: 0.1075\n","val_rmse_target: 0.5162 val_rmse_stderror: 1.765\n","New best_val_rmse: 0.5162\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.3144 train_rmse_stderror: 0.02228 train_kl_div: 0.2176\n","val_rmse_target: 0.5157 val_rmse_stderror: 1.758\n","New best_val_rmse: 0.5157\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.2431 train_rmse_stderror: 0.03297 train_kl_div: 0.1019\n","val_rmse_target: 0.5189 val_rmse_stderror: 1.757\n","Still best_val_rmse: 0.5157 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.2796 train_rmse_stderror: 0.02672 train_kl_div: 0.1678\n","val_rmse_target: 0.5053 val_rmse_stderror: 1.761\n","New best_val_rmse: 0.5053\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.1709 train_rmse_stderror: 0.03017 train_kl_div: 0.0533\n","val_rmse_target: 0.5124 val_rmse_stderror: 1.764\n","Still best_val_rmse: 0.5053 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.09844 train_rmse_stderror: 0.01587 train_kl_div: 0.02096\n","val_rmse_target: 0.5098 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.5053 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1349 train_rmse_stderror: 0.02565 train_kl_div: 0.03505\n","val_rmse_target: 0.5184 val_rmse_stderror: 1.762\n","Still best_val_rmse: 0.5053 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.09581 train_rmse_stderror: 0.0302 train_kl_div: 0.02405\n","val_rmse_target: 0.5 val_rmse_stderror: 1.755\n","New best_val_rmse: 0.5\n","\n","16 steps took 20.2 seconds\n","Epoch: 3 batch_num: 124\n","train_rmse_target: 0.1263 train_rmse_stderror: 0.0192 train_kl_div: 0.03232\n","val_rmse_target: 0.5084 val_rmse_stderror: 1.751\n","Still best_val_rmse: 0.5 (from epoch 3)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 156\n","train_rmse_target: 0.1077 train_rmse_stderror: 0.01258 train_kl_div: 0.02485\n","val_rmse_target: 0.508 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.5 (from epoch 3)\n","\n","32 steps took 40.7 seconds\n","Epoch: 4 batch_num: 0\n","train_rmse_target: 0.1216 train_rmse_stderror: 0.02285 train_kl_div: 0.02623\n","val_rmse_target: 0.5015 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.5 (from epoch 3)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 32\n","train_rmse_target: 0.08026 train_rmse_stderror: 0.02048 train_kl_div: 0.01574\n","val_rmse_target: 0.5 val_rmse_stderror: 1.756\n","Still best_val_rmse: 0.5 (from epoch 3)\n","\n","16 steps took 20.2 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.09657 train_rmse_stderror: 0.02078 train_kl_div: 0.02134\n","val_rmse_target: 0.5024 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.5 (from epoch 3)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.08491 train_rmse_stderror: 0.01681 train_kl_div: 0.01548\n","val_rmse_target: 0.5012 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.5 (from epoch 3)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.06553 train_rmse_stderror: 0.0254 train_kl_div: 0.01093\n","val_rmse_target: 0.5017 val_rmse_stderror: 1.756\n","Still best_val_rmse: 0.5 (from epoch 3)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.09471 train_rmse_stderror: 0.01627 train_kl_div: 0.01924\n","val_rmse_target: 0.5022 val_rmse_stderror: 1.757\n","Still best_val_rmse: 0.5 (from epoch 3)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1554 train_rmse_stderror: 0.02419 train_kl_div: 0.04096\n","val_rmse_target: 0.5023 val_rmse_stderror: 1.757\n","Still best_val_rmse: 0.5 (from epoch 3)\n","\n","Performance estimates:\n","[0.47334423773166556, 0.4939554494572308, 0.4807271314748883, 0.47962187842547155, 0.4999566012225916]\n","Mean: 0.4855210596623696\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m4v-cGx-Mv7S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626839345560,"user_tz":-540,"elapsed":33,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"94c56638-a693-4599-8fc2-ac47d978ec00"},"source":["print(list_val_rmse)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[0.47334423773166556, 0.4939554494572308, 0.4807271314748883, 0.47962187842547155, 0.4999566012225916]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q2CdCMuIKDMP","executionInfo":{"status":"ok","timestamp":1626839345561,"user_tz":-540,"elapsed":21,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["#rep = MemReporter(model)\n","#rep.report()"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLl1yDOOKIe7","executionInfo":{"status":"ok","timestamp":1626839345562,"user_tz":-540,"elapsed":21,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["#rep = MemReporter(model.roberta)\n","#rep.report()"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qkqnknA_m9D","executionInfo":{"status":"ok","timestamp":1626839345562,"user_tz":-540,"elapsed":20,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["#gpuinfo()"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwrqSMdYA6Pu","executionInfo":{"status":"ok","timestamp":1626839345563,"user_tz":-540,"elapsed":20,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["#del model\n","#del optimizer \n","#del train_loader\n","#del val_loader\n","#del scheduler \n","#del list_val_rmse\n","#del train_indices\n","#del val_indices\n","#del tokenizer\n","#torch.cuda.empty_cache()\n","#gpuinfo()"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wXcHyUSJXecL"},"source":["# upload models"]},{"cell_type":"code","metadata":{"id":"YIV6UllSIGoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626839460371,"user_tz":-540,"elapsed":114827,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"f8ee0fd2-0da5-4724-e711-4126e89db5c7"},"source":["%cd\n","!mkdir .kaggle\n","!mkdir /content/model\n","!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\n","\n","!cp -r /content/model_1.pth /content/model/model_1.pth\n","!cp -r /content/model_2.pth /content/model/model_2.pth\n","!cp -r /content/model_3.pth /content/model/model_3.pth\n","!cp -r /content/model_4.pth /content/model/model_4.pth\n","!cp -r /content/model_5.pth /content/model/model_5.pth"],"execution_count":32,"outputs":[{"output_type":"stream","text":["/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"14ddOZH4IMam","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626839589739,"user_tz":-540,"elapsed":129384,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"606897d2-27bf-49f9-fc14-8712e1eb4a84"},"source":["def dataset_upload():\n","    import json\n","    from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","    id = f'{USERID}/{EX_NO}'\n","\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = id\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = f'{EX_NO}'\n","\n","    with open(UPLOAD_DIR / 'dataset-metadata.json', 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","\n","    api = KaggleApi()\n","    api.authenticate()\n","\n","    # データセットがない場合\n","    if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n","        api.dataset_create_new(folder=UPLOAD_DIR,\n","                               convert_to_csv=False,\n","                               dir_mode='skip')\n","    # データセットがある場合\n","    else:\n","        api.dataset_create_version(folder=UPLOAD_DIR,\n","                                   version_notes='update',\n","                                   convert_to_csv=False,\n","                                   delete_old_versions=True,\n","                                   dir_mode='skip')\n","dataset_upload()\n","\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Starting upload for file model_2.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:24<00:00, 57.0MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_2.pth (1GB)\n","Starting upload for file model_4.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:25<00:00, 55.0MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_4.pth (1GB)\n","Starting upload for file model_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:25<00:00, 56.2MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_1.pth (1GB)\n","Starting upload for file model_5.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:24<00:00, 57.3MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_5.pth (1GB)\n","Starting upload for file model_3.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:24<00:00, 58.9MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_3.pth (1GB)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"huJwVMSAPuDO","executionInfo":{"status":"ok","timestamp":1626839589740,"user_tz":-540,"elapsed":23,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zzuBPobmLFu","executionInfo":{"status":"ok","timestamp":1626839589740,"user_tz":-540,"elapsed":22,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wpc8ro9hmNci","executionInfo":{"status":"ok","timestamp":1626839589741,"user_tz":-540,"elapsed":22,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"ceDI72NumT5-","executionInfo":{"status":"ok","timestamp":1626839589741,"user_tz":-540,"elapsed":22,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvRi_JQgwcKI","executionInfo":{"status":"ok","timestamp":1626839589741,"user_tz":-540,"elapsed":21,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":33,"outputs":[]}]}