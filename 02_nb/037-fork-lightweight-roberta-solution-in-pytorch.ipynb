{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conservative-tonight",
   "metadata": {
    "papermill": {
     "duration": 0.017564,
     "end_time": "2021-07-02T16:32:58.358460",
     "exception": false,
     "start_time": "2021-07-02T16:32:58.340896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "The goal is to see the performance of a minimalistic \"roberta-base\" solution, with no pre-training or post-processing.\n",
    "\n",
    "Acknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faced-champion",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-07-02T16:32:58.402241Z",
     "iopub.status.busy": "2021-07-02T16:32:58.401569Z",
     "iopub.status.idle": "2021-07-02T16:33:06.095779Z",
     "shell.execute_reply": "2021-07-02T16:33:06.094810Z",
     "shell.execute_reply.started": "2021-07-02T16:03:06.925822Z"
    },
    "papermill": {
     "duration": 7.721406,
     "end_time": "2021-07-02T16:33:06.095942",
     "exception": false,
     "start_time": "2021-07-02T16:32:58.374536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AdamW # optimizer?\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup # scheduler?\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "casual-friday",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.177417Z",
     "iopub.status.busy": "2021-07-02T16:33:06.176408Z",
     "iopub.status.idle": "2021-07-02T16:33:06.179061Z",
     "shell.execute_reply": "2021-07-02T16:33:06.178642Z",
     "shell.execute_reply.started": "2021-07-02T16:03:14.952052Z"
    },
    "papermill": {
     "duration": 0.066912,
     "end_time": "2021-07-02T16:33:06.179191",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.112279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5 # K Fold\n",
    "NUM_EPOCHS = 3 # Epochs\n",
    "BATCH_SIZE = 16 # Batch Size\n",
    "MAX_LEN = 248 # ベクトル長?\n",
    "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)] # schedulerの何らかの設定？\n",
    "ROBERTA_PATH = \"/kaggle/input/roberta-base\" # roberta pre-trainedモデル(モデルとして指定)\n",
    "TOKENIZER_PATH = \"/kaggle/input/roberta-base\" # roberta pre-trainedモデル(Tokenizerとして指定)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # cudaがなければcpuを使えばいいじゃない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-check",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.217749Z",
     "iopub.status.busy": "2021-07-02T16:33:06.216872Z",
     "iopub.status.idle": "2021-07-02T16:33:06.219180Z",
     "shell.execute_reply": "2021-07-02T16:33:06.219603Z",
     "shell.execute_reply.started": "2021-07-02T16:03:15.003642Z"
    },
    "papermill": {
     "duration": 0.02413,
     "end_time": "2021-07-02T16:33:06.219742",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.195612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seed(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True# cudnnによる最適化で結果が変わらないためのおまじない "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "developing-secret",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.258462Z",
     "iopub.status.busy": "2021-07-02T16:33:06.257911Z",
     "iopub.status.idle": "2021-07-02T16:33:06.368134Z",
     "shell.execute_reply": "2021-07-02T16:33:06.367555Z",
     "shell.execute_reply.started": "2021-07-02T16:03:15.012562Z"
    },
    "papermill": {
     "duration": 0.132439,
     "end_time": "2021-07-02T16:33:06.368261",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.235822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train, testを読む\n",
    "train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n",
    "\n",
    "# Remove incomplete entries if any.\n",
    "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
    "              inplace=True)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reduced-employee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.404459Z",
     "iopub.status.busy": "2021-07-02T16:33:06.403935Z",
     "iopub.status.idle": "2021-07-02T16:33:06.602827Z",
     "shell.execute_reply": "2021-07-02T16:33:06.601908Z",
     "shell.execute_reply.started": "2021-07-02T16:03:15.132675Z"
    },
    "papermill": {
     "duration": 0.218213,
     "end_time": "2021-07-02T16:33:06.602969",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.384756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizerを指定\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-multiple",
   "metadata": {
    "papermill": {
     "duration": 0.016158,
     "end_time": "2021-07-02T16:33:06.635718",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.619560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wrapped-speaking",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.677754Z",
     "iopub.status.busy": "2021-07-02T16:33:06.676754Z",
     "iopub.status.idle": "2021-07-02T16:33:06.679592Z",
     "shell.execute_reply": "2021-07-02T16:33:06.679159Z",
     "shell.execute_reply.started": "2021-07-02T16:03:15.354405Z"
    },
    "papermill": {
     "duration": 0.02753,
     "end_time": "2021-07-02T16:33:06.679708",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.652178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset用のClass。おそらく、trainとtestでインスタンスを生成し、DataFrameと同じように扱えるような思想。\n",
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only # Testデータ用フラグ\n",
    "        self.text = df.excerpt.tolist() # 分析対象カラムをlistにする。(分かち書きではなく、Seriesをlistへ変換するような処理)\n",
    "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text] # 単語単位で分かち書きする場合\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32) # trainのみ、targetをtensorに変換\n",
    "    \n",
    "        self.encoded = tokenizer.batch_encode_plus( # textをtokenize\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True, # 最大長を超える文字は切り捨て\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index): # 変換結果を返す\n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-arkansas",
   "metadata": {
    "papermill": {
     "duration": 0.015729,
     "end_time": "2021-07-02T16:33:06.711876",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.696147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "studied-credits",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.752855Z",
     "iopub.status.busy": "2021-07-02T16:33:06.752102Z",
     "iopub.status.idle": "2021-07-02T16:33:06.754526Z",
     "shell.execute_reply": "2021-07-02T16:33:06.754940Z",
     "shell.execute_reply.started": "2021-07-02T16:03:15.366660Z"
    },
    "papermill": {
     "duration": 0.02678,
     "end_time": "2021-07-02T16:33:06.755067",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.728287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH) # pretrainedからconfigを読み込み\n",
    "        config.update({\"output_hidden_states\":True, # config更新: embedding層を抽出\n",
    "                       \"hidden_dropout_prob\": 0.0, # config更新: dropoutしない\n",
    "                       \"layer_norm_eps\": 1e-7}) # config更新: layer normalizationのepsilon                      \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(# attentionレイヤー            \n",
    "            nn.Linear(768, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential( # 出力レイヤー                    \n",
    "            nn.Linear(768, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids, # robertaに入力データを流し、出力としてrobertaモデル(layerの複合体)を得る\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1] # robertaモデルの最後のlayerを得る\n",
    "\n",
    "        # The number of cells is MAX_LEN.\n",
    "        # The size of the hidden state of each cell is 768 (for roberta-base).\n",
    "        # In order to condense hidden states of all cells to a context vector,\n",
    "        # we compute a weighted average of the hidden states of all cells.\n",
    "        # We compute the weight of each cell, using the attention neural network.\n",
    "        weights = self.attention(last_layer_hidden_states) # robertaの最後のlayerをattentionへ入力し、出力として重みを得る\n",
    "                \n",
    "        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
    "        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
    "        # Now we compute context_vector as the weighted average.\n",
    "        # context_vector.shape is BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) # 重み×最後の層を足し合わせて文書ベクトルとする。\n",
    "        \n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector) # 文書ベクトルを線形層に入力し、targetを出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continuing-hamilton",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.794263Z",
     "iopub.status.busy": "2021-07-02T16:33:06.793471Z",
     "iopub.status.idle": "2021-07-02T16:33:06.796133Z",
     "shell.execute_reply": "2021-07-02T16:33:06.795674Z",
     "shell.execute_reply.started": "2021-07-02T16:03:15.382050Z"
    },
    "papermill": {
     "duration": 0.025005,
     "end_time": "2021-07-02T16:33:06.796275",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.771270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 評価指標(MSE)の計算。最終的に、ルートしてRMSEにすると思われる。\n",
    "def eval_mse(model, data_loader):\n",
    "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
    "    model.eval() # evalモードを選択。Batch Normとかdropoutをしなくなる           \n",
    "    mse_sum = 0\n",
    "\n",
    "    with torch.no_grad(): # 勾配の計算をしないBlock\n",
    "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader): # data_loaderからinput, attentin_mask, targetをbatchごとに取り出す\n",
    "            input_ids = input_ids.to(DEVICE) # 取り出した値をdeviceへブッこむ(下2行も同様)\n",
    "            attention_mask = attention_mask.to(DEVICE)                        \n",
    "            target = target.to(DEVICE)           \n",
    "            \n",
    "            pred = model(input_ids, attention_mask) # 取得した値をモデルへ入力し、出力として予測値を得る。\n",
    "\n",
    "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n",
    "                \n",
    "\n",
    "    return mse_sum / len(data_loader.dataset) # 誤差の合計をdataset長で除し、mseを取得＆返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "domestic-chart",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.834806Z",
     "iopub.status.busy": "2021-07-02T16:33:06.834011Z",
     "iopub.status.idle": "2021-07-02T16:33:06.836304Z",
     "shell.execute_reply": "2021-07-02T16:33:06.836807Z",
     "shell.execute_reply.started": "2021-07-02T16:03:15.395986Z"
    },
    "papermill": {
     "duration": 0.0246,
     "end_time": "2021-07-02T16:33:06.836951",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.812351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 推論結果を返す\n",
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval() # evalモード(dropout, batch_normしない)\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset)) # 結果をdataset長のzero配列として用意\n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad(): # 勾配の計算をしないblock(inputすると、現状の重みによる推論結果を返す)\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader): # data_loaderからbatchごとにinputを得る\n",
    "            input_ids = input_ids.to(DEVICE) # 得たinputをDEVICEへブッこむ(下の行も同様)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask) # modelにinputを入力し、予測結果を得る。\n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\") # result[index ~ predの長さ]へ、予測結果を格納\n",
    "            index += pred.shape[0] # indexを更新\n",
    "\n",
    "    return result # 全batchで推論が終わったら、結果を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "polished-straight",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.880830Z",
     "iopub.status.busy": "2021-07-02T16:33:06.880010Z",
     "iopub.status.idle": "2021-07-02T16:33:06.882673Z",
     "shell.execute_reply": "2021-07-02T16:33:06.882253Z",
     "shell.execute_reply.started": "2021-07-02T16:03:15.407355Z"
    },
    "papermill": {
     "duration": 0.029961,
     "end_time": "2021-07-02T16:33:06.882785",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.852824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 学習\n",
    "def train(model, # モデル\n",
    "          model_path, # モデルのパス、pre_trainedモデルのこと？\n",
    "          train_loader, # train-setのdata_loader?\n",
    "          val_loader, # valid-setのdata_loader?\n",
    "          optimizer, # optimizer\n",
    "          scheduler=None, # scheduler, デフォルトはNone\n",
    "          num_epochs=NUM_EPOCHS # epoch数、notebook冒頭で指定した値\n",
    "         ):    \n",
    "    \n",
    "    best_val_rmse = None\n",
    "    best_epoch = 0\n",
    "    step = 0\n",
    "    last_eval_step = 0\n",
    "    eval_period = EVAL_SCHEDULE[0][1] # eval期間(って何？) 冒頭で決めたEVAL_SCHEDULEの最初のtupleの[1]を取得\n",
    "\n",
    "    start = time.time() # 時間計測用\n",
    "\n",
    "    for epoch in range(num_epochs): # 指定したEpoch数だけ繰り返し\n",
    "        val_rmse = None         \n",
    "\n",
    "        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader): # train_loaderからinput, targetを取得\n",
    "            input_ids = input_ids.to(DEVICE) # inputをDEVICEへ突っ込む\n",
    "            attention_mask = attention_mask.to(DEVICE)            \n",
    "            target = target.to(DEVICE)                        \n",
    "\n",
    "            optimizer.zero_grad() # 勾配を初期化\n",
    "            \n",
    "            model.train() # 学習モード開始\n",
    "\n",
    "            pred = model(input_ids, attention_mask) # input,attention_maskを入力し、予測結果を得る\n",
    "                                                        \n",
    "            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target) # 予測結果からMSEを得る(これが損失関数となる?)\n",
    "                        \n",
    "            mse.backward() # 誤差逆伝播法により勾配を得る\n",
    "\n",
    "            optimizer.step() # 重みの更新\n",
    "            if scheduler:\n",
    "                scheduler.step() # schedulerが与えられた場合は、schedulerの学習率更新\n",
    "            \n",
    "            if step >= last_eval_step + eval_period: # batchを回すごとにstepを増やしていって、「前回evalしたstep + eval_period(16)」を超えたら実行。\n",
    "                # Evaluate the model on val_loader.\n",
    "                elapsed_seconds = time.time() - start # 経過時間\n",
    "                num_steps = step - last_eval_step # 経過ステップ数\n",
    "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
    "                last_eval_step = step # 前回stepの更新\n",
    "                \n",
    "                val_rmse = math.sqrt(eval_mse(model, val_loader)) # valid-setによるrmse計算                           \n",
    "\n",
    "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
    "                      f\"val_rmse: {val_rmse:0.4}\") # epoch, batch, score\n",
    "\n",
    "                for rmse, period in EVAL_SCHEDULE: # eval_periodをvalid-rmseで切り替える処理\n",
    "                    if val_rmse >= rmse: # validのrmseを次の値と比較し、validが大きい場合にbreak : EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
    "                        eval_period = period # eval_periodを更新\n",
    "                        break                               \n",
    "                \n",
    "                if not best_val_rmse or val_rmse < best_val_rmse: # 初回(best_val_rmse==None), またはbest_val_rmseを更新したらモデルを保存する\n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_epoch = epoch\n",
    "                    torch.save(model.state_dict(), model_path) # 最高の自分を保存\n",
    "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
    "                else:       \n",
    "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\", # 更新されない場合は、元のスコアを表示\n",
    "                          f\"(from epoch {best_epoch})\")                                    \n",
    "                    \n",
    "                start = time.time()\n",
    "                                            \n",
    "            step += 1\n",
    "                        \n",
    "    \n",
    "    return best_val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ethical-management",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:06.923038Z",
     "iopub.status.busy": "2021-07-02T16:33:06.922288Z",
     "iopub.status.idle": "2021-07-02T16:33:06.924524Z",
     "shell.execute_reply": "2021-07-02T16:33:06.924954Z",
     "shell.execute_reply.started": "2021-07-02T16:07:52.611537Z"
    },
    "papermill": {
     "duration": 0.026038,
     "end_time": "2021-07-02T16:33:06.925086",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.899048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizerの作成\n",
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters()) # モデルパラメータの取得\n",
    "    \n",
    "    roberta_parameters = named_parameters[:197] # パラメータをroberta用、attention用、regressor用に格納。(どうやって決めてるのだろう？)\n",
    "    attention_parameters = named_parameters[199:203]\n",
    "    regressor_parameters = named_parameters[203:]\n",
    "        \n",
    "    attention_group = [params for (name, params) in attention_parameters] # attention用パラメータをリストとして取得\n",
    "    regressor_group = [params for (name, params) in regressor_parameters] # reg用パラメータをリストとして取得\n",
    "\n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group}) # パラメータをリストに辞書として格納していく\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters): # レイヤーごとにname, paramsを取得していろんな処理\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "\n",
    "        lr = 2e-5\n",
    "\n",
    "        if layer_num >= 69:        \n",
    "            lr = 5e-5\n",
    "\n",
    "        if layer_num >= 133:\n",
    "            lr = 1e-4\n",
    "\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "\n",
    "    return AdamW(parameters) # 最終的に、AdamWにパラメータを入力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nasty-rachel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T16:33:07.132826Z",
     "iopub.status.busy": "2021-07-02T16:33:07.132159Z",
     "iopub.status.idle": "2021-07-02T17:18:47.936839Z",
     "shell.execute_reply": "2021-07-02T17:18:47.937255Z",
     "shell.execute_reply.started": "2021-07-02T16:14:59.604357Z"
    },
    "papermill": {
     "duration": 2740.99631,
     "end_time": "2021-07-02T17:18:47.937457",
     "exception": false,
     "start_time": "2021-07-02T16:33:06.941147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "\n",
      "16 steps took 7.95 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.966\n",
      "New best_val_rmse: 0.966\n",
      "\n",
      "16 steps took 6.45 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.692\n",
      "New best_val_rmse: 0.692\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6652\n",
      "New best_val_rmse: 0.6652\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6079\n",
      "New best_val_rmse: 0.6079\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5878\n",
      "New best_val_rmse: 0.5878\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.6618\n",
      "Still best_val_rmse: 0.5878 (from epoch 0)\n",
      "\n",
      "16 steps took 6.51 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5262\n",
      "New best_val_rmse: 0.5262\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.6079\n",
      "Still best_val_rmse: 0.5262 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 3 val_rmse: 0.5914\n",
      "Still best_val_rmse: 0.5262 (from epoch 0)\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 1 batch_num: 19 val_rmse: 0.5251\n",
      "New best_val_rmse: 0.5251\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 1 batch_num: 35 val_rmse: 0.5073\n",
      "New best_val_rmse: 0.5073\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 1 batch_num: 51 val_rmse: 0.5014\n",
      "New best_val_rmse: 0.5014\n",
      "\n",
      "16 steps took 6.46 seconds\n",
      "Epoch: 1 batch_num: 67 val_rmse: 0.499\n",
      "New best_val_rmse: 0.499\n",
      "\n",
      "8 steps took 3.24 seconds\n",
      "Epoch: 1 batch_num: 75 val_rmse: 0.494\n",
      "New best_val_rmse: 0.494\n",
      "\n",
      "8 steps took 3.26 seconds\n",
      "Epoch: 1 batch_num: 83 val_rmse: 0.4827\n",
      "New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 1 batch_num: 87 val_rmse: 0.5356\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "16 steps took 6.52 seconds\n",
      "Epoch: 1 batch_num: 103 val_rmse: 0.4999\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 3.28 seconds\n",
      "Epoch: 1 batch_num: 111 val_rmse: 0.4928\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 3.23 seconds\n",
      "Epoch: 1 batch_num: 119 val_rmse: 0.4889\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 1 batch_num: 123 val_rmse: 0.4962\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 3.24 seconds\n",
      "Epoch: 1 batch_num: 131 val_rmse: 0.4884\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 1 batch_num: 135 val_rmse: 0.4937\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 3.45 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4846\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4944\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "8 steps took 3.24 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.5059\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "16 steps took 6.46 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4865\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4829\n",
      "Still best_val_rmse: 0.4827 (from epoch 1)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4803\n",
      "New best_val_rmse: 0.4803\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4822\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4882\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.487\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.4832\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.4828\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4838\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.481\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4805\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.484\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4872\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4903\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "8 steps took 3.23 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4849\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4815\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4817\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4837\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4859\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4884\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4898\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4895\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4889\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4879\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4875\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4873\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4872\n",
      "Still best_val_rmse: 0.4803 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.4802685083486929]\n",
      "Mean: 0.4802685083486929\n",
      "\n",
      "Fold 2/5\n",
      "\n",
      "16 steps took 7.01 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9493\n",
      "New best_val_rmse: 0.9493\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8225\n",
      "New best_val_rmse: 0.8225\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6549\n",
      "New best_val_rmse: 0.6549\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6579\n",
      "Still best_val_rmse: 0.6549 (from epoch 0)\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.5935\n",
      "New best_val_rmse: 0.5935\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5315\n",
      "New best_val_rmse: 0.5315\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.7639\n",
      "Still best_val_rmse: 0.5315 (from epoch 0)\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5464\n",
      "Still best_val_rmse: 0.5315 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 3 val_rmse: 0.5195\n",
      "New best_val_rmse: 0.5195\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 1 batch_num: 19 val_rmse: 0.606\n",
      "Still best_val_rmse: 0.5195 (from epoch 1)\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 1 batch_num: 35 val_rmse: 0.5125\n",
      "New best_val_rmse: 0.5125\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 1 batch_num: 51 val_rmse: 0.5108\n",
      "New best_val_rmse: 0.5108\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 1 batch_num: 67 val_rmse: 0.5108\n",
      "New best_val_rmse: 0.5108\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 1 batch_num: 83 val_rmse: 0.5054\n",
      "New best_val_rmse: 0.5054\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 1 batch_num: 99 val_rmse: 0.4907\n",
      "New best_val_rmse: 0.4907\n",
      "\n",
      "8 steps took 3.25 seconds\n",
      "Epoch: 1 batch_num: 107 val_rmse: 0.4855\n",
      "New best_val_rmse: 0.4855\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 1 batch_num: 111 val_rmse: 0.4846\n",
      "New best_val_rmse: 0.4846\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 1 batch_num: 115 val_rmse: 0.4891\n",
      "Still best_val_rmse: 0.4846 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 1 batch_num: 119 val_rmse: 0.4798\n",
      "New best_val_rmse: 0.4798\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 1 batch_num: 121 val_rmse: 0.4862\n",
      "Still best_val_rmse: 0.4798 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 1 batch_num: 125 val_rmse: 0.4854\n",
      "Still best_val_rmse: 0.4798 (from epoch 1)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 1 batch_num: 129 val_rmse: 0.4739\n",
      "New best_val_rmse: 0.4739\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 1 batch_num: 131 val_rmse: 0.4739\n",
      "New best_val_rmse: 0.4739\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 1 batch_num: 133 val_rmse: 0.4732\n",
      "New best_val_rmse: 0.4732\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 1 batch_num: 135 val_rmse: 0.4742\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "2 steps took 0.812 seconds\n",
      "Epoch: 1 batch_num: 137 val_rmse: 0.4803\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "4 steps took 1.73 seconds\n",
      "Epoch: 2 batch_num: 0 val_rmse: 0.4777\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "2 steps took 0.832 seconds\n",
      "Epoch: 2 batch_num: 2 val_rmse: 0.4867\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4761\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "2 steps took 0.807 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4853\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4756\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "2 steps took 0.809 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4756\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "2 steps took 0.812 seconds\n",
      "Epoch: 2 batch_num: 16 val_rmse: 0.4764\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "2 steps took 0.811 seconds\n",
      "Epoch: 2 batch_num: 18 val_rmse: 0.49\n",
      "Still best_val_rmse: 0.4732 (from epoch 1)\n",
      "\n",
      "8 steps took 3.23 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.4668\n",
      "New best_val_rmse: 0.4668\n",
      "\n",
      "1 steps took 0.42 seconds\n",
      "Epoch: 2 batch_num: 27 val_rmse: 0.4667\n",
      "New best_val_rmse: 0.4667\n",
      "\n",
      "1 steps took 0.411 seconds\n",
      "Epoch: 2 batch_num: 28 val_rmse: 0.4669\n",
      "Still best_val_rmse: 0.4667 (from epoch 2)\n",
      "\n",
      "1 steps took 0.406 seconds\n",
      "Epoch: 2 batch_num: 29 val_rmse: 0.4666\n",
      "New best_val_rmse: 0.4666\n",
      "\n",
      "1 steps took 0.414 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4677\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 31 val_rmse: 0.4712\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "2 steps took 0.807 seconds\n",
      "Epoch: 2 batch_num: 33 val_rmse: 0.4774\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 35 val_rmse: 0.4911\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "8 steps took 3.24 seconds\n",
      "Epoch: 2 batch_num: 43 val_rmse: 0.467\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4666\n",
      "New best_val_rmse: 0.4666\n",
      "\n",
      "1 steps took 0.407 seconds\n",
      "Epoch: 2 batch_num: 45 val_rmse: 0.4666\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4668\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.41 seconds\n",
      "Epoch: 2 batch_num: 47 val_rmse: 0.4671\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4685\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.437 seconds\n",
      "Epoch: 2 batch_num: 49 val_rmse: 0.469\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4697\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "1 steps took 0.403 seconds\n",
      "Epoch: 2 batch_num: 51 val_rmse: 0.4705\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 53 val_rmse: 0.4762\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 2 batch_num: 55 val_rmse: 0.4817\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 59 val_rmse: 0.4705\n",
      "Still best_val_rmse: 0.4666 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 61 val_rmse: 0.4664\n",
      "New best_val_rmse: 0.4664\n",
      "\n",
      "1 steps took 0.423 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.466\n",
      "New best_val_rmse: 0.466\n",
      "\n",
      "1 steps took 0.43 seconds\n",
      "Epoch: 2 batch_num: 63 val_rmse: 0.4658\n",
      "New best_val_rmse: 0.4658\n",
      "\n",
      "1 steps took 0.434 seconds\n",
      "Epoch: 2 batch_num: 64 val_rmse: 0.4659\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.406 seconds\n",
      "Epoch: 2 batch_num: 65 val_rmse: 0.4661\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4662\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.403 seconds\n",
      "Epoch: 2 batch_num: 67 val_rmse: 0.4665\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4663\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 69 val_rmse: 0.4665\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4662\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 71 val_rmse: 0.466\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4663\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.406 seconds\n",
      "Epoch: 2 batch_num: 73 val_rmse: 0.4665\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.403 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.467\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.406 seconds\n",
      "Epoch: 2 batch_num: 75 val_rmse: 0.4678\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4688\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 77 val_rmse: 0.4698\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4704\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4725\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4742\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4744\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "2 steps took 0.809 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4742\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4728\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4725\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "2 steps took 0.807 seconds\n",
      "Epoch: 2 batch_num: 92 val_rmse: 0.472\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "2 steps took 0.806 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4711\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.4699\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 97 val_rmse: 0.4693\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.403 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4688\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 99 val_rmse: 0.4684\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4683\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 101 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4677\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 103 val_rmse: 0.4675\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4674\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 105 val_rmse: 0.4675\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4675\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 107 val_rmse: 0.4675\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.416 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 109 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.424 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4675\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 111 val_rmse: 0.4674\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.406 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4674\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.407 seconds\n",
      "Epoch: 2 batch_num: 113 val_rmse: 0.4674\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4674\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 115 val_rmse: 0.4675\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4675\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 117 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.407 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 119 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 121 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4675\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 123 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.41 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4676\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 125 val_rmse: 0.4677\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4678\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 127 val_rmse: 0.4678\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.407 seconds\n",
      "Epoch: 2 batch_num: 129 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.417 seconds\n",
      "Epoch: 2 batch_num: 131 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.429 seconds\n",
      "Epoch: 2 batch_num: 133 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.404 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 135 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.419 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.403 seconds\n",
      "Epoch: 2 batch_num: 137 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.405 seconds\n",
      "Epoch: 2 batch_num: 139 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "1 steps took 0.403 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4679\n",
      "Still best_val_rmse: 0.4658 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.4802685083486929, 0.46584051877659405]\n",
      "Mean: 0.4730545135626435\n",
      "\n",
      "Fold 3/5\n",
      "\n",
      "16 steps took 7.06 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 1.03\n",
      "New best_val_rmse: 1.03\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.7817\n",
      "New best_val_rmse: 0.7817\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6324\n",
      "New best_val_rmse: 0.6324\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.7714\n",
      "Still best_val_rmse: 0.6324 (from epoch 0)\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.7421\n",
      "Still best_val_rmse: 0.6324 (from epoch 0)\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.652\n",
      "Still best_val_rmse: 0.6324 (from epoch 0)\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5765\n",
      "New best_val_rmse: 0.5765\n",
      "\n",
      "16 steps took 6.51 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5202\n",
      "New best_val_rmse: 0.5202\n",
      "\n",
      "16 steps took 6.63 seconds\n",
      "Epoch: 1 batch_num: 3 val_rmse: 0.5249\n",
      "Still best_val_rmse: 0.5202 (from epoch 0)\n",
      "\n",
      "16 steps took 6.45 seconds\n",
      "Epoch: 1 batch_num: 19 val_rmse: 0.5297\n",
      "Still best_val_rmse: 0.5202 (from epoch 0)\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 1 batch_num: 35 val_rmse: 0.5016\n",
      "New best_val_rmse: 0.5016\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 1 batch_num: 51 val_rmse: 0.5063\n",
      "Still best_val_rmse: 0.5016 (from epoch 1)\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 1 batch_num: 67 val_rmse: 0.572\n",
      "Still best_val_rmse: 0.5016 (from epoch 1)\n",
      "\n",
      "16 steps took 6.58 seconds\n",
      "Epoch: 1 batch_num: 83 val_rmse: 0.4976\n",
      "New best_val_rmse: 0.4976\n",
      "\n",
      "8 steps took 3.26 seconds\n",
      "Epoch: 1 batch_num: 91 val_rmse: 0.5423\n",
      "Still best_val_rmse: 0.4976 (from epoch 1)\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 1 batch_num: 107 val_rmse: 0.4968\n",
      "New best_val_rmse: 0.4968\n",
      "\n",
      "8 steps took 3.31 seconds\n",
      "Epoch: 1 batch_num: 115 val_rmse: 0.5032\n",
      "Still best_val_rmse: 0.4968 (from epoch 1)\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 1 batch_num: 131 val_rmse: 0.4941\n",
      "New best_val_rmse: 0.4941\n",
      "\n",
      "8 steps took 3.23 seconds\n",
      "Epoch: 1 batch_num: 139 val_rmse: 0.4998\n",
      "Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "8 steps took 3.37 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4976\n",
      "Still best_val_rmse: 0.4941 (from epoch 1)\n",
      "\n",
      "8 steps took 3.24 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4914\n",
      "New best_val_rmse: 0.4914\n",
      "\n",
      "8 steps took 3.26 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.4845\n",
      "New best_val_rmse: 0.4845\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 26 val_rmse: 0.483\n",
      "New best_val_rmse: 0.483\n",
      "\n",
      "4 steps took 1.72 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4827\n",
      "New best_val_rmse: 0.4827\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.486\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4833\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 42 val_rmse: 0.4864\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4857\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4844\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.486\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 58 val_rmse: 0.5034\n",
      "Still best_val_rmse: 0.4827 (from epoch 2)\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.481\n",
      "New best_val_rmse: 0.481\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4808\n",
      "New best_val_rmse: 0.4808\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.4812\n",
      "Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4834\n",
      "Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4846\n",
      "Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4841\n",
      "Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4816\n",
      "Still best_val_rmse: 0.4808 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4801\n",
      "New best_val_rmse: 0.4801\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4792\n",
      "New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 0.821 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.4792\n",
      "New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 0.81 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4792\n",
      "New best_val_rmse: 0.4792\n",
      "\n",
      "2 steps took 0.833 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4791\n",
      "New best_val_rmse: 0.4791\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.479\n",
      "New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.812 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.479\n",
      "New best_val_rmse: 0.479\n",
      "\n",
      "2 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4789\n",
      "New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4789\n",
      "New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 0.836 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.815 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4789\n",
      "New best_val_rmse: 0.4789\n",
      "\n",
      "2 steps took 0.811 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.848 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.807 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.827 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.807 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.81 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.807 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "2 steps took 0.809 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4789 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.4802685083486929, 0.46584051877659405, 0.47890673274629275]\n",
      "Mean: 0.4750052532905266\n",
      "\n",
      "Fold 4/5\n",
      "\n",
      "16 steps took 7.0 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9736\n",
      "New best_val_rmse: 0.9736\n",
      "\n",
      "16 steps took 6.54 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.83\n",
      "New best_val_rmse: 0.83\n",
      "\n",
      "16 steps took 6.51 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.708\n",
      "New best_val_rmse: 0.708\n",
      "\n",
      "16 steps took 6.53 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.6581\n",
      "New best_val_rmse: 0.6581\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6555\n",
      "New best_val_rmse: 0.6555\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5345\n",
      "New best_val_rmse: 0.5345\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.6421\n",
      "Still best_val_rmse: 0.5345 (from epoch 0)\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5446\n",
      "Still best_val_rmse: 0.5345 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 3 val_rmse: 0.542\n",
      "Still best_val_rmse: 0.5345 (from epoch 0)\n",
      "\n",
      "16 steps took 6.61 seconds\n",
      "Epoch: 1 batch_num: 19 val_rmse: 0.5675\n",
      "Still best_val_rmse: 0.5345 (from epoch 0)\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 1 batch_num: 35 val_rmse: 0.5634\n",
      "Still best_val_rmse: 0.5345 (from epoch 0)\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 1 batch_num: 51 val_rmse: 0.5273\n",
      "New best_val_rmse: 0.5273\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 67 val_rmse: 0.5025\n",
      "New best_val_rmse: 0.5025\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 1 batch_num: 83 val_rmse: 0.5577\n",
      "Still best_val_rmse: 0.5025 (from epoch 1)\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 1 batch_num: 99 val_rmse: 0.5076\n",
      "Still best_val_rmse: 0.5025 (from epoch 1)\n",
      "\n",
      "16 steps took 6.46 seconds\n",
      "Epoch: 1 batch_num: 115 val_rmse: 0.5463\n",
      "Still best_val_rmse: 0.5025 (from epoch 1)\n",
      "\n",
      "16 steps took 6.46 seconds\n",
      "Epoch: 1 batch_num: 131 val_rmse: 0.4873\n",
      "New best_val_rmse: 0.4873\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 1 batch_num: 135 val_rmse: 0.4869\n",
      "New best_val_rmse: 0.4869\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 1 batch_num: 139 val_rmse: 0.5043\n",
      "Still best_val_rmse: 0.4869 (from epoch 1)\n",
      "\n",
      "16 steps took 6.64 seconds\n",
      "Epoch: 2 batch_num: 14 val_rmse: 0.4914\n",
      "Still best_val_rmse: 0.4869 (from epoch 1)\n",
      "\n",
      "8 steps took 3.25 seconds\n",
      "Epoch: 2 batch_num: 22 val_rmse: 0.49\n",
      "Still best_val_rmse: 0.4869 (from epoch 1)\n",
      "\n",
      "8 steps took 3.29 seconds\n",
      "Epoch: 2 batch_num: 30 val_rmse: 0.4818\n",
      "New best_val_rmse: 0.4818\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 34 val_rmse: 0.4805\n",
      "New best_val_rmse: 0.4805\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 38 val_rmse: 0.4913\n",
      "Still best_val_rmse: 0.4805 (from epoch 2)\n",
      "\n",
      "8 steps took 3.27 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.48\n",
      "New best_val_rmse: 0.48\n",
      "\n",
      "2 steps took 0.869 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4809\n",
      "Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 52 val_rmse: 0.4807\n",
      "Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 56 val_rmse: 0.4803\n",
      "Still best_val_rmse: 0.48 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 60 val_rmse: 0.4788\n",
      "New best_val_rmse: 0.4788\n",
      "\n",
      "2 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 62 val_rmse: 0.4805\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 66 val_rmse: 0.4791\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.809 seconds\n",
      "Epoch: 2 batch_num: 68 val_rmse: 0.4802\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 72 val_rmse: 0.4828\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4809\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4818\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4865\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 88 val_rmse: 0.4916\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "8 steps took 3.26 seconds\n",
      "Epoch: 2 batch_num: 96 val_rmse: 0.483\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 100 val_rmse: 0.4798\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.837 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4793\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.814 seconds\n",
      "Epoch: 2 batch_num: 104 val_rmse: 0.4791\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.807 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.479\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 108 val_rmse: 0.479\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.811 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4793\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4797\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "2 steps took 0.829 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4801\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4806\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4817\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4824\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4826\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4825\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4825\n",
      "Still best_val_rmse: 0.4788 (from epoch 2)\n",
      "\n",
      "Performance estimates:\n",
      "[0.4802685083486929, 0.46584051877659405, 0.47890673274629275, 0.47880262303135024]\n",
      "Mean: 0.4759545957257325\n",
      "\n",
      "Fold 5/5\n",
      "\n",
      "16 steps took 7.02 seconds\n",
      "Epoch: 0 batch_num: 16 val_rmse: 0.9385\n",
      "New best_val_rmse: 0.9385\n",
      "\n",
      "16 steps took 6.56 seconds\n",
      "Epoch: 0 batch_num: 32 val_rmse: 0.8014\n",
      "New best_val_rmse: 0.8014\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 0 batch_num: 48 val_rmse: 0.6624\n",
      "New best_val_rmse: 0.6624\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 0 batch_num: 64 val_rmse: 0.783\n",
      "Still best_val_rmse: 0.6624 (from epoch 0)\n",
      "\n",
      "16 steps took 6.46 seconds\n",
      "Epoch: 0 batch_num: 80 val_rmse: 0.6569\n",
      "New best_val_rmse: 0.6569\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 0 batch_num: 96 val_rmse: 0.5875\n",
      "New best_val_rmse: 0.5875\n",
      "\n",
      "16 steps took 6.62 seconds\n",
      "Epoch: 0 batch_num: 112 val_rmse: 0.5941\n",
      "Still best_val_rmse: 0.5875 (from epoch 0)\n",
      "\n",
      "16 steps took 6.46 seconds\n",
      "Epoch: 0 batch_num: 128 val_rmse: 0.5209\n",
      "New best_val_rmse: 0.5209\n",
      "\n",
      "16 steps took 6.65 seconds\n",
      "Epoch: 1 batch_num: 3 val_rmse: 0.6177\n",
      "Still best_val_rmse: 0.5209 (from epoch 0)\n",
      "\n",
      "16 steps took 6.59 seconds\n",
      "Epoch: 1 batch_num: 19 val_rmse: 0.5478\n",
      "Still best_val_rmse: 0.5209 (from epoch 0)\n",
      "\n",
      "16 steps took 6.51 seconds\n",
      "Epoch: 1 batch_num: 35 val_rmse: 0.5603\n",
      "Still best_val_rmse: 0.5209 (from epoch 0)\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 1 batch_num: 51 val_rmse: 0.4886\n",
      "New best_val_rmse: 0.4886\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 1 batch_num: 55 val_rmse: 0.5112\n",
      "Still best_val_rmse: 0.4886 (from epoch 1)\n",
      "\n",
      "16 steps took 6.47 seconds\n",
      "Epoch: 1 batch_num: 71 val_rmse: 0.5009\n",
      "Still best_val_rmse: 0.4886 (from epoch 1)\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 1 batch_num: 87 val_rmse: 0.5696\n",
      "Still best_val_rmse: 0.4886 (from epoch 1)\n",
      "\n",
      "16 steps took 6.55 seconds\n",
      "Epoch: 1 batch_num: 103 val_rmse: 0.4961\n",
      "Still best_val_rmse: 0.4886 (from epoch 1)\n",
      "\n",
      "8 steps took 3.26 seconds\n",
      "Epoch: 1 batch_num: 111 val_rmse: 0.5015\n",
      "Still best_val_rmse: 0.4886 (from epoch 1)\n",
      "\n",
      "16 steps took 6.49 seconds\n",
      "Epoch: 1 batch_num: 127 val_rmse: 0.4856\n",
      "New best_val_rmse: 0.4856\n",
      "\n",
      "4 steps took 1.66 seconds\n",
      "Epoch: 1 batch_num: 131 val_rmse: 0.4769\n",
      "New best_val_rmse: 0.4769\n",
      "\n",
      "2 steps took 0.823 seconds\n",
      "Epoch: 1 batch_num: 133 val_rmse: 0.4757\n",
      "New best_val_rmse: 0.4757\n",
      "\n",
      "2 steps took 0.824 seconds\n",
      "Epoch: 1 batch_num: 135 val_rmse: 0.479\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.811 seconds\n",
      "Epoch: 1 batch_num: 137 val_rmse: 0.4946\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "8 steps took 3.38 seconds\n",
      "Epoch: 2 batch_num: 4 val_rmse: 0.4763\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.831 seconds\n",
      "Epoch: 2 batch_num: 6 val_rmse: 0.4779\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 8 val_rmse: 0.4853\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 12 val_rmse: 0.4949\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "8 steps took 3.23 seconds\n",
      "Epoch: 2 batch_num: 20 val_rmse: 0.4839\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 24 val_rmse: 0.5121\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "16 steps took 6.48 seconds\n",
      "Epoch: 2 batch_num: 40 val_rmse: 0.4838\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.64 seconds\n",
      "Epoch: 2 batch_num: 44 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.809 seconds\n",
      "Epoch: 2 batch_num: 46 val_rmse: 0.4789\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.809 seconds\n",
      "Epoch: 2 batch_num: 48 val_rmse: 0.4788\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.816 seconds\n",
      "Epoch: 2 batch_num: 50 val_rmse: 0.4832\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 54 val_rmse: 0.5038\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "16 steps took 6.5 seconds\n",
      "Epoch: 2 batch_num: 70 val_rmse: 0.4818\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 74 val_rmse: 0.4794\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 76 val_rmse: 0.4779\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 78 val_rmse: 0.4766\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 80 val_rmse: 0.4768\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.809 seconds\n",
      "Epoch: 2 batch_num: 82 val_rmse: 0.478\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 84 val_rmse: 0.4798\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.81 seconds\n",
      "Epoch: 2 batch_num: 86 val_rmse: 0.4816\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.65 seconds\n",
      "Epoch: 2 batch_num: 90 val_rmse: 0.4833\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 94 val_rmse: 0.4843\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.61 seconds\n",
      "Epoch: 2 batch_num: 98 val_rmse: 0.4829\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 102 val_rmse: 0.4819\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.62 seconds\n",
      "Epoch: 2 batch_num: 106 val_rmse: 0.4801\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "4 steps took 1.63 seconds\n",
      "Epoch: 2 batch_num: 110 val_rmse: 0.4779\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.825 seconds\n",
      "Epoch: 2 batch_num: 112 val_rmse: 0.4775\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.807 seconds\n",
      "Epoch: 2 batch_num: 114 val_rmse: 0.4773\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 116 val_rmse: 0.4771\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.809 seconds\n",
      "Epoch: 2 batch_num: 118 val_rmse: 0.4773\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.819 seconds\n",
      "Epoch: 2 batch_num: 120 val_rmse: 0.4774\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.828 seconds\n",
      "Epoch: 2 batch_num: 122 val_rmse: 0.4774\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.807 seconds\n",
      "Epoch: 2 batch_num: 124 val_rmse: 0.4776\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.809 seconds\n",
      "Epoch: 2 batch_num: 126 val_rmse: 0.4778\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 128 val_rmse: 0.4779\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.81 seconds\n",
      "Epoch: 2 batch_num: 130 val_rmse: 0.4779\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.826 seconds\n",
      "Epoch: 2 batch_num: 132 val_rmse: 0.4779\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.83 seconds\n",
      "Epoch: 2 batch_num: 134 val_rmse: 0.4779\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.808 seconds\n",
      "Epoch: 2 batch_num: 136 val_rmse: 0.4778\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.806 seconds\n",
      "Epoch: 2 batch_num: 138 val_rmse: 0.4778\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "2 steps took 0.806 seconds\n",
      "Epoch: 2 batch_num: 140 val_rmse: 0.4778\n",
      "Still best_val_rmse: 0.4757 (from epoch 1)\n",
      "\n",
      "Performance estimates:\n",
      "[0.4802685083486929, 0.46584051877659405, 0.47890673274629275, 0.47880262303135024, 0.47573807270278945]\n",
      "Mean: 0.4759112911211439\n"
     ]
    }
   ],
   "source": [
    "# 実行処理。 KFold & 学習\n",
    "gc.collect()\n",
    "\n",
    "SEED = 1000\n",
    "list_val_rmse = []\n",
    "\n",
    "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
    "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
    "    model_path = f\"model_{fold + 1}.pth\" # model_fold数_.pth\n",
    "        \n",
    "    set_random_seed(SEED + fold) # SEEDはfold別に変わるようにする\n",
    "    \n",
    "    train_dataset = LitDataset(train_df.loc[train_indices]) # train, validのDataset\n",
    "    val_dataset = LitDataset(train_df.loc[val_indices])\n",
    "        \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              drop_last=True, shuffle=True, num_workers=2) # train, validのDataLoader\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                            drop_last=False, shuffle=False, num_workers=2)    \n",
    "        \n",
    "    set_random_seed(SEED + fold) # なんで二回SEEDをセットするのだろう？\n",
    "    \n",
    "    model = LitModel().to(DEVICE) # modelをDEVICEへぶち込む\n",
    "    \n",
    "    optimizer = create_optimizer(model) # optimizerをモデルから作成\n",
    "    scheduler = get_cosine_schedule_with_warmup( # schedulerを作成\n",
    "        optimizer,\n",
    "        num_training_steps=NUM_EPOCHS * len(train_loader),\n",
    "        num_warmup_steps=50)    \n",
    "    \n",
    "    list_val_rmse.append(train(model, model_path, train_loader,\n",
    "                               val_loader, optimizer, scheduler=scheduler)) # 学習開始し、val_rmseのリストを格納\n",
    "\n",
    "    del model # モデルは保存したので、消す\n",
    "    gc.collect() \n",
    "    \n",
    "    print(\"\\nPerformance estimates:\")\n",
    "    print(list_val_rmse)\n",
    "    print(\"Mean:\", np.array(list_val_rmse).mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-distributor",
   "metadata": {
    "papermill": {
     "duration": 0.21369,
     "end_time": "2021-07-02T17:18:48.370700",
     "exception": false,
     "start_time": "2021-07-02T17:18:48.157010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fitting-ethnic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T17:18:48.819132Z",
     "iopub.status.busy": "2021-07-02T17:18:48.818324Z",
     "iopub.status.idle": "2021-07-02T17:18:48.848705Z",
     "shell.execute_reply": "2021-07-02T17:18:48.849455Z"
    },
    "papermill": {
     "duration": 0.263811,
     "end_time": "2021-07-02T17:18:48.849676",
     "exception": false,
     "start_time": "2021-07-02T17:18:48.585865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = LitDataset(test_df, inference_only=True) # TestのDataset作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cordless-jersey",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T17:18:49.302659Z",
     "iopub.status.busy": "2021-07-02T17:18:49.301879Z",
     "iopub.status.idle": "2021-07-02T17:19:13.546213Z",
     "shell.execute_reply": "2021-07-02T17:19:13.546680Z"
    },
    "papermill": {
     "duration": 24.465236,
     "end_time": "2021-07-02T17:19:13.546854",
     "exception": false,
     "start_time": "2021-07-02T17:18:49.081618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_1.pth\n",
      "\n",
      "Using model_2.pth\n",
      "\n",
      "Using model_3.pth\n",
      "\n",
      "Using model_4.pth\n",
      "\n",
      "Using model_5.pth\n"
     ]
    }
   ],
   "source": [
    "# 推論実行\n",
    "all_predictions = np.zeros((len(list_val_rmse), len(test_df))) # 推論結果について、「fold　× 推論df」のzero行列で枠を作る\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True) # TestのDataset(何で、もう一回作るのだろう？)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2) # TestのDataLoader\n",
    "\n",
    "for index in range(len(list_val_rmse)): # Fold数ごとに推論する\n",
    "    model_path = f\"model_{index + 1}.pth\" # 対応するモデルを読む\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path))    # 対応するモデルから、重みを読み込む\n",
    "    model.to(DEVICE) # モデルをDEVICEへぶち込む\n",
    "    \n",
    "    all_predictions[index] = predict(model, test_loader) # 推論結果行列の対象列に、推論結果を入力(以後、繰り返し)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fifty-kentucky",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T17:19:14.048663Z",
     "iopub.status.busy": "2021-07-02T17:19:14.047947Z",
     "iopub.status.idle": "2021-07-02T17:19:15.064359Z",
     "shell.execute_reply": "2021-07-02T17:19:15.063395Z"
    },
    "papermill": {
     "duration": 1.296135,
     "end_time": "2021-07-02T17:19:15.064548",
     "exception": false,
     "start_time": "2021-07-02T17:19:13.768413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id    target\n",
      "0  c0f722661 -0.500003\n",
      "1  f0953f0a5 -0.508400\n",
      "2  0df072751 -0.421301\n",
      "3  04caf4e0c -2.518566\n",
      "4  0e63f8bea -1.699257\n",
      "5  12537fe78 -1.414659\n",
      "6  965e592c0  0.043717\n"
     ]
    }
   ],
   "source": [
    "predictions = all_predictions.mean(axis=0) # 全FOLDの推論の平均を最終結果とする。\n",
    "submission_df.target = predictions # 予測結果をsub用dfと結合\n",
    "print(submission_df) # 結果表示\n",
    "submission_df.to_csv(\"submission.csv\", index=False) # sub用csv出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-persian",
   "metadata": {
    "papermill": {
     "duration": 0.218168,
     "end_time": "2021-07-02T17:19:15.501460",
     "exception": false,
     "start_time": "2021-07-02T17:19:15.283292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-plane",
   "metadata": {
    "papermill": {
     "duration": 0.227233,
     "end_time": "2021-07-02T17:19:15.947838",
     "exception": false,
     "start_time": "2021-07-02T17:19:15.720605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-responsibility",
   "metadata": {
    "papermill": {
     "duration": 0.227051,
     "end_time": "2021-07-02T17:19:16.406940",
     "exception": false,
     "start_time": "2021-07-02T17:19:16.179889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-mercury",
   "metadata": {
    "papermill": {
     "duration": 0.231111,
     "end_time": "2021-07-02T17:19:16.857890",
     "exception": false,
     "start_time": "2021-07-02T17:19:16.626779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-softball",
   "metadata": {
    "papermill": {
     "duration": 0.241054,
     "end_time": "2021-07-02T17:19:17.355244",
     "exception": false,
     "start_time": "2021-07-02T17:19:17.114190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-interface",
   "metadata": {
    "papermill": {
     "duration": 0.221437,
     "end_time": "2021-07-02T17:19:17.807458",
     "exception": false,
     "start_time": "2021-07-02T17:19:17.586021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2789.584994,
   "end_time": "2021-07-02T17:19:21.124281",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-02T16:32:51.539287",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
