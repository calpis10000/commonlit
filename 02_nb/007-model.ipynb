{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import yaml\n",
    "import warnings\n",
    "import random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import hashlib\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# usual\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# preprocess\n",
    "from fasttext import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "#import texthero as hero\n",
    "import nltk\n",
    "import collections\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "import cv2\n",
    "import string\n",
    "import re\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "#import optuna.integration.lightgbm as lgb  # チューニング用\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport  # profile report を作る用\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# plot settings\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = False\n",
    "plt.rcParams['font.family'] = 'sans_serif'\n",
    "sns.set(style=\"whitegrid\",  palette=\"muted\", color_codes=True, rc={'grid.linestyle': '--'})\n",
    "red = sns.xkcd_rgb[\"light red\"]\n",
    "green = sns.xkcd_rgb[\"medium green\"]\n",
    "blue = sns.xkcd_rgb[\"denim blue\"]\n",
    "\n",
    "# plot extentions\n",
    "#import japanize_matplotlib\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb007\n",
      "nb007_20210522_235358\n"
     ]
    }
   ],
   "source": [
    "# 試験ID生成\n",
    "trial_prefix = 'nb007'  # ←手動で指定 \n",
    "dttm_now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "trial_id = f'{trial_prefix}_{dttm_now}'\n",
    "\n",
    "print(trial_prefix)\n",
    "print(trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アウトプットの出力先指定\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    OUTPUT_DIR = Path(\".\")\n",
    "else:\n",
    "    OUTPUT_DIR = Path(f\"../03_outputs/{trial_prefix}\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed固定\n",
    "def set_seed(seed=2021):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インプットフォルダ指定\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    DATA_DIR = '../input/commonlitreadabilityprize/'\n",
    "else:\n",
    "    DATA_DIR = '../00_input/commonlitreadabilityprize/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data\n",
    "train_base = pd.read_csv(DATA_DIR + 'train.csv')\n",
    "test_base = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "sample = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴作成_共通処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベースとなる継承元のクラス\n",
    "class BaseBlock(object):\n",
    "    def fit(self, input_df, y=None):\n",
    "        return self.transform(input_df)\n",
    "    def transform(self, input_df):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト特徴_共通処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ktdogome/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ローカルの場合、stopwordsをダウンロード\n",
    "import nltk\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    pass\n",
    "else:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    os.listdir(os.path.expanduser('~/nltk_data/corpora/stopwords/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキスト前処理\n",
    "# https://www.kaggle.com/alaasedeeq/commonlit-readability-eda\n",
    "\n",
    "#filtering the unwanted symbols, spaces, ....etc\n",
    "to_replace_by_space = re.compile('[/(){}\\[\\]|@,;]')\n",
    "punctuation = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "bad_symbols = re.compile('[^0-9a-z #+_]')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    '''\n",
    "    text: a string\n",
    "    returna modified version of the string\n",
    "    '''\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(punctuation, '',text)\n",
    "    text = re.sub(to_replace_by_space, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(bad_symbols, \"\", text)         # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = \" \".join([word for word in text.split(\" \") if word not in stopwords]) # delete stopwords from text\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(s:pd.Series):\n",
    "    x = s.apply(text_prepare)\n",
    "    return x\n",
    "\n",
    "# Counterオブジェクトを取得\n",
    "def get_counter(text:str):\n",
    "    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n",
    "    counter = collections.Counter(text_list)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト特徴_統計量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDescriptionBlock(BaseBlock):\n",
    "    \"\"\"テキストに関する統計量を返す block\"\"\"\n",
    "    def __init__(self, column: str):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "\n",
    "    # 前処理\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "        \n",
    "    def fit(self, input_df, y=None, n_components=50):\n",
    "        self.text = self.preprocess(input_df)\n",
    "        self.counters = self.text.map(get_counter)\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        _length = input_df[self.column].fillna('').map(lambda x: len(x) if x!='' else np.nan)\n",
    "        _wrd_cnt = self.counters.map(lambda x: sum(x.values()))\n",
    "        _wrd_nuniq = self.counters.map(lambda x: len(x))\n",
    "        _wrd_mean = self.counters.map(lambda x: np.mean(list(x.values())))\n",
    "        _wrd_max = self.counters.map(lambda x: np.max(list(x.values())))\n",
    "        \n",
    "        word_length = self.counters.map(lambda x: np.array([len(i) for i in x.keys()]))\n",
    "        word_length_desc = word_length.map(lambda x: pd.Series(x.ravel()).describe())\n",
    "        _word_length_desc_df = pd.DataFrame(word_length_desc.tolist()).iloc[:,1:]\n",
    "        _word_length_desc_df = _word_length_desc_df.add_prefix('word_length_')\n",
    "        \n",
    "        out_df = pd.concat([_length, _wrd_cnt, _wrd_nuniq, _wrd_mean, _wrd_max], axis=1)\n",
    "        out_df.columns = ['text_length', 'word_count', 'word_nunique', 'word_appearance_mean', 'word_appearance_max']\n",
    "        out_df = pd.concat([out_df, _word_length_desc_df], axis=1)\n",
    "        return out_df.add_suffix(f'_{self.column}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト特徴_TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考: https://www.guruguru.science/competitions/16/discussions/556029f7-484d-40d4-ad6a-9d86337487e2/\n",
    "\n",
    "class TfidfBlock(BaseBlock):\n",
    "    \"\"\"tfidf x SVD による圧縮を行なう block\"\"\"\n",
    "    def __init__(self, column: str, n_components=50, ngram_range=(1,1)):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.n_components=n_components\n",
    "        self.ngram_range=ngram_range\n",
    "\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "\n",
    "    def get_master(self, _master_df):\n",
    "        \"\"\"tdidfを計算するための全体集合を返す.\"\"\"\n",
    "        return _master_df\n",
    "\n",
    "    def fit(self, \n",
    "            input_df, \n",
    "            _master_df=None, \n",
    "            y=None\n",
    "           ):\n",
    "        master_df = input_df if _master_df is None else self.get_master(_master_df)\n",
    "        text = self.preprocess(master_df)\n",
    "        self.pileline_ = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=100000, ngram_range=self.ngram_range)),\n",
    "            ('svd', TruncatedSVD(n_components=self.n_components, random_state=SEED)),\n",
    "        ])\n",
    "\n",
    "        self.pileline_.fit(text)\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        text = self.preprocess(input_df)\n",
    "        z = self.pileline_.transform(text)\n",
    "\n",
    "        out_df = pd.DataFrame(z)\n",
    "        return out_df.add_prefix(f'{self.column}_tfidf_{\"_\".join([str(i) for i in self.ngram_range])}_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト特徴_W2V(平均)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.guruguru.science/competitions/16/discussions/2fafef06-5a26-4d33-b535-a94cc9549ac4/\n",
    "# https://www.guruguru.science/competitions/16/discussions/4a6f5f84-8491-4324-ba69-dec49dc648cd/\n",
    "\n",
    "def hashfxn(x):\n",
    "    return int(hashlib.md5(str(x).encode()).hexdigest(), 16)\n",
    "\n",
    "class W2VTrainBlock(BaseBlock):\n",
    "    \"\"\"Word2Vecを学習し、文書のベクトル表現を得るブロック。\n",
    "       学習済みモデルを使うパターンは、別に作成するものとする。\"\"\"\n",
    "    def __init__(self, \n",
    "                 column: str, \n",
    "                 model_size=50, \n",
    "                 min_count=1, \n",
    "                 window=5,\n",
    "                 n_iter=100\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.model_size=model_size\n",
    "        self.min_count=min_count\n",
    "        self.window=window\n",
    "        self.n_iter=n_iter\n",
    "\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "\n",
    "    def get_master(self, _master_df):\n",
    "        \"\"\"Word2Vecを学習するための全体集合を返す.\"\"\"\n",
    "        return _master_df\n",
    "\n",
    "    def fit(self, \n",
    "            input_df, \n",
    "            _master_df=None, \n",
    "            y=None\n",
    "           ):\n",
    "        master_df = input_df if _master_df is None else self.get_master(_master_df)\n",
    "        text = self.preprocess(master_df)\n",
    "        word_lists = text.map(lambda x: [i for i in x.split(' ') if i not in (' ')])\n",
    "        self.w2v_model = word2vec.Word2Vec(word_lists.values.tolist(),\n",
    "                                      vector_size=self.model_size,\n",
    "                                      min_count=self.min_count,\n",
    "                                      window=self.window,\n",
    "                                      seed=SEED,\n",
    "                                      workers=1,\n",
    "                                      hashfxn=hashfxn,\n",
    "                                      iter=self.n_iter)\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        text = self.preprocess(input_df)\n",
    "        word_lists = text.map(lambda x: [i for i in x.split(' ') if i not in (' ')])\n",
    "\n",
    "        # 各文章ごとにそれぞれの単語をベクトル表現に直し、平均をとって文章ベクトルにする\n",
    "        sentence_vectors = word_lists.progress_apply(\n",
    "            lambda x: np.mean([self.w2v_model.wv[e] for e in x], axis=0))\n",
    "        sentence_vectors = np.vstack([x for x in sentence_vectors])\n",
    "        sentence_vector_df = pd.DataFrame(sentence_vectors,\n",
    "                                          columns=[f\"{self.column}_w2v_w{self.window}_{i}\"\n",
    "                                                   for i in range(self.model_size)])\n",
    "        \n",
    "        return sentence_vector_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期化\n",
    "train_feat = pd.DataFrame()\n",
    "test_feat = pd.DataFrame()\n",
    "train_target = train_base['target'].copy()\n",
    "\n",
    "# tfidf作成用のdf作成\n",
    "whole_df = pd.concat([train_base[['id', 'excerpt']], test_base[['id', 'excerpt']]], axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block初期化\n",
    "text_desc = TextDescriptionBlock('excerpt')\n",
    "\n",
    "tfidf_uni = TfidfBlock('excerpt', ngram_range=(1,1))\n",
    "tfidf_bi = TfidfBlock('excerpt', ngram_range=(2,2))\n",
    "tfidf_uni_bi = TfidfBlock('excerpt', ngram_range=(1,2))\n",
    "\n",
    "w2v_w3 = W2VTrainBlock('excerpt', window=3)\n",
    "w2v_w10 = W2VTrainBlock('excerpt', window=10)\n",
    "w2v_w100 = W2VTrainBlock('excerpt', window=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'vector_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-615c69007683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_uni_bi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhole_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_w3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhole_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtrain_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_w10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhole_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrain_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_w100\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhole_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-bb84bdda2cb7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, input_df, _master_df, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mword_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         self.w2v_model = word2vec.Word2Vec(word_lists.values.tolist(),\n\u001b[0m\u001b[1;32m     45\u001b[0m                                       \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                       \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'vector_size'"
     ]
    }
   ],
   "source": [
    "# fit & trainの特徴作成\n",
    "# 単体で完結するもの\n",
    "train_feat = pd.concat([train_feat, text_desc.fit(train_base)], axis='columns')\n",
    "\n",
    "# 対象dfとは別に、特徴作成用dfを指定するもの\n",
    "train_feat = pd.concat([train_feat, tfidf_uni.fit(train_base, whole_df)], axis='columns')\n",
    "train_feat = pd.concat([train_feat, tfidf_bi.fit(train_base, whole_df)], axis='columns')\n",
    "train_feat = pd.concat([train_feat, tfidf_uni_bi.fit(train_base, whole_df)], axis='columns')\n",
    "\n",
    "train_feat = pd.concat([train_feat, w2v_w3.fit(train_base, whole_df)], axis='columns')\n",
    "train_feat = pd.concat([train_feat, w2v_w10.fit(train_base, whole_df)], axis='columns')\n",
    "train_feat = pd.concat([train_feat, w2v_w100.fit(train_base, whole_df)], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testの特徴作成\n",
    "# 単体で完結するもの\n",
    "test_feat = pd.concat([test_feat, text_desc.fit(test_base)], axis='columns')\n",
    "\n",
    "# 対象dfとは別に、特徴作成用dfを指定するもの\n",
    "test_feat = pd.concat([test_feat, tfidf_uni.transform(test_base)], axis='columns')\n",
    "test_feat = pd.concat([test_feat, tfidf_bi.transform(test_base)], axis='columns')\n",
    "test_feat = pd.concat([test_feat, tfidf_uni_bi.transform(test_base)], axis='columns')\n",
    "\n",
    "test_feat = pd.concat([test_feat, w2v_w3.transform(test_base)], axis='columns')\n",
    "test_feat = pd.concat([test_feat, w2v_w10.transform(test_base)], axis='columns')\n",
    "test_feat = pd.concat([test_feat, w2v_w100.transform(test_base)], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_feat.shape)\n",
    "train_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_feat.shape)\n",
    "test_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_feat.shape)\n",
    "print(test_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_cv(X, y, n_splits=5, random_state=0):\n",
    "    folds = KFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "    return list(folds.split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "cv = kfold_cv(train_feat, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metrics': 'rmse',\n",
    "    'seed': SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oof_preds = np.zeros(len(train_feat))\n",
    "test_preds = np.zeros(len(test_feat))\n",
    "\n",
    "importances = pd.DataFrame()\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(cv):\n",
    "    print(f'\\nFold {i + 1}')\n",
    "    trn_x, trn_y = train_feat.iloc[train_index], train_target.iloc[train_index]\n",
    "    val_x, val_y = train_feat.iloc[valid_index], train_target.iloc[valid_index]\n",
    "    \n",
    "    #dtrain = lgb.Dataset(trn_x, trn_y, categorical_feature = ['LE_' + val_ for val_ in le_categories])\n",
    "    #dvalid = lgb.Dataset(val_x, val_y, categorical_feature = ['LE_' + val_ for val_ in le_categories])\n",
    "\n",
    "    dtrain = lgb.Dataset(trn_x, trn_y)\n",
    "    dvalid = lgb.Dataset(val_x, val_y)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=100000,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=['training', 'valid'],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=50\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict(val_x)\n",
    "    oof_preds[valid_index] = val_preds\n",
    "    test_preds += model.predict(test_feat) / 5\n",
    "    \n",
    "    val_score = model.best_score['valid']['rmse']\n",
    "    scores.append(val_score)\n",
    "    models.append(model)\n",
    "    \n",
    "    imp_df = pd.DataFrame({\n",
    "        'feature': model.feature_name(),\n",
    "        'gain': model.feature_importance(importance_type='gain'),\n",
    "        'fold': i+1\n",
    "    })\n",
    "    \n",
    "    importances = pd.concat([importances, imp_df], axis=0)\n",
    "    \n",
    "mean_score = np.mean(scores)\n",
    "std_score  = np.std(scores)\n",
    "all_score  = np.sqrt(mean_squared_error(train_target, oof_preds))\n",
    "metrics_name = 'RMSE'\n",
    "print(f'Mean {metrics_name}: {mean_score}, std: {std_score}, All {metrics_name}: {all_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['target'] = test_preds\n",
    "\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    sample.to_csv('submission.csv',index=False)\n",
    "else:\n",
    "    sample.to_csv(OUTPUT_DIR/'submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.shape)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance(kaggle環境では描画しない)\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    pass\n",
    "else:\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x='gain', y='feature', data=importances.sort_values('gain', ascending=False));\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'feature_importance.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance_boxen(kaggle環境では描画しない)\n",
    "# 参考: https://www.guruguru.science/competitions/13/discussions/d8f2d66a-aeee-4789-8b3d-d5935c26b1b7/\n",
    "\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    pass\n",
    "else:\n",
    "    order = importances.groupby('feature')\\\n",
    "        .sum()[['gain']]\\\n",
    "        .sort_values('gain', ascending=False).index[:50]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(order) * .4), 7))\n",
    "    sns.boxenplot(data=importances, x='feature', y='gain', order=order, ax=ax, palette='viridis')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(OUTPUT_DIR, 'feature_importance_boxen.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP(kaggle環境では描画しない)\n",
    "# 参考その1: https://github.com/slundberg/shap/issues/337\n",
    "# 参考その2: https://github.com/slundberg/shap/issues/630\n",
    "import shap\n",
    "\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    pass\n",
    "else:\n",
    "    shap_values = []\n",
    "    for model_ in models:\n",
    "        explainer = shap.TreeExplainer(model_)\n",
    "        shap_values.append(explainer.shap_values(train_feat))\n",
    "\n",
    "    shap_mean = np.mean(shap_values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP_summary_plot\n",
    "# 参考_画像の出力について: https://github.com/slundberg/shap/issues/153\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    pass\n",
    "else:\n",
    "    shap.summary_plot(shap_mean, train_feat, show=False)\n",
    "    plt.subplots_adjust(left=0.4, right=1.0)  # 保存画像のラベルが欠けるのを防ぐ\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'shap_summary_plot.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SHAP_dependence_plot\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    pass\n",
    "else:\n",
    "    for col_ in train_feat.columns:\n",
    "        shap.dependence_plot(col_, shap_mean, train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分布(train_vs_oof)\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    pass\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sns.distplot(train_target, label='Train', ax=ax, color='C1')\n",
    "    sns.distplot(oof_preds, label='Out Of Fold', ax=ax, color='C2')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'train_vs_oof.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
