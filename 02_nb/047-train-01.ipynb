{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"047-train-01.ipynb","provenance":[{"file_id":"17a4F4aC9L0QBqU8BRTrdqPn0WwJ0b08b","timestamp":1626746992716},{"file_id":"1G_W9irFTrEmDeHR0S6_u0bjpk8nxipXW","timestamp":1626689695352},{"file_id":"1bhhkorT--y8XXaVLM8hibVgC-tLqZ16P","timestamp":1626358153868},{"file_id":"1WtT2hX6O9Qbt_hb9sF50nM2QmDXFi-XA","timestamp":1626338366006},{"file_id":"1k_p5wftcUeo711Xho1-T5an2Xkneau-J","timestamp":1626323813472},{"file_id":"1Vz2GB2BNTWuefEFkCSh3TBPEIel7KG1t","timestamp":1626317426487},{"file_id":"1djoMWojeaIPopG5tS1jNMohn8ineblRh","timestamp":1626306831897},{"file_id":"1-6tlDO8158Pi6TpptIF884oFaEiT4Uxb","timestamp":1626276420047},{"file_id":"1js8eA3mDNS8mwSpCiHuzPeARFlUPAVrg","timestamp":1626272452526},{"file_id":"1yhcPgulwJtjJKUK9IuRKmNMhJ-4YXGol","timestamp":1626267205517},{"file_id":"1mnnSv0Pofn1QxArywV81VYqnZPB8uUWN","timestamp":1626180468522},{"file_id":"1RRdjt_UAeHmr5QQBAMyC82Fq1s31OWdK","timestamp":1625833136005},{"file_id":"1JPgg44HFemzwk8VSCXih3PejL0idy-C4","timestamp":1625825483466},{"file_id":"1Ye6wqVX71xAAAhmjXkw9IpRvTqeUyJDA","timestamp":1625812137500}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"id":"Z6yRwt-PXtbP","executionInfo":{"status":"ok","timestamp":1626775576726,"user_tz":-540,"elapsed":354,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"bc4d3dba-3178-4c07-d700-b8c2f9be079a"},"source":["\"\"\"\n","if 'google.colab' in sys.modules:  # colab環境特有の処理_初回のみ\n","  # Google Driveのマウント\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules' \\\n","   -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/requirements.txt' \\\n","   --ignore-installed\n","\n","  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules' \\\n","   transformers -U\n","  !pip install gensim==4.0.1 --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\n","  !pip install pytorch_memlab --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nif 'google.colab' in sys.modules:  # colab環境特有の処理_初回のみ\\n  # Google Driveのマウント\\n  from google.colab import drive\\n  drive.mount('/content/drive')\\n\\n  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'    -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/requirements.txt'    --ignore-installed\\n\\n  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'    transformers -U\\n  !pip install gensim==4.0.1 --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\\n  !pip install pytorch_memlab --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\\n\""]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucCbvGD1XvG7","executionInfo":{"status":"ok","timestamp":1626775893233,"user_tz":-540,"elapsed":314135,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"9b04b644-884d-4174-ef15-c5df118383f0"},"source":["import sys\n","if 'google.colab' in sys.modules:  # colab特有の処理_2回目以降\n","  # Google Driveのマウント\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  # データセットをDriveから取得\n","  !mkdir -p 'input'\n","  !cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input/commonlitreadabilityprize/' '/content/input'\n","  !cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch5' '/content/clrp-roberta-large'\n","  # ライブラリのパス指定\n","  sys.path.append('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"N6kqvVoPrnpj","executionInfo":{"status":"ok","timestamp":1626775902530,"user_tz":-540,"elapsed":486,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"584416db-fc21-4155-be7f-7f70ebef1520"},"source":["\"\"\"\n","%cd\n","!mkdir .kaggle\n","!mkdir /content/model\n","!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n%cd\\n!mkdir .kaggle\\n!mkdir /content/model\\n!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\\n'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"Axihy1acGjie","executionInfo":{"status":"ok","timestamp":1626775902860,"user_tz":-540,"elapsed":6,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"0869000d-c956-4e23-c129-73b45dfefc0a"},"source":["\"\"\"\n","!mkdir /content/pre-trained-roberta\n","!kaggle datasets download -p /content/pre-trained-roberta clrprobertalarge --unzip\n","!cp -r /content/pre-trained-roberta/ /content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch5\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n!mkdir /content/pre-trained-roberta\\n!kaggle datasets download -p /content/pre-trained-roberta clrprobertalarge --unzip\\n!cp -r /content/pre-trained-roberta/ /content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch5\\n'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"GpYkj74lHeU_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RV9-VwbpZLZ9"},"source":["from pathlib import Path\n","\n","# input\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    DATA_DIR = Path('../input/commonlitreadabilityprize/')\n","\n","elif 'google.colab' in sys.modules: # Colab環境\n","    DATA_DIR = Path('/content/input/commonlitreadabilityprize')\n","\n","else:\n","    DATA_DIR = Path('../00_input/commonlitreadabilityprize/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5difyXe00UV"},"source":["from pathlib import Path\n","\n","# tokenizer\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    TOKENIZER_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    TOKENIZER_DIR = '/content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    TOKENIZER_DIR = 'roberta-large'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKjsUxnOeDYl"},"source":["from pathlib import Path\n","\n","# pre-trained model\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    PRE_TRAINED_MODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    PRE_TRAINED_MODEL_DIR = '/content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    PRE_TRAINED_MODEL_DIR = 'roberta-large'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLaT2V0ReoAZ"},"source":["UPLOAD_DIR = Path('/content/model')\n","EX_NO = '047-train-01'  # 実験番号などを入れる、folderのpathにする\n","USERID = 'calpis10000'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOGjAb4pAJ0F"},"source":["import subprocess\n","import shlex\n","\n","def gpuinfo():\n","    \"\"\"\n","    Returns size of total GPU RAM and used GPU RAM.\n","\n","    Parameters\n","    ----------\n","    None\n","\n","    Returns\n","    -------\n","    info : dict\n","        Total GPU RAM in integer for key 'total_MiB'.\n","        Used GPU RAM in integer for key 'used_MiB'.\n","    \"\"\"\n","\n","    command = 'nvidia-smi -q -d MEMORY | sed -n \"/FB Memory Usage/,/Free/p\" | sed -e \"1d\" -e \"4d\" -e \"s/ MiB//g\" | cut -d \":\" -f 2 | cut -c2-'\n","    commands = [shlex.split(part) for part in command.split(' | ')]\n","    for i, cmd in enumerate(commands):\n","        if i==0:\n","            res = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n","        else:\n","            res = subprocess.Popen(cmd, stdin=res.stdout, stdout=subprocess.PIPE)\n","    total, used = map(int, res.communicate()[0].decode('utf-8').strip().split('\\n'))\n","    info = {'total_MiB':total, 'used_MiB':used}\n","    return info\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g3-6m5MKXecB"},"source":["# Overview\n","This nb is based on copy from https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch .\n","\n","Acknowledgments(from base nb): \n","some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T06:26:32.834365Z","iopub.execute_input":"2021-07-04T06:26:32.834903Z","iopub.status.idle":"2021-07-04T06:26:40.143740Z","shell.execute_reply.started":"2021-07-04T06:26:32.834785Z","shell.execute_reply":"2021-07-04T06:26:40.142864Z"},"trusted":true,"id":"HRsRZ06WXecD"},"source":["import os\n","import math\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from transformers import AdamW # optimizer\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoConfig\n","from transformers import get_cosine_schedule_with_warmup # scheduler\n","from pytorch_memlab import profile\n","import pytorch_memlab\n","from pytorch_memlab import MemReporter\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gc\n","gc.enable()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.145217Z","iopub.execute_input":"2021-07-04T06:26:40.145539Z","iopub.status.idle":"2021-07-04T06:26:40.201326Z","shell.execute_reply.started":"2021-07-04T06:26:40.145504Z","shell.execute_reply":"2021-07-04T06:26:40.200136Z"},"trusted":true,"id":"omBfwshTXecE"},"source":["NUM_FOLDS = 5 # K Fold\n","NUM_EPOCHS = 5 # Epochs\n","BATCH_SIZE = 12 # Batch Size\n","MAX_LEN = 248 # ベクトル長\n","EVAL_SCHEDULE = [(0.55, 64), (0.50, 32), (0.49, 16), (0.48, 8), (0.47, 4), (0.46, 2), (-1., 1)] # schedulerの何らかの設定？\n","ROBERTA_PATH = PRE_TRAINED_MODEL_DIR # roberta pre-trainedモデル(モデルとして指定)\n","TOKENIZER_PATH = TOKENIZER_DIR # roberta pre-trainedモデル(Tokenizerとして指定)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # cudaがなければcpuを使えばいいじゃない"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.203398Z","iopub.execute_input":"2021-07-04T06:26:40.204055Z","iopub.status.idle":"2021-07-04T06:26:40.211572Z","shell.execute_reply.started":"2021-07-04T06:26:40.204015Z","shell.execute_reply":"2021-07-04T06:26:40.210762Z"},"trusted":true,"id":"4qcuXqwtXecF"},"source":["def set_random_seed(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n","\n","    torch.manual_seed(random_seed)\n","    torch.cuda.manual_seed(random_seed)\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","    torch.backends.cudnn.deterministic = True# cudnnによる最適化で結果が変わらないためのおまじない "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.214188Z","iopub.execute_input":"2021-07-04T06:26:40.214809Z","iopub.status.idle":"2021-07-04T06:26:40.309744Z","shell.execute_reply.started":"2021-07-04T06:26:40.214769Z","shell.execute_reply":"2021-07-04T06:26:40.308926Z"},"trusted":true,"id":"70PyLsJTXecF"},"source":["# train, testを読む\n","train_df = pd.read_csv(DATA_DIR/\"train.csv\")\n","\n","# Remove incomplete entries if any.\n","train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n","              inplace=True)\n","train_df.reset_index(drop=True, inplace=True)\n","\n","test_df = pd.read_csv(DATA_DIR/\"test.csv\")\n","submission_df = pd.read_csv(DATA_DIR/\"sample_submission.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"9ZYOB59L8qtA","executionInfo":{"status":"ok","timestamp":1626777916868,"user_tz":-540,"elapsed":8,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"98b58c55-9832-4b40-a852-d6f2c0cee103"},"source":["train_df.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>url_legal</th>\n","      <th>license</th>\n","      <th>excerpt</th>\n","      <th>target</th>\n","      <th>standard_error</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>c12129c31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>When the young people returned to the ballroom...</td>\n","      <td>-0.340259</td>\n","      <td>0.464009</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>85aa80a4c</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n","      <td>-0.315372</td>\n","      <td>0.480805</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>b69ac6792</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>As Roger had predicted, the snow departed as q...</td>\n","      <td>-0.580118</td>\n","      <td>0.476676</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>dd1000b26</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>And outside before the palace a great garden w...</td>\n","      <td>-1.054013</td>\n","      <td>0.450007</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>37c1b32fb</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Once upon a time there were Three Bears who li...</td>\n","      <td>0.247197</td>\n","      <td>0.510845</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id url_legal  ...    target standard_error\n","0  c12129c31       NaN  ... -0.340259       0.464009\n","1  85aa80a4c       NaN  ... -0.315372       0.480805\n","2  b69ac6792       NaN  ... -0.580118       0.476676\n","3  dd1000b26       NaN  ... -1.054013       0.450007\n","4  37c1b32fb       NaN  ...  0.247197       0.510845\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.311021Z","iopub.execute_input":"2021-07-04T06:26:40.311347Z","iopub.status.idle":"2021-07-04T06:26:40.624393Z","shell.execute_reply.started":"2021-07-04T06:26:40.311314Z","shell.execute_reply":"2021-07-04T06:26:40.623347Z"},"trusted":true,"id":"xf0662k4XecF"},"source":["# tokenizerを指定\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6aaghNkXecG"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.628883Z","iopub.execute_input":"2021-07-04T06:26:40.629347Z","iopub.status.idle":"2021-07-04T06:26:40.644338Z","shell.execute_reply.started":"2021-07-04T06:26:40.629309Z","shell.execute_reply":"2021-07-04T06:26:40.643336Z"},"trusted":true,"id":"zkopT0U1XecG"},"source":["# Dataset用のClass。おそらく、trainとtestでインスタンスを生成し、DataFrameと同じように扱えるような思想。\n","class LitDataset(Dataset):\n","    def __init__(self, df, inference_only=False):\n","        super().__init__()\n","\n","        self.df = df        \n","        self.inference_only = inference_only # Testデータ用フラグ\n","        self.text = df.excerpt.tolist() # 分析対象カラムをlistにする。(分かち書きではなく、Seriesをlistへ変換するような処理)\n","        #self.text = [text.replace(\"\\n\", \" \") for text in self.text] # 単語単位で分かち書きする場合\n","        \n","        if not self.inference_only:\n","            self.target = torch.tensor(df.target.values, dtype=torch.float32) # trainのみ、targetをtensorに変換\n","            self.standard_error = torch.tensor(df.standard_error.values, dtype=torch.float32) \n","\n","        self.encoded = tokenizer.batch_encode_plus( # textをtokenize\n","            self.text,\n","            padding = 'max_length',            \n","            max_length = MAX_LEN,\n","            truncation = True, # 最大長を超える文字は切り捨て\n","            return_attention_mask=True\n","        )        \n"," \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, index): # 変換結果を返す\n","        input_ids = torch.tensor(self.encoded['input_ids'][index])\n","        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n","        \n","        if self.inference_only:\n","            return (input_ids, attention_mask)            \n","        else:\n","            target = self.target[index]\n","            standard_error = self.standard_error[index]\n","            return (input_ids, attention_mask, target, standard_error)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKtdy32wXecG"},"source":["# Model\n","The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.649629Z","iopub.execute_input":"2021-07-04T06:26:40.650066Z","iopub.status.idle":"2021-07-04T06:26:40.666374Z","shell.execute_reply.started":"2021-07-04T06:26:40.650002Z","shell.execute_reply":"2021-07-04T06:26:40.665211Z"},"trusted":true,"id":"BpkxjXEUXecH"},"source":["class LitModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(ROBERTA_PATH) # pretrainedからconfigを読み込み\n","        config.update({\"output_hidden_states\":True, # config更新: embedding層を抽出\n","                       \"hidden_dropout_prob\": 0.0, # config更新: dropoutしない\n","                       \"layer_norm_eps\": 1e-7}) # config更新: layer normalizationのepsilon                      \n","        \n","        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config) # cpuで処理する\n","            \n","        self.attention = nn.Sequential(# attentionレイヤー            \n","            nn.Linear(config.hidden_size, 512),      \n","            nn.Tanh(),                       \n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","\n","        self.regressor = nn.Sequential( # 出力レイヤー                    \n","            nn.Linear(config.hidden_size, 2)                        \n","        )\n","\n","    def forward(self, input_ids, attention_mask):\n","        roberta_output = self.roberta(input_ids=input_ids, # robertaに入力データを流し、出力としてrobertaモデル(layerの複合体)を得る\n","                                      attention_mask=attention_mask)     \n","\n","        last_hidden_state = roberta_output.hidden_states[-1] # robertaモデルの最後のlayerを得る\n","        weights = self.attention(last_hidden_state) # robertaの最後のlayerをattentionへ入力し、出力として重みを得る                \n","        context_vector = torch.sum(weights * last_hidden_state, dim=1) # 重み×最後の層を足し合わせて文書ベクトルとする。\n","        return self.regressor(context_vector) # 文書ベクトルを線形層に入力し、targetを出力する\n","\n","        # https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently\n","        #last_hidden_state = roberta_output[0]\n","        #input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        #sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        #sum_mask = input_mask_expanded.sum(1)\n","        #sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        #mean_embeddings = sum_embeddings / sum_mask\n","\n","        \n","        # Now we reduce the context vector to the prediction score.\n","        #return self.regressor(mean_embeddings) # 文書ベクトルを線形層に入力し、targetを出力する"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.672515Z","iopub.execute_input":"2021-07-04T06:26:40.672944Z","iopub.status.idle":"2021-07-04T06:26:40.684593Z","shell.execute_reply.started":"2021-07-04T06:26:40.672908Z","shell.execute_reply":"2021-07-04T06:26:40.683569Z"},"trusted":true,"id":"bB4jvQTxXecH"},"source":["# 評価指標(MSE)の計算。最終的に、ルートしてRMSEにすると思われる。\n","def eval_mse(model, data_loader):\n","    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモードを選択。Batch Normとかdropoutをしなくなる           \n","    mse_mean_sum = 0\n","    mse_std_sum = 0\n","\n","    with torch.no_grad(): # 勾配の計算をしないBlock\n","        for batch_num, (input_ids, attention_mask, target, standard_error) in enumerate(data_loader): # data_loaderからinput, attentin_mask, targetをbatchごとに取り出す\n","            input_ids = input_ids.to(DEVICE)   \n","            attention_mask = attention_mask.to(DEVICE)   \n","            target = target.to(DEVICE)      \n","            standard_error = standard_error.to(DEVICE) \n","            \n","            output = model(input_ids, attention_mask) # 取得した値をモデルへ入力し、出力として予測値を得る。\n","\n","            mse_mean_sum += nn.MSELoss(reduction=\"sum\")(output[:,0].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","            mse_std_sum += nn.MSELoss(reduction=\"sum\")(output[:,1].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","\n","    del input_ids\n","    del attention_mask\n","    del target\n","\n","    mse_mean_result = mse_mean_sum / len(data_loader.dataset)\n","    mse_std_result = mse_std_sum / len(data_loader.dataset)\n","  \n","    return mse_mean_result, mse_std_result # 誤差の合計をdataset長で除し、mseを取得＆返す"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.690155Z","iopub.execute_input":"2021-07-04T06:26:40.692530Z","iopub.status.idle":"2021-07-04T06:26:40.703425Z","shell.execute_reply.started":"2021-07-04T06:26:40.692488Z","shell.execute_reply":"2021-07-04T06:26:40.702366Z"},"trusted":true,"id":"47bDno_LXecI"},"source":["# 推論結果を返す\n","def predict(model, data_loader):\n","    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモード(dropout, batch_normしない)\n","\n","    result = np.zeros(len(data_loader.dataset)) # 結果をdataset長のzero配列として用意\n","    index = 0\n","    \n","    with torch.no_grad(): # 勾配の計算をしないblock(inputすると、現状の重みによる推論結果を返す)\n","        for batch_num, (input_ids, attention_mask) in enumerate(data_loader): # data_loaderからbatchごとにinputを得る\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)\n","                        \n","            output = model(input_ids, attention_mask) # modelにinputを入力し、予測結果を得る。\n","\n","            result[index : index + output[:,0].shape[0]] = output[:,0].flatten().to(\"cpu\") # result[index ~ predの長さ]へ、予測結果を格納\n","            index += pred.shape[0] # indexを更新\n","\n","    return result # 全batchで推論が終わったら、結果を返す"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.708605Z","iopub.execute_input":"2021-07-04T06:26:40.709024Z","iopub.status.idle":"2021-07-04T06:26:40.730675Z","shell.execute_reply.started":"2021-07-04T06:26:40.708983Z","shell.execute_reply":"2021-07-04T06:26:40.729705Z"},"trusted":true,"id":"oInneuAmXecI"},"source":["# 学習\n","def train(model, # モデル\n","          model_path, # モデルのアウトプット先\n","          train_loader, # train-setのdata_loader\n","          val_loader, # valid-setのdata_loader\n","          optimizer, # optimizer\n","          scheduler=None, # scheduler, デフォルトはNone\n","          num_epochs=NUM_EPOCHS # epoch数、notebook冒頭で指定した値\n","         ):    \n","    \n","    best_val_rmse = None\n","    best_epoch = 0\n","    step = 0\n","    last_eval_step = 0\n","    eval_period = EVAL_SCHEDULE[0][1] # eval期間(って何？) 冒頭で決めたEVAL_SCHEDULEの最初のtupleの[1]を取得\n","\n","    start = time.time() # 時間計測用\n","\n","    for epoch in range(num_epochs): # 指定したEpoch数だけ繰り返し\n","        val_rmse = None         \n","\n","        for batch_num, (input_ids, attention_mask, target, standard_error) in enumerate(train_loader): # train_loaderからinput, targetを取得\n","            input_ids = input_ids.to(DEVICE) # inputをDEVICEへ突っ込む\n","            attention_mask = attention_mask.to(DEVICE)       \n","            target = target.to(DEVICE)\n","            standard_error = standard_error.to(DEVICE)  \n","\n","            optimizer.zero_grad() # 勾配を初期化            \n","            model.train() # 学習モード開始\n","\n","            # https://www.kaggle.com/c/commonlitreadabilityprize/discussion/239421\n","            output = model(input_ids, attention_mask) # input,attention_maskを入力し、予測結果を得る\n","            p = torch.distributions.Normal(output[:,0], torch.sqrt(output[:,1]**2))\n","            q = torch.distributions.Normal(target, standard_error)\n","            kl_vector = torch.distributions.kl_divergence(p, q)\n","            loss = kl_vector.mean()\n","\n","            loss.backward() # 誤差逆伝播法により勾配を得る\n","            optimizer.step() # 重みを更新する\n","\n","            if scheduler:\n","                scheduler.step() # schedulerが与えられた場合は、schedulerの学習率更新\n","            \n","            if step >= last_eval_step + eval_period: # batchを回すごとにstepを増やしていって、「前回evalしたstep + eval_period(16)」を超えたら実行。\n","                # Evaluate the model on val_loader.\n","                elapsed_seconds = time.time() - start # 経過時間\n","                num_steps = step - last_eval_step # 経過ステップ数\n","                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n","                last_eval_step = step # 前回stepの更新\n","                \n","                # valid-setによるrmse計算\n","                train_mean_mse = nn.MSELoss(reduction=\"mean\")(output[:,0].flatten(), target) \n","                train_std_mse = nn.MSELoss(reduction=\"mean\")(torch.sqrt(output[:,1]**2).flatten(), standard_error) \n","\n","                train_mean_rmse = math.sqrt(train_mean_mse)\n","                train_std_rmse = math.sqrt(train_std_mse)\n","\n","                val_mean_mse, val_std_mse = eval_mse(model, val_loader)\n","                val_mean_rmse = math.sqrt(val_mean_mse)                            \n","                val_std_rmse = math.sqrt(val_std_mse)                            \n","\n","                print(f\"Epoch: {epoch} batch_num: {batch_num}\")\n","                print(f\"train_rmse_target: {train_mean_rmse:0.4}\",\n","                      f\"train_rmse_stderror: {train_std_rmse:0.4}\",\n","                      f\"train_kl_div: {loss:0.4}\",\n","                      )\n","                print(f\"val_rmse_target: {val_mean_rmse:0.4}\",\n","                      f\"val_rmse_stderror: {val_std_rmse:0.4}\"\n","                      )\n","\n","                for rmse, period in EVAL_SCHEDULE: # eval_periodをvalid-rmseで切り替える処理\n","                    if val_mean_rmse >= rmse: # valid rmseをEVAL_SCHEDULEと比較し、0項 > valid rmseとなるまで回す : EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n","                        eval_period = period # eval_periodを更新\n","                        break                               \n","\n","                if not best_val_rmse or val_mean_rmse < best_val_rmse: # 初回(best_val_rmse==None), またはbest_val_rmseを更新したらモデルを保存する\n","                    best_val_rmse = val_mean_rmse\n","                    best_epoch = epoch\n","                    torch.save(model.state_dict(), model_path) # 最高の自分を保存\n","                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n","                else:       \n","                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\", # 更新されない場合は、元のスコアを表示\n","                          f\"(from epoch {best_epoch})\")      \n","                                                  \n","                start = time.time()\n","            \n","            # batchごとにメモリ解放\n","            del input_ids\n","            del attention_mask\n","            del target\n","            torch.cuda.empty_cache()                                            \n","            step += 1\n","    \n","    return best_val_rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.735798Z","iopub.execute_input":"2021-07-04T06:26:40.738398Z","iopub.status.idle":"2021-07-04T06:26:40.750876Z","shell.execute_reply.started":"2021-07-04T06:26:40.738356Z","shell.execute_reply":"2021-07-04T06:26:40.749635Z"},"trusted":true,"id":"rMY0fjXwXecJ"},"source":["# optimizerの作成\n","def create_optimizer(model):\n","    named_parameters = list(model.named_parameters()) # モデルパラメータの取得\n","    \n","    roberta_parameters = list(model.roberta.named_parameters())[:-2] # パラメータをroberta用、attention用、regressor用に格納。(直接引っ張ってくる形式に変更)\n","    attention_parameters = list(model.attention.named_parameters())\n","    regressor_parameters = list(model.regressor.named_parameters())\n","        \n","    attention_group = [params for (name, params) in attention_parameters] # attention用パラメータをリストとして取得\n","    regressor_group = [params for (name, params) in regressor_parameters] # reg用パラメータをリストとして取得\n","\n","    parameters = []\n","    parameters.append({\"params\": attention_group}) # パラメータをリストに辞書として格納していく\n","    parameters.append({\"params\": regressor_group})\n","\n","    for layer_num, (name, params) in enumerate(roberta_parameters): # レイヤーごとにname, paramsを取得していろんな処理\n","        weight_decay = 0.0 if \"bias\" in name else 0.01\n","\n","        lr = 8e-6\n","\n","        if layer_num >= 69:        \n","            lr = 2e-5\n","\n","        if layer_num >= 133:\n","            lr = 4e-5\n","\n","        parameters.append({\"params\": params,\n","                           \"weight_decay\": weight_decay,\n","                           \"lr\": lr})\n","\n","    return AdamW(parameters) # 最終的に、AdamWにパラメータを入力する。\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbaJojz0Zjif"},"source":["# https://www.kaggle.com/abhishek/step-1-create-folds\n","def create_folds(data, num_splits, SEED, return_df=False):\n","    # we create a new column called kfold and fill it with -1\n","    data[\"kfold\"] = -1\n","    \n","    # the next step is to randomize the rows of the data\n","    data = data.sample(frac=1).reset_index(drop=True)\n","\n","    # calculate number of bins by Sturge's rule\n","    # I take the floor of the value, you can also\n","    # just round it\n","    num_bins = int(np.floor(1 + np.log2(len(data))))\n","    \n","    # bin targets\n","    data.loc[:, \"bins_tg\"] = pd.cut(\n","        data[\"target\"], bins=num_bins, labels=False\n","    ).map(lambda x: str(x))\n","\n","    # bin standard_error\n","    data.loc[:, \"bins_std\"] = pd.cut(\n","        data[\"standard_error\"], bins=num_bins, labels=False\n","    )\n","\n","    # bins\n","    data.loc[:, \"bins\"] = data['bins_tg'].map(lambda x: str(x)) + data['bins_std'].map(lambda x: str(x))\n","\n","    # initiate the kfold class from model_selection module\n","    kf = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n","\n","    # note that, instead of targets, we use bins!\n","    if return_df:\n","      for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n","        data.loc[v_, 'kfold'] = f\n","      return data\n","    else:\n","      return kf.split(X=data, y=data.bins.values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"id":"vAmhaYaylMk5","executionInfo":{"status":"ok","timestamp":1626777942455,"user_tz":-540,"elapsed":9,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"9277d655-06c1-422b-edd1-ad836ebf2e1a"},"source":["# 検証用\n","SEED = 1000\n","st_kfold_bins_df = create_folds(train_df, num_splits=5, SEED=SEED, return_df=True)\n","st_kfold_bins_df['bins_tg'] = st_kfold_bins_df['bins_tg'].map(lambda x: float(x))\n","st_kfold_bins_df['bins_std'] = st_kfold_bins_df['bins_std'].map(lambda x: float(x))\n","st_kfold_bins_df.groupby('kfold').agg({'bins_tg': ['min', 'max', 'mean'],\n","                                    'bins_std': ['min', 'max', 'mean']})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n","  % (min_groups, self.n_splits)), UserWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"3\" halign=\"left\">bins_tg</th>\n","      <th colspan=\"3\" halign=\"left\">bins_std</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th>min</th>\n","      <th>max</th>\n","      <th>mean</th>\n","      <th>min</th>\n","      <th>max</th>\n","      <th>mean</th>\n","    </tr>\n","    <tr>\n","      <th>kfold</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.525573</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.927690</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.566138</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.927690</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.555556</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.954145</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.544170</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.909894</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>5.558304</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>2.925795</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      bins_tg                 bins_std                \n","          min   max      mean      min   max      mean\n","kfold                                                 \n","0         0.0  11.0  5.525573      0.0  11.0  2.927690\n","1         0.0  11.0  5.566138      0.0  11.0  2.927690\n","2         0.0  11.0  5.555556      0.0  11.0  2.954145\n","3         0.0  11.0  5.544170      0.0  11.0  2.909894\n","4         0.0  11.0  5.558304      0.0  11.0  2.925795"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"TyjgRCu3mmqG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PLKHwvKtNBn"},"source":["def train_and_save_model(train_indices, val_indices, model_path):\n","    train_dataset = LitDataset(train_df.loc[train_indices]) # train, validのDataset\n","    val_dataset = LitDataset(train_df.loc[val_indices])\n","        \n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              drop_last=True, shuffle=True, num_workers=2) # train, validのDataLoader\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                            drop_last=False, shuffle=False, num_workers=2)    \n","\n","    model = LitModel().to(DEVICE) # modelをDEVICEへぶち込む\n","    optimizer = create_optimizer(model) # optimizerをモデルから作成\n","    scheduler = get_cosine_schedule_with_warmup( # schedulerを作成\n","        optimizer,\n","        num_training_steps=NUM_EPOCHS * len(train_loader),\n","        num_warmup_steps=50)    \n","    rmse = train(model, model_path, train_loader, val_loader, optimizer, scheduler=scheduler)\n","\n","    del train_dataset\n","    del val_dataset\n","    del train_loader\n","    del val_loader\n","    del model\n","    del optimizer\n","    del scheduler\n","    gc.collect() \n","    torch.cuda.empty_cache()\n","    return rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.755813Z","iopub.execute_input":"2021-07-04T06:26:40.758373Z","iopub.status.idle":"2021-07-04T06:27:12.493221Z","shell.execute_reply.started":"2021-07-04T06:26:40.758265Z","shell.execute_reply":"2021-07-04T06:27:12.490139Z"},"trusted":true,"id":"k2LGJD3XXecK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626789620997,"user_tz":-540,"elapsed":11673111,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"63951d3d-6d28-4572-a594-6dfa7cbc90ed"},"source":["# 実行処理。 KFold & 学習\n","SEED = 1000\n","list_val_rmse = []\n","\n","#kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n","kfold = create_folds(train_df, 5, SEED=SEED, return_df=False) # binsで切る場合\n","\n","for fold, (train_indices, val_indices) in enumerate(kfold):    \n","    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n","    print(gpuinfo())\n","    model_path = f\"model_{fold + 1}.pth\" # model_fold数_.pth\n","    set_random_seed(SEED + fold) # SEEDはfold別に変わるようにする\n","    list_val_rmse.append(train_and_save_model(train_indices, val_indices, model_path))\n","\n","    print(\"\\nPerformance estimates:\")\n","    print(list_val_rmse)\n","    print(\"Mean:\", np.array(list_val_rmse).mean())\n","    print(gpuinfo())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n","  % (min_groups, self.n_splits)), UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Fold 1/5\n","{'total_MiB': 16280, 'used_MiB': 2}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 82.5 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.7521 train_rmse_stderror: 0.06717 train_kl_div: 1.196\n","val_rmse_target: 0.8965 val_rmse_stderror: 1.792\n","New best_val_rmse: 0.8965\n","\n","64 steps took 81.4 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.5372 train_rmse_stderror: 0.0463 train_kl_div: 0.5867\n","val_rmse_target: 0.5725 val_rmse_stderror: 1.806\n","New best_val_rmse: 0.5725\n","\n","64 steps took 81.6 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.3204 train_rmse_stderror: 0.04765 train_kl_div: 0.2313\n","val_rmse_target: 0.5587 val_rmse_stderror: 1.838\n","New best_val_rmse: 0.5587\n","\n","64 steps took 81.4 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.2622 train_rmse_stderror: 0.04158 train_kl_div: 0.1584\n","val_rmse_target: 0.539 val_rmse_stderror: 1.765\n","New best_val_rmse: 0.539\n","\n","32 steps took 40.7 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.4855 train_rmse_stderror: 0.03749 train_kl_div: 0.4394\n","val_rmse_target: 0.5386 val_rmse_stderror: 1.814\n","New best_val_rmse: 0.5386\n","\n","32 steps took 40.7 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.4974 train_rmse_stderror: 0.041 train_kl_div: 0.5465\n","val_rmse_target: 0.5674 val_rmse_stderror: 1.803\n","Still best_val_rmse: 0.5386 (from epoch 1)\n","\n","64 steps took 81.6 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.2833 train_rmse_stderror: 0.03986 train_kl_div: 0.1769\n","val_rmse_target: 0.5271 val_rmse_stderror: 1.803\n","New best_val_rmse: 0.5271\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2613 train_rmse_stderror: 0.04047 train_kl_div: 0.1348\n","val_rmse_target: 0.512 val_rmse_stderror: 1.803\n","New best_val_rmse: 0.512\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2391 train_rmse_stderror: 0.02437 train_kl_div: 0.126\n","val_rmse_target: 0.5066 val_rmse_stderror: 1.797\n","New best_val_rmse: 0.5066\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.3822 train_rmse_stderror: 0.03936 train_kl_div: 0.2463\n","val_rmse_target: 0.5321 val_rmse_stderror: 1.793\n","Still best_val_rmse: 0.5066 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.5016 train_rmse_stderror: 0.03363 train_kl_div: 0.3727\n","val_rmse_target: 0.5154 val_rmse_stderror: 1.801\n","Still best_val_rmse: 0.5066 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.299 train_rmse_stderror: 0.0233 train_kl_div: 0.1987\n","val_rmse_target: 0.4895 val_rmse_stderror: 1.815\n","New best_val_rmse: 0.4895\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 176\n","train_rmse_target: 0.1465 train_rmse_stderror: 0.02225 train_kl_div: 0.04615\n","val_rmse_target: 0.4904 val_rmse_stderror: 1.804\n","Still best_val_rmse: 0.4895 (from epoch 2)\n","\n","16 steps took 20.6 seconds\n","Epoch: 3 batch_num: 4\n","train_rmse_target: 0.1753 train_rmse_stderror: 0.01848 train_kl_div: 0.06744\n","val_rmse_target: 0.492 val_rmse_stderror: 1.801\n","Still best_val_rmse: 0.4895 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 20\n","train_rmse_target: 0.2054 train_rmse_stderror: 0.02525 train_kl_div: 0.09017\n","val_rmse_target: 0.492 val_rmse_stderror: 1.807\n","Still best_val_rmse: 0.4895 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 36\n","train_rmse_target: 0.1778 train_rmse_stderror: 0.02285 train_kl_div: 0.06731\n","val_rmse_target: 0.5113 val_rmse_stderror: 1.801\n","Still best_val_rmse: 0.4895 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 68\n","train_rmse_target: 0.1493 train_rmse_stderror: 0.02448 train_kl_div: 0.05663\n","val_rmse_target: 0.4933 val_rmse_stderror: 1.81\n","Still best_val_rmse: 0.4895 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 84\n","train_rmse_target: 0.2451 train_rmse_stderror: 0.03615 train_kl_div: 0.1164\n","val_rmse_target: 0.4993 val_rmse_stderror: 1.813\n","Still best_val_rmse: 0.4895 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 100\n","train_rmse_target: 0.1178 train_rmse_stderror: 0.02314 train_kl_div: 0.03139\n","val_rmse_target: 0.4914 val_rmse_stderror: 1.81\n","Still best_val_rmse: 0.4895 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 116\n","train_rmse_target: 0.1607 train_rmse_stderror: 0.01709 train_kl_div: 0.0575\n","val_rmse_target: 0.4913 val_rmse_stderror: 1.803\n","Still best_val_rmse: 0.4895 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 132\n","train_rmse_target: 0.1246 train_rmse_stderror: 0.02409 train_kl_div: 0.03688\n","val_rmse_target: 0.4886 val_rmse_stderror: 1.815\n","New best_val_rmse: 0.4886\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1114 train_rmse_stderror: 0.0233 train_kl_div: 0.02587\n","val_rmse_target: 0.4877 val_rmse_stderror: 1.806\n","New best_val_rmse: 0.4877\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 148\n","train_rmse_target: 0.1543 train_rmse_stderror: 0.01819 train_kl_div: 0.04954\n","val_rmse_target: 0.4956 val_rmse_stderror: 1.802\n","Still best_val_rmse: 0.4877 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 164\n","train_rmse_target: 0.1831 train_rmse_stderror: 0.02219 train_kl_div: 0.07483\n","val_rmse_target: 0.4881 val_rmse_stderror: 1.808\n","Still best_val_rmse: 0.4877 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.2011 train_rmse_stderror: 0.02129 train_kl_div: 0.08885\n","val_rmse_target: 0.4869 val_rmse_stderror: 1.811\n","New best_val_rmse: 0.4869\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 180\n","train_rmse_target: 0.1226 train_rmse_stderror: 0.02184 train_kl_div: 0.03229\n","val_rmse_target: 0.4904 val_rmse_stderror: 1.808\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","16 steps took 20.5 seconds\n","Epoch: 4 batch_num: 8\n","train_rmse_target: 0.156 train_rmse_stderror: 0.02413 train_kl_div: 0.05058\n","val_rmse_target: 0.4896 val_rmse_stderror: 1.81\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1061 train_rmse_stderror: 0.03282 train_kl_div: 0.02418\n","val_rmse_target: 0.4891 val_rmse_stderror: 1.807\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 24\n","train_rmse_target: 0.1413 train_rmse_stderror: 0.02038 train_kl_div: 0.04246\n","val_rmse_target: 0.4893 val_rmse_stderror: 1.806\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 32\n","train_rmse_target: 0.1288 train_rmse_stderror: 0.02095 train_kl_div: 0.03478\n","val_rmse_target: 0.4914 val_rmse_stderror: 1.804\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1128 train_rmse_stderror: 0.01697 train_kl_div: 0.02798\n","val_rmse_target: 0.4883 val_rmse_stderror: 1.811\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 56\n","train_rmse_target: 0.08575 train_rmse_stderror: 0.0223 train_kl_div: 0.01809\n","val_rmse_target: 0.4904 val_rmse_stderror: 1.809\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 72\n","train_rmse_target: 0.1259 train_rmse_stderror: 0.02046 train_kl_div: 0.03444\n","val_rmse_target: 0.4878 val_rmse_stderror: 1.809\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.141 train_rmse_stderror: 0.02389 train_kl_div: 0.03876\n","val_rmse_target: 0.4885 val_rmse_stderror: 1.806\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 88\n","train_rmse_target: 0.2116 train_rmse_stderror: 0.02926 train_kl_div: 0.06915\n","val_rmse_target: 0.4893 val_rmse_stderror: 1.805\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 96\n","train_rmse_target: 0.1072 train_rmse_stderror: 0.02534 train_kl_div: 0.02711\n","val_rmse_target: 0.4895 val_rmse_stderror: 1.805\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 104\n","train_rmse_target: 0.1137 train_rmse_stderror: 0.02148 train_kl_div: 0.02916\n","val_rmse_target: 0.4886 val_rmse_stderror: 1.806\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.08172 train_rmse_stderror: 0.0269 train_kl_div: 0.01689\n","val_rmse_target: 0.4889 val_rmse_stderror: 1.806\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 120\n","train_rmse_target: 0.09599 train_rmse_stderror: 0.03171 train_kl_div: 0.01695\n","val_rmse_target: 0.4895 val_rmse_stderror: 1.806\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 128\n","train_rmse_target: 0.1104 train_rmse_stderror: 0.03049 train_kl_div: 0.03007\n","val_rmse_target: 0.4896 val_rmse_stderror: 1.807\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 136\n","train_rmse_target: 0.0864 train_rmse_stderror: 0.02543 train_kl_div: 0.01566\n","val_rmse_target: 0.4896 val_rmse_stderror: 1.808\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.06496 train_rmse_stderror: 0.02377 train_kl_div: 0.01102\n","val_rmse_target: 0.4897 val_rmse_stderror: 1.808\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 152\n","train_rmse_target: 0.1095 train_rmse_stderror: 0.0277 train_kl_div: 0.02914\n","val_rmse_target: 0.49 val_rmse_stderror: 1.809\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 160\n","train_rmse_target: 0.05933 train_rmse_stderror: 0.01439 train_kl_div: 0.008034\n","val_rmse_target: 0.49 val_rmse_stderror: 1.809\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 168\n","train_rmse_target: 0.1153 train_rmse_stderror: 0.02412 train_kl_div: 0.02822\n","val_rmse_target: 0.4898 val_rmse_stderror: 1.809\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1463 train_rmse_stderror: 0.02001 train_kl_div: 0.04859\n","val_rmse_target: 0.4898 val_rmse_stderror: 1.809\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 184\n","train_rmse_target: 0.08765 train_rmse_stderror: 0.01322 train_kl_div: 0.01683\n","val_rmse_target: 0.4898 val_rmse_stderror: 1.809\n","Still best_val_rmse: 0.4869 (from epoch 3)\n","\n","Performance estimates:\n","[0.48686168177282346]\n","Mean: 0.48686168177282346\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 2/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 82.3 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.7402 train_rmse_stderror: 0.09082 train_kl_div: 1.138\n","val_rmse_target: 0.7776 val_rmse_stderror: 1.085\n","New best_val_rmse: 0.7776\n","\n","64 steps took 81.4 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.5405 train_rmse_stderror: 0.0488 train_kl_div: 0.6511\n","val_rmse_target: 0.575 val_rmse_stderror: 1.122\n","New best_val_rmse: 0.575\n","\n","64 steps took 81.6 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.5065 train_rmse_stderror: 0.03096 train_kl_div: 0.5399\n","val_rmse_target: 0.6494 val_rmse_stderror: 1.087\n","Still best_val_rmse: 0.575 (from epoch 0)\n","\n","64 steps took 81.3 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.4258 train_rmse_stderror: 0.02645 train_kl_div: 0.3593\n","val_rmse_target: 0.5637 val_rmse_stderror: 1.095\n","New best_val_rmse: 0.5637\n","\n","64 steps took 81.4 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.5015 train_rmse_stderror: 0.03967 train_kl_div: 0.4643\n","val_rmse_target: 0.5173 val_rmse_stderror: 1.1\n","New best_val_rmse: 0.5173\n","\n","32 steps took 40.7 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.5282 train_rmse_stderror: 0.04334 train_kl_div: 0.5379\n","val_rmse_target: 0.5323 val_rmse_stderror: 1.086\n","Still best_val_rmse: 0.5173 (from epoch 1)\n","\n","32 steps took 40.9 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.2281 train_rmse_stderror: 0.04279 train_kl_div: 0.1156\n","val_rmse_target: 0.5141 val_rmse_stderror: 1.113\n","New best_val_rmse: 0.5141\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2313 train_rmse_stderror: 0.03685 train_kl_div: 0.1032\n","val_rmse_target: 0.5162 val_rmse_stderror: 1.109\n","Still best_val_rmse: 0.5141 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2727 train_rmse_stderror: 0.03468 train_kl_div: 0.1573\n","val_rmse_target: 0.5067 val_rmse_stderror: 1.094\n","New best_val_rmse: 0.5067\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.3273 train_rmse_stderror: 0.03975 train_kl_div: 0.1974\n","val_rmse_target: 0.5024 val_rmse_stderror: 1.106\n","New best_val_rmse: 0.5024\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.2731 train_rmse_stderror: 0.04567 train_kl_div: 0.1388\n","val_rmse_target: 0.5184 val_rmse_stderror: 1.108\n","Still best_val_rmse: 0.5024 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.2834 train_rmse_stderror: 0.02346 train_kl_div: 0.1778\n","val_rmse_target: 0.4975 val_rmse_stderror: 1.105\n","New best_val_rmse: 0.4975\n","\n","16 steps took 20.3 seconds\n","Epoch: 2 batch_num: 184\n","train_rmse_target: 0.3262 train_rmse_stderror: 0.02557 train_kl_div: 0.2192\n","val_rmse_target: 0.5178 val_rmse_stderror: 1.098\n","Still best_val_rmse: 0.4975 (from epoch 2)\n","\n","32 steps took 40.9 seconds\n","Epoch: 3 batch_num: 28\n","train_rmse_target: 0.1109 train_rmse_stderror: 0.02389 train_kl_div: 0.02772\n","val_rmse_target: 0.4932 val_rmse_stderror: 1.104\n","New best_val_rmse: 0.4932\n","\n","16 steps took 20.4 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.1772 train_rmse_stderror: 0.02452 train_kl_div: 0.0728\n","val_rmse_target: 0.4985 val_rmse_stderror: 1.099\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 60\n","train_rmse_target: 0.1283 train_rmse_stderror: 0.02531 train_kl_div: 0.03637\n","val_rmse_target: 0.4932 val_rmse_stderror: 1.105\n","New best_val_rmse: 0.4932\n","\n","16 steps took 20.4 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1523 train_rmse_stderror: 0.0305 train_kl_div: 0.05575\n","val_rmse_target: 0.4963 val_rmse_stderror: 1.106\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 92\n","train_rmse_target: 0.09921 train_rmse_stderror: 0.02053 train_kl_div: 0.02113\n","val_rmse_target: 0.4954 val_rmse_stderror: 1.106\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1144 train_rmse_stderror: 0.0226 train_kl_div: 0.02991\n","val_rmse_target: 0.4947 val_rmse_stderror: 1.105\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 124\n","train_rmse_target: 0.1858 train_rmse_stderror: 0.02256 train_kl_div: 0.06078\n","val_rmse_target: 0.4991 val_rmse_stderror: 1.106\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1008 train_rmse_stderror: 0.01386 train_kl_div: 0.02072\n","val_rmse_target: 0.4951 val_rmse_stderror: 1.103\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 156\n","train_rmse_target: 0.1012 train_rmse_stderror: 0.03388 train_kl_div: 0.02797\n","val_rmse_target: 0.4941 val_rmse_stderror: 1.11\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.2444 train_rmse_stderror: 0.03081 train_kl_div: 0.0943\n","val_rmse_target: 0.4965 val_rmse_stderror: 1.109\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.6 seconds\n","Epoch: 4 batch_num: 0\n","train_rmse_target: 0.06749 train_rmse_stderror: 0.01854 train_kl_div: 0.009199\n","val_rmse_target: 0.4966 val_rmse_stderror: 1.105\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.07366 train_rmse_stderror: 0.02495 train_kl_div: 0.01397\n","val_rmse_target: 0.4942 val_rmse_stderror: 1.106\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 32\n","train_rmse_target: 0.08636 train_rmse_stderror: 0.0236 train_kl_div: 0.01805\n","val_rmse_target: 0.4956 val_rmse_stderror: 1.106\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1093 train_rmse_stderror: 0.01727 train_kl_div: 0.02776\n","val_rmse_target: 0.4951 val_rmse_stderror: 1.105\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 64\n","train_rmse_target: 0.08256 train_rmse_stderror: 0.01939 train_kl_div: 0.01523\n","val_rmse_target: 0.4947 val_rmse_stderror: 1.102\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.0889 train_rmse_stderror: 0.02396 train_kl_div: 0.01764\n","val_rmse_target: 0.4939 val_rmse_stderror: 1.103\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 96\n","train_rmse_target: 0.06025 train_rmse_stderror: 0.02788 train_kl_div: 0.009553\n","val_rmse_target: 0.4958 val_rmse_stderror: 1.106\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.0845 train_rmse_stderror: 0.01568 train_kl_div: 0.01655\n","val_rmse_target: 0.4941 val_rmse_stderror: 1.103\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 128\n","train_rmse_target: 0.1056 train_rmse_stderror: 0.01871 train_kl_div: 0.02481\n","val_rmse_target: 0.4943 val_rmse_stderror: 1.103\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.07115 train_rmse_stderror: 0.0173 train_kl_div: 0.01182\n","val_rmse_target: 0.4941 val_rmse_stderror: 1.103\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 160\n","train_rmse_target: 0.1091 train_rmse_stderror: 0.02109 train_kl_div: 0.02438\n","val_rmse_target: 0.4942 val_rmse_stderror: 1.103\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1248 train_rmse_stderror: 0.02397 train_kl_div: 0.03073\n","val_rmse_target: 0.4943 val_rmse_stderror: 1.103\n","Still best_val_rmse: 0.4932 (from epoch 3)\n","\n","Performance estimates:\n","[0.48686168177282346, 0.49318919309887377]\n","Mean: 0.4900254374358486\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 3/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 82.4 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.6275 train_rmse_stderror: 0.04589 train_kl_div: 0.8451\n","val_rmse_target: 0.5935 val_rmse_stderror: 1.737\n","New best_val_rmse: 0.5935\n","\n","64 steps took 81.4 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.4705 train_rmse_stderror: 0.03664 train_kl_div: 0.4768\n","val_rmse_target: 0.5626 val_rmse_stderror: 1.728\n","New best_val_rmse: 0.5626\n","\n","64 steps took 81.6 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.5484 train_rmse_stderror: 0.03624 train_kl_div: 0.6203\n","val_rmse_target: 0.6367 val_rmse_stderror: 1.723\n","Still best_val_rmse: 0.5626 (from epoch 0)\n","\n","64 steps took 81.3 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.3763 train_rmse_stderror: 0.04364 train_kl_div: 0.3227\n","val_rmse_target: 0.57 val_rmse_stderror: 1.761\n","Still best_val_rmse: 0.5626 (from epoch 0)\n","\n","64 steps took 81.4 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.3505 train_rmse_stderror: 0.03971 train_kl_div: 0.2271\n","val_rmse_target: 0.4959 val_rmse_stderror: 1.729\n","New best_val_rmse: 0.4959\n","\n","16 steps took 20.4 seconds\n","Epoch: 1 batch_num: 148\n","train_rmse_target: 0.3377 train_rmse_stderror: 0.04319 train_kl_div: 0.2527\n","val_rmse_target: 0.5543 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4959 (from epoch 1)\n","\n","64 steps took 81.6 seconds\n","Epoch: 2 batch_num: 24\n","train_rmse_target: 0.2394 train_rmse_stderror: 0.02606 train_kl_div: 0.1186\n","val_rmse_target: 0.4985 val_rmse_stderror: 1.763\n","Still best_val_rmse: 0.4959 (from epoch 1)\n","\n","16 steps took 20.3 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.4032 train_rmse_stderror: 0.02826 train_kl_div: 0.3204\n","val_rmse_target: 0.4902 val_rmse_stderror: 1.74\n","New best_val_rmse: 0.4902\n","\n","16 steps took 20.4 seconds\n","Epoch: 2 batch_num: 56\n","train_rmse_target: 0.3268 train_rmse_stderror: 0.03109 train_kl_div: 0.2181\n","val_rmse_target: 0.4855 val_rmse_stderror: 1.761\n","New best_val_rmse: 0.4855\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 64\n","train_rmse_target: 0.2565 train_rmse_stderror: 0.02707 train_kl_div: 0.1358\n","val_rmse_target: 0.4808 val_rmse_stderror: 1.739\n","New best_val_rmse: 0.4808\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.4429 train_rmse_stderror: 0.04621 train_kl_div: 0.4531\n","val_rmse_target: 0.4768 val_rmse_stderror: 1.736\n","New best_val_rmse: 0.4768\n","\n","4 steps took 5.08 seconds\n","Epoch: 2 batch_num: 76\n","train_rmse_target: 0.3737 train_rmse_stderror: 0.03476 train_kl_div: 0.2216\n","val_rmse_target: 0.4846 val_rmse_stderror: 1.751\n","Still best_val_rmse: 0.4768 (from epoch 2)\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 84\n","train_rmse_target: 0.2866 train_rmse_stderror: 0.02353 train_kl_div: 0.1624\n","val_rmse_target: 0.4766 val_rmse_stderror: 1.732\n","New best_val_rmse: 0.4766\n","\n","4 steps took 5.11 seconds\n","Epoch: 2 batch_num: 88\n","train_rmse_target: 0.2078 train_rmse_stderror: 0.02352 train_kl_div: 0.09173\n","val_rmse_target: 0.4772 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4766 (from epoch 2)\n","\n","4 steps took 5.07 seconds\n","Epoch: 2 batch_num: 92\n","train_rmse_target: 0.3311 train_rmse_stderror: 0.03986 train_kl_div: 0.2055\n","val_rmse_target: 0.4695 val_rmse_stderror: 1.749\n","New best_val_rmse: 0.4695\n","\n","2 steps took 2.56 seconds\n","Epoch: 2 batch_num: 94\n","train_rmse_target: 0.1914 train_rmse_stderror: 0.02611 train_kl_div: 0.07615\n","val_rmse_target: 0.4689 val_rmse_stderror: 1.745\n","New best_val_rmse: 0.4689\n","\n","2 steps took 2.54 seconds\n","Epoch: 2 batch_num: 96\n","train_rmse_target: 0.2258 train_rmse_stderror: 0.03091 train_kl_div: 0.1091\n","val_rmse_target: 0.4735 val_rmse_stderror: 1.753\n","Still best_val_rmse: 0.4689 (from epoch 2)\n","\n","4 steps took 5.08 seconds\n","Epoch: 2 batch_num: 100\n","train_rmse_target: 0.2893 train_rmse_stderror: 0.03932 train_kl_div: 0.1873\n","val_rmse_target: 0.4742 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4689 (from epoch 2)\n","\n","4 steps took 5.07 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.2296 train_rmse_stderror: 0.0308 train_kl_div: 0.1165\n","val_rmse_target: 0.469 val_rmse_stderror: 1.73\n","Still best_val_rmse: 0.4689 (from epoch 2)\n","\n","2 steps took 2.54 seconds\n","Epoch: 2 batch_num: 106\n","train_rmse_target: 0.2577 train_rmse_stderror: 0.01831 train_kl_div: 0.1496\n","val_rmse_target: 0.4677 val_rmse_stderror: 1.736\n","New best_val_rmse: 0.4677\n","\n","2 steps took 2.55 seconds\n","Epoch: 2 batch_num: 108\n","train_rmse_target: 0.2599 train_rmse_stderror: 0.02012 train_kl_div: 0.1408\n","val_rmse_target: 0.4694 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","2 steps took 2.53 seconds\n","Epoch: 2 batch_num: 110\n","train_rmse_target: 0.3986 train_rmse_stderror: 0.04394 train_kl_div: 0.2703\n","val_rmse_target: 0.4757 val_rmse_stderror: 1.756\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","4 steps took 5.08 seconds\n","Epoch: 2 batch_num: 114\n","train_rmse_target: 0.4023 train_rmse_stderror: 0.03425 train_kl_div: 0.361\n","val_rmse_target: 0.4878 val_rmse_stderror: 1.751\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 122\n","train_rmse_target: 0.3424 train_rmse_stderror: 0.03159 train_kl_div: 0.2534\n","val_rmse_target: 0.4781 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","4 steps took 5.07 seconds\n","Epoch: 2 batch_num: 126\n","train_rmse_target: 0.3457 train_rmse_stderror: 0.03775 train_kl_div: 0.2783\n","val_rmse_target: 0.5105 val_rmse_stderror: 1.75\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 158\n","train_rmse_target: 0.2185 train_rmse_stderror: 0.03812 train_kl_div: 0.1007\n","val_rmse_target: 0.4953 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 2 batch_num: 174\n","train_rmse_target: 0.2571 train_rmse_stderror: 0.02708 train_kl_div: 0.1489\n","val_rmse_target: 0.4701 val_rmse_stderror: 1.752\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","4 steps took 5.07 seconds\n","Epoch: 2 batch_num: 178\n","train_rmse_target: 0.3996 train_rmse_stderror: 0.03602 train_kl_div: 0.2734\n","val_rmse_target: 0.4729 val_rmse_stderror: 1.736\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","4 steps took 5.08 seconds\n","Epoch: 2 batch_num: 182\n","train_rmse_target: 0.3197 train_rmse_stderror: 0.03155 train_kl_div: 0.2255\n","val_rmse_target: 0.4766 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","4 steps took 5.07 seconds\n","Epoch: 2 batch_num: 186\n","train_rmse_target: 0.29 train_rmse_stderror: 0.02191 train_kl_div: 0.163\n","val_rmse_target: 0.4702 val_rmse_stderror: 1.753\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","4 steps took 5.25 seconds\n","Epoch: 3 batch_num: 2\n","train_rmse_target: 0.2556 train_rmse_stderror: 0.02321 train_kl_div: 0.1272\n","val_rmse_target: 0.4867 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 10\n","train_rmse_target: 0.2168 train_rmse_stderror: 0.0377 train_kl_div: 0.0962\n","val_rmse_target: 0.4706 val_rmse_stderror: 1.75\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","4 steps took 5.08 seconds\n","Epoch: 3 batch_num: 14\n","train_rmse_target: 0.2049 train_rmse_stderror: 0.02721 train_kl_div: 0.08339\n","val_rmse_target: 0.4794 val_rmse_stderror: 1.749\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","4 steps took 5.08 seconds\n","Epoch: 3 batch_num: 18\n","train_rmse_target: 0.1318 train_rmse_stderror: 0.04174 train_kl_div: 0.04417\n","val_rmse_target: 0.5031 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4677 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 50\n","train_rmse_target: 0.1784 train_rmse_stderror: 0.02851 train_kl_div: 0.06161\n","val_rmse_target: 0.4636 val_rmse_stderror: 1.756\n","New best_val_rmse: 0.4636\n","\n","2 steps took 2.56 seconds\n","Epoch: 3 batch_num: 52\n","train_rmse_target: 0.204 train_rmse_stderror: 0.03452 train_kl_div: 0.09429\n","val_rmse_target: 0.4693 val_rmse_stderror: 1.755\n","Still best_val_rmse: 0.4636 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 54\n","train_rmse_target: 0.1485 train_rmse_stderror: 0.0353 train_kl_div: 0.04602\n","val_rmse_target: 0.4692 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4636 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 56\n","train_rmse_target: 0.1998 train_rmse_stderror: 0.02642 train_kl_div: 0.0852\n","val_rmse_target: 0.4633 val_rmse_stderror: 1.735\n","New best_val_rmse: 0.4633\n","\n","2 steps took 2.56 seconds\n","Epoch: 3 batch_num: 58\n","train_rmse_target: 0.1687 train_rmse_stderror: 0.04008 train_kl_div: 0.06034\n","val_rmse_target: 0.4661 val_rmse_stderror: 1.736\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 60\n","train_rmse_target: 0.1091 train_rmse_stderror: 0.02499 train_kl_div: 0.02682\n","val_rmse_target: 0.4892 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 68\n","train_rmse_target: 0.2224 train_rmse_stderror: 0.02172 train_kl_div: 0.1059\n","val_rmse_target: 0.4649 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 70\n","train_rmse_target: 0.1756 train_rmse_stderror: 0.01855 train_kl_div: 0.0663\n","val_rmse_target: 0.4662 val_rmse_stderror: 1.737\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 72\n","train_rmse_target: 0.1735 train_rmse_stderror: 0.03601 train_kl_div: 0.06338\n","val_rmse_target: 0.4649 val_rmse_stderror: 1.741\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 74\n","train_rmse_target: 0.332 train_rmse_stderror: 0.03406 train_kl_div: 0.1685\n","val_rmse_target: 0.4639 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1715 train_rmse_stderror: 0.01786 train_kl_div: 0.06066\n","val_rmse_target: 0.4667 val_rmse_stderror: 1.748\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 78\n","train_rmse_target: 0.2279 train_rmse_stderror: 0.02976 train_kl_div: 0.1099\n","val_rmse_target: 0.4767 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.08 seconds\n","Epoch: 3 batch_num: 82\n","train_rmse_target: 0.2117 train_rmse_stderror: 0.02402 train_kl_div: 0.102\n","val_rmse_target: 0.4829 val_rmse_stderror: 1.737\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 90\n","train_rmse_target: 0.1617 train_rmse_stderror: 0.01567 train_kl_div: 0.05553\n","val_rmse_target: 0.4649 val_rmse_stderror: 1.739\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 92\n","train_rmse_target: 0.1219 train_rmse_stderror: 0.03145 train_kl_div: 0.03743\n","val_rmse_target: 0.4697 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 94\n","train_rmse_target: 0.157 train_rmse_stderror: 0.02121 train_kl_div: 0.0555\n","val_rmse_target: 0.4747 val_rmse_stderror: 1.741\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.07 seconds\n","Epoch: 3 batch_num: 98\n","train_rmse_target: 0.129 train_rmse_stderror: 0.01974 train_kl_div: 0.04043\n","val_rmse_target: 0.4684 val_rmse_stderror: 1.738\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 100\n","train_rmse_target: 0.135 train_rmse_stderror: 0.02559 train_kl_div: 0.04045\n","val_rmse_target: 0.4653 val_rmse_stderror: 1.739\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 102\n","train_rmse_target: 0.1491 train_rmse_stderror: 0.02114 train_kl_div: 0.04791\n","val_rmse_target: 0.4655 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 104\n","train_rmse_target: 0.166 train_rmse_stderror: 0.03341 train_kl_div: 0.04973\n","val_rmse_target: 0.4657 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 106\n","train_rmse_target: 0.1333 train_rmse_stderror: 0.03031 train_kl_div: 0.04135\n","val_rmse_target: 0.4659 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.2052 train_rmse_stderror: 0.03431 train_kl_div: 0.08997\n","val_rmse_target: 0.4664 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 110\n","train_rmse_target: 0.2077 train_rmse_stderror: 0.02877 train_kl_div: 0.08709\n","val_rmse_target: 0.4677 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 112\n","train_rmse_target: 0.1938 train_rmse_stderror: 0.0394 train_kl_div: 0.0795\n","val_rmse_target: 0.4687 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 114\n","train_rmse_target: 0.2172 train_rmse_stderror: 0.02539 train_kl_div: 0.09012\n","val_rmse_target: 0.4683 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 116\n","train_rmse_target: 0.1297 train_rmse_stderror: 0.02465 train_kl_div: 0.0355\n","val_rmse_target: 0.467 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 118\n","train_rmse_target: 0.2262 train_rmse_stderror: 0.03427 train_kl_div: 0.1026\n","val_rmse_target: 0.4663 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 120\n","train_rmse_target: 0.2047 train_rmse_stderror: 0.03126 train_kl_div: 0.08009\n","val_rmse_target: 0.4679 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 122\n","train_rmse_target: 0.2137 train_rmse_stderror: 0.01837 train_kl_div: 0.08931\n","val_rmse_target: 0.4693 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 124\n","train_rmse_target: 0.109 train_rmse_stderror: 0.0291 train_kl_div: 0.0285\n","val_rmse_target: 0.4672 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 126\n","train_rmse_target: 0.1176 train_rmse_stderror: 0.02445 train_kl_div: 0.03075\n","val_rmse_target: 0.4684 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 128\n","train_rmse_target: 0.1906 train_rmse_stderror: 0.01778 train_kl_div: 0.07288\n","val_rmse_target: 0.4691 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 130\n","train_rmse_target: 0.2679 train_rmse_stderror: 0.03642 train_kl_div: 0.1161\n","val_rmse_target: 0.4697 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 132\n","train_rmse_target: 0.3132 train_rmse_stderror: 0.025 train_kl_div: 0.1999\n","val_rmse_target: 0.4677 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 134\n","train_rmse_target: 0.1785 train_rmse_stderror: 0.02242 train_kl_div: 0.07383\n","val_rmse_target: 0.4664 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 136\n","train_rmse_target: 0.163 train_rmse_stderror: 0.02834 train_kl_div: 0.06096\n","val_rmse_target: 0.4645 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 138\n","train_rmse_target: 0.3295 train_rmse_stderror: 0.02645 train_kl_div: 0.2018\n","val_rmse_target: 0.4651 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1586 train_rmse_stderror: 0.02081 train_kl_div: 0.05549\n","val_rmse_target: 0.4686 val_rmse_stderror: 1.739\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 142\n","train_rmse_target: 0.1966 train_rmse_stderror: 0.02066 train_kl_div: 0.08616\n","val_rmse_target: 0.4714 val_rmse_stderror: 1.735\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.08 seconds\n","Epoch: 3 batch_num: 146\n","train_rmse_target: 0.1171 train_rmse_stderror: 0.03087 train_kl_div: 0.03223\n","val_rmse_target: 0.4732 val_rmse_stderror: 1.735\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.06 seconds\n","Epoch: 3 batch_num: 150\n","train_rmse_target: 0.1912 train_rmse_stderror: 0.01641 train_kl_div: 0.07862\n","val_rmse_target: 0.4696 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 152\n","train_rmse_target: 0.1628 train_rmse_stderror: 0.01521 train_kl_div: 0.05497\n","val_rmse_target: 0.4673 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 154\n","train_rmse_target: 0.2294 train_rmse_stderror: 0.01887 train_kl_div: 0.1123\n","val_rmse_target: 0.467 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 156\n","train_rmse_target: 0.1511 train_rmse_stderror: 0.01494 train_kl_div: 0.05003\n","val_rmse_target: 0.4675 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 158\n","train_rmse_target: 0.1464 train_rmse_stderror: 0.02997 train_kl_div: 0.04958\n","val_rmse_target: 0.4671 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 160\n","train_rmse_target: 0.2613 train_rmse_stderror: 0.03344 train_kl_div: 0.127\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 162\n","train_rmse_target: 0.1318 train_rmse_stderror: 0.01893 train_kl_div: 0.0347\n","val_rmse_target: 0.4701 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.07 seconds\n","Epoch: 3 batch_num: 166\n","train_rmse_target: 0.0741 train_rmse_stderror: 0.03245 train_kl_div: 0.01642\n","val_rmse_target: 0.4656 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 168\n","train_rmse_target: 0.1957 train_rmse_stderror: 0.02481 train_kl_div: 0.08249\n","val_rmse_target: 0.4654 val_rmse_stderror: 1.749\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 170\n","train_rmse_target: 0.121 train_rmse_stderror: 0.01681 train_kl_div: 0.03142\n","val_rmse_target: 0.4656 val_rmse_stderror: 1.748\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.1253 train_rmse_stderror: 0.01747 train_kl_div: 0.03028\n","val_rmse_target: 0.4654 val_rmse_stderror: 1.748\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 174\n","train_rmse_target: 0.1774 train_rmse_stderror: 0.02667 train_kl_div: 0.05975\n","val_rmse_target: 0.4656 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 176\n","train_rmse_target: 0.1732 train_rmse_stderror: 0.02134 train_kl_div: 0.05708\n","val_rmse_target: 0.466 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 178\n","train_rmse_target: 0.1328 train_rmse_stderror: 0.01847 train_kl_div: 0.03803\n","val_rmse_target: 0.4656 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 180\n","train_rmse_target: 0.183 train_rmse_stderror: 0.01746 train_kl_div: 0.07232\n","val_rmse_target: 0.4659 val_rmse_stderror: 1.738\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 3 batch_num: 182\n","train_rmse_target: 0.1454 train_rmse_stderror: 0.02318 train_kl_div: 0.04812\n","val_rmse_target: 0.467 val_rmse_stderror: 1.737\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 184\n","train_rmse_target: 0.2914 train_rmse_stderror: 0.05825 train_kl_div: 0.1254\n","val_rmse_target: 0.4679 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 3 batch_num: 186\n","train_rmse_target: 0.181 train_rmse_stderror: 0.02953 train_kl_div: 0.07038\n","val_rmse_target: 0.4689 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.7 seconds\n","Epoch: 4 batch_num: 0\n","train_rmse_target: 0.2071 train_rmse_stderror: 0.0212 train_kl_div: 0.0858\n","val_rmse_target: 0.4685 val_rmse_stderror: 1.748\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 2\n","train_rmse_target: 0.1239 train_rmse_stderror: 0.02962 train_kl_div: 0.03484\n","val_rmse_target: 0.4683 val_rmse_stderror: 1.748\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 4\n","train_rmse_target: 0.1355 train_rmse_stderror: 0.02752 train_kl_div: 0.03976\n","val_rmse_target: 0.4677 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 6\n","train_rmse_target: 0.1253 train_rmse_stderror: 0.02579 train_kl_div: 0.0348\n","val_rmse_target: 0.4668 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 8\n","train_rmse_target: 0.1029 train_rmse_stderror: 0.02611 train_kl_div: 0.02476\n","val_rmse_target: 0.4661 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 10\n","train_rmse_target: 0.2332 train_rmse_stderror: 0.0385 train_kl_div: 0.08563\n","val_rmse_target: 0.4656 val_rmse_stderror: 1.739\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 12\n","train_rmse_target: 0.1454 train_rmse_stderror: 0.02063 train_kl_div: 0.04284\n","val_rmse_target: 0.465 val_rmse_stderror: 1.739\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 14\n","train_rmse_target: 0.102 train_rmse_stderror: 0.0162 train_kl_div: 0.02278\n","val_rmse_target: 0.465 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.2343 train_rmse_stderror: 0.0331 train_kl_div: 0.1293\n","val_rmse_target: 0.4653 val_rmse_stderror: 1.741\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 18\n","train_rmse_target: 0.2115 train_rmse_stderror: 0.03453 train_kl_div: 0.08598\n","val_rmse_target: 0.4663 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 20\n","train_rmse_target: 0.1439 train_rmse_stderror: 0.01772 train_kl_div: 0.04454\n","val_rmse_target: 0.4684 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 22\n","train_rmse_target: 0.1348 train_rmse_stderror: 0.02955 train_kl_div: 0.03841\n","val_rmse_target: 0.4718 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.07 seconds\n","Epoch: 4 batch_num: 26\n","train_rmse_target: 0.1067 train_rmse_stderror: 0.01456 train_kl_div: 0.0263\n","val_rmse_target: 0.4755 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.08 seconds\n","Epoch: 4 batch_num: 30\n","train_rmse_target: 0.1681 train_rmse_stderror: 0.0177 train_kl_div: 0.04799\n","val_rmse_target: 0.4724 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.07 seconds\n","Epoch: 4 batch_num: 34\n","train_rmse_target: 0.1542 train_rmse_stderror: 0.02857 train_kl_div: 0.05139\n","val_rmse_target: 0.4675 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 36\n","train_rmse_target: 0.1474 train_rmse_stderror: 0.03108 train_kl_div: 0.04602\n","val_rmse_target: 0.4665 val_rmse_stderror: 1.745\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 38\n","train_rmse_target: 0.1219 train_rmse_stderror: 0.02652 train_kl_div: 0.0345\n","val_rmse_target: 0.4656 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 40\n","train_rmse_target: 0.09352 train_rmse_stderror: 0.02453 train_kl_div: 0.02019\n","val_rmse_target: 0.465 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 42\n","train_rmse_target: 0.1272 train_rmse_stderror: 0.02239 train_kl_div: 0.03343\n","val_rmse_target: 0.4648 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 44\n","train_rmse_target: 0.09943 train_rmse_stderror: 0.02168 train_kl_div: 0.02192\n","val_rmse_target: 0.465 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 46\n","train_rmse_target: 0.1881 train_rmse_stderror: 0.02983 train_kl_div: 0.05941\n","val_rmse_target: 0.4659 val_rmse_stderror: 1.739\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1471 train_rmse_stderror: 0.02962 train_kl_div: 0.04633\n","val_rmse_target: 0.4673 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 50\n","train_rmse_target: 0.097 train_rmse_stderror: 0.01536 train_kl_div: 0.02195\n","val_rmse_target: 0.4687 val_rmse_stderror: 1.741\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 52\n","train_rmse_target: 0.1034 train_rmse_stderror: 0.0257 train_kl_div: 0.02459\n","val_rmse_target: 0.4698 val_rmse_stderror: 1.741\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 54\n","train_rmse_target: 0.1261 train_rmse_stderror: 0.02101 train_kl_div: 0.03646\n","val_rmse_target: 0.4705 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.08 seconds\n","Epoch: 4 batch_num: 58\n","train_rmse_target: 0.1049 train_rmse_stderror: 0.02991 train_kl_div: 0.02514\n","val_rmse_target: 0.4712 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","4 steps took 5.08 seconds\n","Epoch: 4 batch_num: 62\n","train_rmse_target: 0.1112 train_rmse_stderror: 0.02365 train_kl_div: 0.02804\n","val_rmse_target: 0.4694 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 64\n","train_rmse_target: 0.1237 train_rmse_stderror: 0.0224 train_kl_div: 0.03439\n","val_rmse_target: 0.4681 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 66\n","train_rmse_target: 0.05234 train_rmse_stderror: 0.01469 train_kl_div: 0.006627\n","val_rmse_target: 0.4673 val_rmse_stderror: 1.748\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 68\n","train_rmse_target: 0.08852 train_rmse_stderror: 0.03113 train_kl_div: 0.01976\n","val_rmse_target: 0.4668 val_rmse_stderror: 1.748\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 70\n","train_rmse_target: 0.07086 train_rmse_stderror: 0.02146 train_kl_div: 0.01201\n","val_rmse_target: 0.4664 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 72\n","train_rmse_target: 0.1312 train_rmse_stderror: 0.02651 train_kl_div: 0.04085\n","val_rmse_target: 0.466 val_rmse_stderror: 1.746\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 74\n","train_rmse_target: 0.1299 train_rmse_stderror: 0.02412 train_kl_div: 0.0379\n","val_rmse_target: 0.4656 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 76\n","train_rmse_target: 0.1607 train_rmse_stderror: 0.02647 train_kl_div: 0.04821\n","val_rmse_target: 0.4654 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 78\n","train_rmse_target: 0.1352 train_rmse_stderror: 0.02221 train_kl_div: 0.04065\n","val_rmse_target: 0.4651 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.1371 train_rmse_stderror: 0.02191 train_kl_div: 0.04067\n","val_rmse_target: 0.4649 val_rmse_stderror: 1.741\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 82\n","train_rmse_target: 0.1261 train_rmse_stderror: 0.02022 train_kl_div: 0.03319\n","val_rmse_target: 0.4651 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 84\n","train_rmse_target: 0.1445 train_rmse_stderror: 0.02495 train_kl_div: 0.04567\n","val_rmse_target: 0.4656 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 86\n","train_rmse_target: 0.1038 train_rmse_stderror: 0.02207 train_kl_div: 0.02426\n","val_rmse_target: 0.466 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 88\n","train_rmse_target: 0.1086 train_rmse_stderror: 0.02123 train_kl_div: 0.02782\n","val_rmse_target: 0.4661 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 90\n","train_rmse_target: 0.1154 train_rmse_stderror: 0.02157 train_kl_div: 0.02956\n","val_rmse_target: 0.4663 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 92\n","train_rmse_target: 0.09163 train_rmse_stderror: 0.01908 train_kl_div: 0.02073\n","val_rmse_target: 0.4666 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 94\n","train_rmse_target: 0.1114 train_rmse_stderror: 0.01854 train_kl_div: 0.02818\n","val_rmse_target: 0.4668 val_rmse_stderror: 1.74\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 96\n","train_rmse_target: 0.08372 train_rmse_stderror: 0.01876 train_kl_div: 0.01582\n","val_rmse_target: 0.4669 val_rmse_stderror: 1.741\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 98\n","train_rmse_target: 0.168 train_rmse_stderror: 0.03121 train_kl_div: 0.04553\n","val_rmse_target: 0.4671 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 100\n","train_rmse_target: 0.1194 train_rmse_stderror: 0.02081 train_kl_div: 0.03006\n","val_rmse_target: 0.467 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 102\n","train_rmse_target: 0.1641 train_rmse_stderror: 0.01722 train_kl_div: 0.0637\n","val_rmse_target: 0.4668 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 104\n","train_rmse_target: 0.09521 train_rmse_stderror: 0.02173 train_kl_div: 0.02213\n","val_rmse_target: 0.4666 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 106\n","train_rmse_target: 0.1759 train_rmse_stderror: 0.01678 train_kl_div: 0.06568\n","val_rmse_target: 0.4666 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 108\n","train_rmse_target: 0.128 train_rmse_stderror: 0.03604 train_kl_div: 0.03573\n","val_rmse_target: 0.4666 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 110\n","train_rmse_target: 0.1098 train_rmse_stderror: 0.01811 train_kl_div: 0.02807\n","val_rmse_target: 0.4667 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.1355 train_rmse_stderror: 0.02882 train_kl_div: 0.04014\n","val_rmse_target: 0.4667 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 114\n","train_rmse_target: 0.221 train_rmse_stderror: 0.0368 train_kl_div: 0.08726\n","val_rmse_target: 0.4667 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 116\n","train_rmse_target: 0.07811 train_rmse_stderror: 0.03095 train_kl_div: 0.01711\n","val_rmse_target: 0.4669 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 118\n","train_rmse_target: 0.1172 train_rmse_stderror: 0.02029 train_kl_div: 0.03207\n","val_rmse_target: 0.4669 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 120\n","train_rmse_target: 0.1303 train_rmse_stderror: 0.02323 train_kl_div: 0.0357\n","val_rmse_target: 0.4669 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 122\n","train_rmse_target: 0.09378 train_rmse_stderror: 0.02164 train_kl_div: 0.02069\n","val_rmse_target: 0.467 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 124\n","train_rmse_target: 0.1045 train_rmse_stderror: 0.03424 train_kl_div: 0.02367\n","val_rmse_target: 0.467 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 126\n","train_rmse_target: 0.1246 train_rmse_stderror: 0.02129 train_kl_div: 0.03171\n","val_rmse_target: 0.467 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 128\n","train_rmse_target: 0.1257 train_rmse_stderror: 0.02311 train_kl_div: 0.03519\n","val_rmse_target: 0.4671 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 130\n","train_rmse_target: 0.135 train_rmse_stderror: 0.02174 train_kl_div: 0.0369\n","val_rmse_target: 0.4671 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 132\n","train_rmse_target: 0.1079 train_rmse_stderror: 0.02885 train_kl_div: 0.02677\n","val_rmse_target: 0.4672 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 134\n","train_rmse_target: 0.09665 train_rmse_stderror: 0.02376 train_kl_div: 0.02274\n","val_rmse_target: 0.4672 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 136\n","train_rmse_target: 0.07484 train_rmse_stderror: 0.02423 train_kl_div: 0.01523\n","val_rmse_target: 0.4672 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 138\n","train_rmse_target: 0.1458 train_rmse_stderror: 0.02097 train_kl_div: 0.0428\n","val_rmse_target: 0.4671 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 140\n","train_rmse_target: 0.09822 train_rmse_stderror: 0.0281 train_kl_div: 0.02309\n","val_rmse_target: 0.4672 val_rmse_stderror: 1.744\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 142\n","train_rmse_target: 0.1032 train_rmse_stderror: 0.02816 train_kl_div: 0.02502\n","val_rmse_target: 0.4672 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.1024 train_rmse_stderror: 0.02342 train_kl_div: 0.02538\n","val_rmse_target: 0.4672 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 146\n","train_rmse_target: 0.1322 train_rmse_stderror: 0.01784 train_kl_div: 0.04004\n","val_rmse_target: 0.4672 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 148\n","train_rmse_target: 0.1085 train_rmse_stderror: 0.02659 train_kl_div: 0.02617\n","val_rmse_target: 0.4672 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 150\n","train_rmse_target: 0.09279 train_rmse_stderror: 0.02145 train_kl_div: 0.02127\n","val_rmse_target: 0.4673 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 152\n","train_rmse_target: 0.1109 train_rmse_stderror: 0.02675 train_kl_div: 0.02518\n","val_rmse_target: 0.4674 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 154\n","train_rmse_target: 0.1143 train_rmse_stderror: 0.01689 train_kl_div: 0.02892\n","val_rmse_target: 0.4674 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 156\n","train_rmse_target: 0.1652 train_rmse_stderror: 0.02479 train_kl_div: 0.05601\n","val_rmse_target: 0.4675 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 158\n","train_rmse_target: 0.242 train_rmse_stderror: 0.04016 train_kl_div: 0.1071\n","val_rmse_target: 0.4675 val_rmse_stderror: 1.742\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 160\n","train_rmse_target: 0.1095 train_rmse_stderror: 0.01822 train_kl_div: 0.02506\n","val_rmse_target: 0.4675 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 162\n","train_rmse_target: 0.08368 train_rmse_stderror: 0.01239 train_kl_div: 0.01495\n","val_rmse_target: 0.4675 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 164\n","train_rmse_target: 0.2218 train_rmse_stderror: 0.04222 train_kl_div: 0.08039\n","val_rmse_target: 0.4675 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 166\n","train_rmse_target: 0.1258 train_rmse_stderror: 0.02523 train_kl_div: 0.03501\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 168\n","train_rmse_target: 0.1479 train_rmse_stderror: 0.02077 train_kl_div: 0.04281\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 170\n","train_rmse_target: 0.1685 train_rmse_stderror: 0.02089 train_kl_div: 0.05774\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 172\n","train_rmse_target: 0.2151 train_rmse_stderror: 0.02511 train_kl_div: 0.09329\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 174\n","train_rmse_target: 0.08475 train_rmse_stderror: 0.01969 train_kl_div: 0.01657\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1689 train_rmse_stderror: 0.03189 train_kl_div: 0.04601\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 178\n","train_rmse_target: 0.08825 train_rmse_stderror: 0.03425 train_kl_div: 0.01799\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 180\n","train_rmse_target: 0.1121 train_rmse_stderror: 0.02921 train_kl_div: 0.02859\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.54 seconds\n","Epoch: 4 batch_num: 182\n","train_rmse_target: 0.1269 train_rmse_stderror: 0.02445 train_kl_div: 0.02913\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 184\n","train_rmse_target: 0.07371 train_rmse_stderror: 0.02399 train_kl_div: 0.01334\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","2 steps took 2.53 seconds\n","Epoch: 4 batch_num: 186\n","train_rmse_target: 0.1503 train_rmse_stderror: 0.02103 train_kl_div: 0.04576\n","val_rmse_target: 0.4676 val_rmse_stderror: 1.743\n","Still best_val_rmse: 0.4633 (from epoch 3)\n","\n","Performance estimates:\n","[0.48686168177282346, 0.49318919309887377, 0.4632869161410703]\n","Mean: 0.4811125970042558\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 4/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 82.3 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 1.0 train_rmse_stderror: 0.06107 train_kl_div: 1.727\n","val_rmse_target: 0.6746 val_rmse_stderror: 1.831\n","New best_val_rmse: 0.6746\n","\n","64 steps took 81.4 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.457 train_rmse_stderror: 0.04387 train_kl_div: 0.468\n","val_rmse_target: 0.5378 val_rmse_stderror: 1.846\n","New best_val_rmse: 0.5378\n","\n","32 steps took 40.7 seconds\n","Epoch: 0 batch_num: 160\n","train_rmse_target: 0.6224 train_rmse_stderror: 0.02235 train_kl_div: 0.7292\n","val_rmse_target: 0.5217 val_rmse_stderror: 1.857\n","New best_val_rmse: 0.5217\n","\n","32 steps took 40.9 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.6718 train_rmse_stderror: 0.03537 train_kl_div: 0.9784\n","val_rmse_target: 0.6078 val_rmse_stderror: 1.8\n","Still best_val_rmse: 0.5217 (from epoch 0)\n","\n","64 steps took 81.4 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.608 train_rmse_stderror: 0.03671 train_kl_div: 0.7977\n","val_rmse_target: 0.5496 val_rmse_stderror: 1.824\n","Still best_val_rmse: 0.5217 (from epoch 0)\n","\n","32 steps took 40.7 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.3373 train_rmse_stderror: 0.03218 train_kl_div: 0.2444\n","val_rmse_target: 0.5473 val_rmse_stderror: 1.831\n","Still best_val_rmse: 0.5217 (from epoch 0)\n","\n","32 steps took 40.7 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.4121 train_rmse_stderror: 0.01422 train_kl_div: 0.3467\n","val_rmse_target: 0.5089 val_rmse_stderror: 1.835\n","New best_val_rmse: 0.5089\n","\n","32 steps took 40.7 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.3869 train_rmse_stderror: 0.04832 train_kl_div: 0.2388\n","val_rmse_target: 0.5276 val_rmse_stderror: 1.832\n","Still best_val_rmse: 0.5089 (from epoch 1)\n","\n","32 steps took 40.9 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.2832 train_rmse_stderror: 0.03172 train_kl_div: 0.1719\n","val_rmse_target: 0.5054 val_rmse_stderror: 1.839\n","New best_val_rmse: 0.5054\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2396 train_rmse_stderror: 0.02448 train_kl_div: 0.1198\n","val_rmse_target: 0.5017 val_rmse_stderror: 1.838\n","New best_val_rmse: 0.5017\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2291 train_rmse_stderror: 0.0125 train_kl_div: 0.1179\n","val_rmse_target: 0.483 val_rmse_stderror: 1.828\n","New best_val_rmse: 0.483\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 80\n","train_rmse_target: 0.2276 train_rmse_stderror: 0.02402 train_kl_div: 0.1141\n","val_rmse_target: 0.4865 val_rmse_stderror: 1.857\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","8 steps took 10.1 seconds\n","Epoch: 2 batch_num: 88\n","train_rmse_target: 0.3199 train_rmse_stderror: 0.04538 train_kl_div: 0.1796\n","val_rmse_target: 0.555 val_rmse_stderror: 1.834\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","64 steps took 81.3 seconds\n","Epoch: 2 batch_num: 152\n","train_rmse_target: 0.2895 train_rmse_stderror: 0.02189 train_kl_div: 0.1713\n","val_rmse_target: 0.4851 val_rmse_stderror: 1.839\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 160\n","train_rmse_target: 0.2015 train_rmse_stderror: 0.01772 train_kl_div: 0.087\n","val_rmse_target: 0.4844 val_rmse_stderror: 1.846\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.3087 train_rmse_stderror: 0.02879 train_kl_div: 0.2104\n","val_rmse_target: 0.5139 val_rmse_stderror: 1.83\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","32 steps took 40.9 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.3051 train_rmse_stderror: 0.03018 train_kl_div: 0.1712\n","val_rmse_target: 0.4925 val_rmse_stderror: 1.852\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 28\n","train_rmse_target: 0.1765 train_rmse_stderror: 0.01611 train_kl_div: 0.0658\n","val_rmse_target: 0.5217 val_rmse_stderror: 1.832\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 60\n","train_rmse_target: 0.1845 train_rmse_stderror: 0.02605 train_kl_div: 0.07609\n","val_rmse_target: 0.4855 val_rmse_stderror: 1.837\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 68\n","train_rmse_target: 0.1765 train_rmse_stderror: 0.02364 train_kl_div: 0.06408\n","val_rmse_target: 0.4853 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.2258 train_rmse_stderror: 0.02246 train_kl_div: 0.1113\n","val_rmse_target: 0.4886 val_rmse_stderror: 1.843\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 84\n","train_rmse_target: 0.2075 train_rmse_stderror: 0.02522 train_kl_div: 0.09365\n","val_rmse_target: 0.4904 val_rmse_stderror: 1.833\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 100\n","train_rmse_target: 0.2144 train_rmse_stderror: 0.02793 train_kl_div: 0.08787\n","val_rmse_target: 0.508 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 132\n","train_rmse_target: 0.2163 train_rmse_stderror: 0.03733 train_kl_div: 0.07298\n","val_rmse_target: 0.5162 val_rmse_stderror: 1.835\n","Still best_val_rmse: 0.483 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 164\n","train_rmse_target: 0.1469 train_rmse_stderror: 0.03197 train_kl_div: 0.05047\n","val_rmse_target: 0.4803 val_rmse_stderror: 1.837\n","New best_val_rmse: 0.4803\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.2415 train_rmse_stderror: 0.02823 train_kl_div: 0.1319\n","val_rmse_target: 0.4966 val_rmse_stderror: 1.846\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","16 steps took 20.6 seconds\n","Epoch: 4 batch_num: 0\n","train_rmse_target: 0.1178 train_rmse_stderror: 0.02138 train_kl_div: 0.03013\n","val_rmse_target: 0.4845 val_rmse_stderror: 1.845\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 8\n","train_rmse_target: 0.08854 train_rmse_stderror: 0.01544 train_kl_div: 0.01773\n","val_rmse_target: 0.4874 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1389 train_rmse_stderror: 0.02217 train_kl_div: 0.04133\n","val_rmse_target: 0.4867 val_rmse_stderror: 1.837\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 24\n","train_rmse_target: 0.1147 train_rmse_stderror: 0.02465 train_kl_div: 0.02798\n","val_rmse_target: 0.4828 val_rmse_stderror: 1.833\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 32\n","train_rmse_target: 0.07757 train_rmse_stderror: 0.01802 train_kl_div: 0.01252\n","val_rmse_target: 0.49 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1108 train_rmse_stderror: 0.02267 train_kl_div: 0.02777\n","val_rmse_target: 0.4849 val_rmse_stderror: 1.835\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 56\n","train_rmse_target: 0.1132 train_rmse_stderror: 0.02441 train_kl_div: 0.02637\n","val_rmse_target: 0.484 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 64\n","train_rmse_target: 0.1337 train_rmse_stderror: 0.0253 train_kl_div: 0.0351\n","val_rmse_target: 0.4899 val_rmse_stderror: 1.841\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 72\n","train_rmse_target: 0.1749 train_rmse_stderror: 0.02794 train_kl_div: 0.06387\n","val_rmse_target: 0.4948 val_rmse_stderror: 1.839\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 88\n","train_rmse_target: 0.1209 train_rmse_stderror: 0.01836 train_kl_div: 0.03065\n","val_rmse_target: 0.4843 val_rmse_stderror: 1.839\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 96\n","train_rmse_target: 0.09902 train_rmse_stderror: 0.01071 train_kl_div: 0.02225\n","val_rmse_target: 0.4849 val_rmse_stderror: 1.839\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 104\n","train_rmse_target: 0.09304 train_rmse_stderror: 0.01913 train_kl_div: 0.01954\n","val_rmse_target: 0.4859 val_rmse_stderror: 1.839\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.08391 train_rmse_stderror: 0.02037 train_kl_div: 0.01554\n","val_rmse_target: 0.4871 val_rmse_stderror: 1.839\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 120\n","train_rmse_target: 0.05437 train_rmse_stderror: 0.02478 train_kl_div: 0.007725\n","val_rmse_target: 0.4864 val_rmse_stderror: 1.839\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 128\n","train_rmse_target: 0.1282 train_rmse_stderror: 0.02998 train_kl_div: 0.03158\n","val_rmse_target: 0.4866 val_rmse_stderror: 1.839\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 136\n","train_rmse_target: 0.1087 train_rmse_stderror: 0.01138 train_kl_div: 0.02462\n","val_rmse_target: 0.4869 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.1179 train_rmse_stderror: 0.02261 train_kl_div: 0.03295\n","val_rmse_target: 0.4875 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 152\n","train_rmse_target: 0.1319 train_rmse_stderror: 0.01571 train_kl_div: 0.03244\n","val_rmse_target: 0.4876 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 160\n","train_rmse_target: 0.07521 train_rmse_stderror: 0.01585 train_kl_div: 0.01268\n","val_rmse_target: 0.4877 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 168\n","train_rmse_target: 0.09555 train_rmse_stderror: 0.02319 train_kl_div: 0.01973\n","val_rmse_target: 0.4877 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1202 train_rmse_stderror: 0.02602 train_kl_div: 0.03256\n","val_rmse_target: 0.4876 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 184\n","train_rmse_target: 0.1266 train_rmse_stderror: 0.0179 train_kl_div: 0.0378\n","val_rmse_target: 0.4876 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.4803 (from epoch 3)\n","\n","Performance estimates:\n","[0.48686168177282346, 0.49318919309887377, 0.4632869161410703, 0.48028027209396107]\n","Mean: 0.48090451577668214\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 5/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-roberta-large/pre-trained-roberta/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 82.3 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.8114 train_rmse_stderror: 0.01419 train_kl_div: 1.407\n","val_rmse_target: 1.073 val_rmse_stderror: 1.748\n","New best_val_rmse: 1.073\n","\n","64 steps took 81.3 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.6349 train_rmse_stderror: 0.03372 train_kl_div: 0.8311\n","val_rmse_target: 0.5745 val_rmse_stderror: 1.776\n","New best_val_rmse: 0.5745\n","\n","64 steps took 81.6 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.3219 train_rmse_stderror: 0.03209 train_kl_div: 0.2316\n","val_rmse_target: 0.5514 val_rmse_stderror: 1.743\n","New best_val_rmse: 0.5514\n","\n","64 steps took 81.4 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.4193 train_rmse_stderror: 0.03255 train_kl_div: 0.3678\n","val_rmse_target: 0.5308 val_rmse_stderror: 1.757\n","New best_val_rmse: 0.5308\n","\n","32 steps took 40.7 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.3364 train_rmse_stderror: 0.0236 train_kl_div: 0.2465\n","val_rmse_target: 0.5555 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.5308 (from epoch 1)\n","\n","64 steps took 81.4 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.3863 train_rmse_stderror: 0.02083 train_kl_div: 0.3193\n","val_rmse_target: 0.5438 val_rmse_stderror: 1.778\n","Still best_val_rmse: 0.5308 (from epoch 1)\n","\n","32 steps took 40.9 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.2074 train_rmse_stderror: 0.02276 train_kl_div: 0.0928\n","val_rmse_target: 0.5035 val_rmse_stderror: 1.774\n","New best_val_rmse: 0.5035\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2096 train_rmse_stderror: 0.04914 train_kl_div: 0.1059\n","val_rmse_target: 0.5208 val_rmse_stderror: 1.784\n","Still best_val_rmse: 0.5035 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.3458 train_rmse_stderror: 0.03953 train_kl_div: 0.2066\n","val_rmse_target: 0.5069 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.5035 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.3467 train_rmse_stderror: 0.03951 train_kl_div: 0.28\n","val_rmse_target: 0.5265 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.5035 (from epoch 2)\n","\n","32 steps took 40.6 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.1945 train_rmse_stderror: 0.0211 train_kl_div: 0.08169\n","val_rmse_target: 0.519 val_rmse_stderror: 1.768\n","Still best_val_rmse: 0.5035 (from epoch 2)\n","\n","32 steps took 40.6 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.2719 train_rmse_stderror: 0.02644 train_kl_div: 0.1566\n","val_rmse_target: 0.4832 val_rmse_stderror: 1.766\n","New best_val_rmse: 0.4832\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 176\n","train_rmse_target: 0.2676 train_rmse_stderror: 0.03268 train_kl_div: 0.1592\n","val_rmse_target: 0.4821 val_rmse_stderror: 1.777\n","New best_val_rmse: 0.4821\n","\n","8 steps took 10.2 seconds\n","Epoch: 2 batch_num: 184\n","train_rmse_target: 0.4084 train_rmse_stderror: 0.02742 train_kl_div: 0.3433\n","val_rmse_target: 0.497 val_rmse_stderror: 1.764\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","16 steps took 20.6 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.2888 train_rmse_stderror: 0.02468 train_kl_div: 0.1431\n","val_rmse_target: 0.5029 val_rmse_stderror: 1.776\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.2347 train_rmse_stderror: 0.02895 train_kl_div: 0.1131\n","val_rmse_target: 0.5075 val_rmse_stderror: 1.766\n","Still best_val_rmse: 0.4821 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1919 train_rmse_stderror: 0.01585 train_kl_div: 0.07648\n","val_rmse_target: 0.4773 val_rmse_stderror: 1.776\n","New best_val_rmse: 0.4773\n","\n","4 steps took 5.1 seconds\n","Epoch: 3 batch_num: 80\n","train_rmse_target: 0.2453 train_rmse_stderror: 0.03333 train_kl_div: 0.1322\n","val_rmse_target: 0.5125 val_rmse_stderror: 1.765\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 112\n","train_rmse_target: 0.1955 train_rmse_stderror: 0.01885 train_kl_div: 0.08598\n","val_rmse_target: 0.486 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 120\n","train_rmse_target: 0.3083 train_rmse_stderror: 0.03529 train_kl_div: 0.1798\n","val_rmse_target: 0.4994 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 3 batch_num: 136\n","train_rmse_target: 0.1425 train_rmse_stderror: 0.01812 train_kl_div: 0.04129\n","val_rmse_target: 0.482 val_rmse_stderror: 1.763\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 144\n","train_rmse_target: 0.1856 train_rmse_stderror: 0.02133 train_kl_div: 0.07199\n","val_rmse_target: 0.4831 val_rmse_stderror: 1.773\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 3 batch_num: 152\n","train_rmse_target: 0.2894 train_rmse_stderror: 0.02372 train_kl_div: 0.178\n","val_rmse_target: 0.5022 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 184\n","train_rmse_target: 0.2321 train_rmse_stderror: 0.02322 train_kl_div: 0.1244\n","val_rmse_target: 0.4806 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.4 seconds\n","Epoch: 4 batch_num: 4\n","train_rmse_target: 0.1448 train_rmse_stderror: 0.01977 train_kl_div: 0.04854\n","val_rmse_target: 0.4977 val_rmse_stderror: 1.766\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 20\n","train_rmse_target: 0.1687 train_rmse_stderror: 0.03673 train_kl_div: 0.05752\n","val_rmse_target: 0.4804 val_rmse_stderror: 1.776\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 28\n","train_rmse_target: 0.1545 train_rmse_stderror: 0.02652 train_kl_div: 0.04801\n","val_rmse_target: 0.5001 val_rmse_stderror: 1.771\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","32 steps took 40.7 seconds\n","Epoch: 4 batch_num: 60\n","train_rmse_target: 0.1393 train_rmse_stderror: 0.01707 train_kl_div: 0.04167\n","val_rmse_target: 0.49 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 68\n","train_rmse_target: 0.1702 train_rmse_stderror: 0.02069 train_kl_div: 0.05281\n","val_rmse_target: 0.4912 val_rmse_stderror: 1.768\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","16 steps took 20.3 seconds\n","Epoch: 4 batch_num: 84\n","train_rmse_target: 0.1656 train_rmse_stderror: 0.03103 train_kl_div: 0.04344\n","val_rmse_target: 0.4891 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 92\n","train_rmse_target: 0.1218 train_rmse_stderror: 0.0162 train_kl_div: 0.03279\n","val_rmse_target: 0.4868 val_rmse_stderror: 1.766\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 100\n","train_rmse_target: 0.145 train_rmse_stderror: 0.03149 train_kl_div: 0.04675\n","val_rmse_target: 0.4863 val_rmse_stderror: 1.766\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 108\n","train_rmse_target: 0.1196 train_rmse_stderror: 0.01729 train_kl_div: 0.03321\n","val_rmse_target: 0.4841 val_rmse_stderror: 1.766\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 116\n","train_rmse_target: 0.1227 train_rmse_stderror: 0.01558 train_kl_div: 0.0323\n","val_rmse_target: 0.4854 val_rmse_stderror: 1.766\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 124\n","train_rmse_target: 0.1263 train_rmse_stderror: 0.02163 train_kl_div: 0.03441\n","val_rmse_target: 0.4874 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 132\n","train_rmse_target: 0.1425 train_rmse_stderror: 0.02184 train_kl_div: 0.04735\n","val_rmse_target: 0.4891 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 140\n","train_rmse_target: 0.1144 train_rmse_stderror: 0.02314 train_kl_div: 0.02993\n","val_rmse_target: 0.4898 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 148\n","train_rmse_target: 0.1585 train_rmse_stderror: 0.02328 train_kl_div: 0.04346\n","val_rmse_target: 0.4896 val_rmse_stderror: 1.767\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 156\n","train_rmse_target: 0.1657 train_rmse_stderror: 0.02355 train_kl_div: 0.05767\n","val_rmse_target: 0.489 val_rmse_stderror: 1.768\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.2 seconds\n","Epoch: 4 batch_num: 164\n","train_rmse_target: 0.13 train_rmse_stderror: 0.01975 train_kl_div: 0.03206\n","val_rmse_target: 0.4886 val_rmse_stderror: 1.768\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 172\n","train_rmse_target: 0.2005 train_rmse_stderror: 0.02764 train_kl_div: 0.07886\n","val_rmse_target: 0.4884 val_rmse_stderror: 1.768\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","8 steps took 10.1 seconds\n","Epoch: 4 batch_num: 180\n","train_rmse_target: 0.128 train_rmse_stderror: 0.02596 train_kl_div: 0.03738\n","val_rmse_target: 0.4883 val_rmse_stderror: 1.768\n","Still best_val_rmse: 0.4773 (from epoch 3)\n","\n","Performance estimates:\n","[0.48686168177282346, 0.49318919309887377, 0.4632869161410703, 0.48028027209396107, 0.47730628278272486]\n","Mean: 0.48018486917789066\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m4v-cGx-Mv7S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626789620999,"user_tz":-540,"elapsed":31,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"315e36c2-e076-427a-a529-def31df278fb"},"source":["print(list_val_rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.48686168177282346, 0.49318919309887377, 0.4632869161410703, 0.48028027209396107, 0.47730628278272486]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q2CdCMuIKDMP"},"source":["#rep = MemReporter(model)\n","#rep.report()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLl1yDOOKIe7"},"source":["#rep = MemReporter(model.roberta)\n","#rep.report()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qkqnknA_m9D"},"source":["#gpuinfo()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwrqSMdYA6Pu"},"source":["#del model\n","#del optimizer \n","#del train_loader\n","#del val_loader\n","#del scheduler \n","#del list_val_rmse\n","#del train_indices\n","#del val_indices\n","#del tokenizer\n","#torch.cuda.empty_cache()\n","#gpuinfo()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wXcHyUSJXecL"},"source":["# upload models"]},{"cell_type":"code","metadata":{"id":"YIV6UllSIGoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626789739804,"user_tz":-540,"elapsed":118821,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"143e4576-7e7d-4957-c755-cc026bf9c75f"},"source":["%cd\n","!mkdir .kaggle\n","!mkdir /content/model\n","!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\n","\n","!cp -r /content/model_1.pth /content/model/model_1.pth\n","!cp -r /content/model_2.pth /content/model/model_2.pth\n","!cp -r /content/model_3.pth /content/model/model_3.pth\n","!cp -r /content/model_4.pth /content/model/model_4.pth\n","!cp -r /content/model_5.pth /content/model/model_5.pth"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"14ddOZH4IMam","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626790174947,"user_tz":-540,"elapsed":435159,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"2f811b90-7453-4fcc-c55d-cd11cc68505a"},"source":["def dataset_upload():\n","    import json\n","    from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","    id = f'{USERID}/{EX_NO}'\n","\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = id\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = f'{EX_NO}'\n","\n","    with open(UPLOAD_DIR / 'dataset-metadata.json', 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","\n","    api = KaggleApi()\n","    api.authenticate()\n","\n","    # データセットがない場合\n","    if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n","        api.dataset_create_new(folder=UPLOAD_DIR,\n","                               convert_to_csv=False,\n","                               dir_mode='skip')\n","    # データセットがある場合\n","    else:\n","        api.dataset_create_version(folder=UPLOAD_DIR,\n","                                   version_notes='update',\n","                                   convert_to_csv=False,\n","                                   delete_old_versions=True,\n","                                   dir_mode='skip')\n","dataset_upload()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting upload for file model_2.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [01:28<00:00, 16.0MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_2.pth (1GB)\n","Starting upload for file model_4.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [01:26<00:00, 16.4MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_4.pth (1GB)\n","Starting upload for file model_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [01:25<00:00, 16.7MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_1.pth (1GB)\n","Starting upload for file model_5.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [01:23<00:00, 17.0MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_5.pth (1GB)\n","Starting upload for file model_3.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [01:24<00:00, 16.8MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_3.pth (1GB)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"huJwVMSAPuDO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zzuBPobmLFu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wpc8ro9hmNci"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ceDI72NumT5-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvRi_JQgwcKI"},"source":[""],"execution_count":null,"outputs":[]}]}