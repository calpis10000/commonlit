{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/calpis10000/commonlit/blob/main/02_nb/032_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:29.367686Z",
     "iopub.status.busy": "2021-06-19T09:29:29.367266Z",
     "iopub.status.idle": "2021-06-19T09:29:29.383067Z",
     "shell.execute_reply": "2021-06-19T09:29:29.381497Z",
     "shell.execute_reply.started": "2021-06-19T09:29:29.367601Z"
    },
    "id": "-ILQV9f9LjB7",
    "outputId": "8ed456f9-73ad-417e-be7d-e9e8ed92158f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "if 'google.colab' in sys.modules:  # colab環境特有の処理_初回のみ\n",
    "  # Google Driveのマウント\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules' \\\n",
    "   -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/requirements.txt' \\\n",
    "   --ignore-installed\n",
    "\n",
    "  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules' \\\n",
    "   transformers -U\n",
    "  !pip install gensim==4.0.1 --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zuIP4iaRWwmu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA\n",
      "BBB\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "if 5 >= n >= 4:\n",
    "    print('AAA')\n",
    "    \n",
    "if n % 2 == 1:\n",
    "    print('BBB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4fe6b780d7c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/poet-ml-bpC4FPMR-py3.8/lib/python3.8/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/poet-ml-bpC4FPMR-py3.8/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m _DEFAULT_TAGS = {\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/poet-ml-bpC4FPMR-py3.8/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .validation import (as_float_array,\n\u001b[1;32m     29\u001b[0m                          \u001b[0massert_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/poet-ml-bpC4FPMR-py3.8/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/poet-ml-bpC4FPMR-py3.8/lib/python3.8/site-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \"\"\"\n\u001b[0;32m--> 388\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/poet-ml-bpC4FPMR-py3.8/lib/python3.8/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m from scipy._lib._util import (_lazywhere, check_random_state, MapWrapper,\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/poet-ml-bpC4FPMR-py3.8/lib/python3.8/site-packages/scipy/spatial/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mkdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mckdtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqhull\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_spherical_voronoi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSphericalVoronoi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_plotutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.8.0/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.dataset import iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [1,2,3,4,2]\n",
    "sr_ = pd.Series(lst)\n",
    "sr_ = sr_.map(lambda x: True if x==2 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sr_.to_csv(sys.stdout, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:29.391024Z",
     "iopub.status.busy": "2021-06-19T09:29:29.389933Z",
     "iopub.status.idle": "2021-06-19T09:29:29.400487Z",
     "shell.execute_reply": "2021-06-19T09:29:29.399396Z",
     "shell.execute_reply.started": "2021-06-19T09:29:29.390974Z"
    },
    "id": "nUxJBg4jKDiV",
    "outputId": "fda7a0a4-e514-4c05-fcdf-200cd12e0200"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # colab特有の処理_2回目移行\n",
    "  # Google Driveのマウント\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "\n",
    "  # データセットをDriveから取得\n",
    "  !mkdir -p 'input'\n",
    "  !cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input' '/content/input'\n",
    "\n",
    "  # ライブラリのパス指定\n",
    "  sys.path.append('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:29.403168Z",
     "iopub.status.busy": "2021-06-19T09:29:29.402593Z",
     "iopub.status.idle": "2021-06-19T09:29:37.324476Z",
     "shell.execute_reply": "2021-06-19T09:29:37.323442Z",
     "shell.execute_reply.started": "2021-06-19T09:29:29.403109Z"
    },
    "id": "jnI6rNwDGJKD",
    "papermill": {
     "duration": 6.051579,
     "end_time": "2021-06-12T04:12:35.921582",
     "exception": false,
     "start_time": "2021-06-12T04:12:29.870003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# basic\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import yaml\n",
    "import warnings\n",
    "import random\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import hashlib\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# usual\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# preprocess\n",
    "from fasttext import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "#import texthero as hero\n",
    "import nltk\n",
    "import collections\n",
    "import gensim\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "import cv2\n",
    "import string\n",
    "import re\n",
    "import fasttext\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "#import optuna.integration.lightgbm as lgb  # チューニング用\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport  # profile report を作る用\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# plot settings\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = False\n",
    "plt.rcParams['font.family'] = 'sans_serif'\n",
    "sns.set(style=\"whitegrid\",  palette=\"muted\", color_codes=True, rc={'grid.linestyle': '--'})\n",
    "red = sns.xkcd_rgb[\"light red\"]\n",
    "green = sns.xkcd_rgb[\"medium green\"]\n",
    "blue = sns.xkcd_rgb[\"denim blue\"]\n",
    "\n",
    "# plot extentions\n",
    "#import japanize_matplotlib\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (AutoModel, AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "y_ = Fore.YELLOW\n",
    "r_ = Fore.RED\n",
    "g_ = Fore.GREEN\n",
    "b_ = Fore.BLUE\n",
    "m_ = Fore.MAGENTA\n",
    "c_ = Fore.CYAN\n",
    "sr_ = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.328801Z",
     "iopub.status.busy": "2021-06-19T09:29:37.328399Z",
     "iopub.status.idle": "2021-06-19T09:29:37.340258Z",
     "shell.execute_reply": "2021-06-19T09:29:37.339155Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.328753Z"
    },
    "id": "lm5sI6LlGJKG",
    "outputId": "ab4add12-8f94-4a51-9f95-9c08397a4162",
    "papermill": {
     "duration": 0.044898,
     "end_time": "2021-06-12T04:12:36.003748",
     "exception": false,
     "start_time": "2021-06-12T04:12:35.95885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 試験ID生成\n",
    "trial_prefix = 'nb034-01'  # ←手動で指定 \n",
    "dttm_now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "trial_id = f'{trial_prefix}_{dttm_now}'\n",
    "\n",
    "print(trial_prefix)\n",
    "print(trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.344905Z",
     "iopub.status.busy": "2021-06-19T09:29:37.344559Z",
     "iopub.status.idle": "2021-06-19T09:29:37.35362Z",
     "shell.execute_reply": "2021-06-19T09:29:37.352306Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.344874Z"
    },
    "id": "lQpccp-VGJKH",
    "papermill": {
     "duration": 0.043839,
     "end_time": "2021-06-12T04:12:36.083942",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.040103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# アウトプットの出力先指定\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    OUTPUT_DIR = Path(\".\")\n",
    "elif 'google.colab' in sys.modules:\n",
    "    OUTPUT_DIR = Path(\"/content/drive/MyDrive/Colab_Files/kaggle/commonlit/03_outputs\")\n",
    "else:\n",
    "    OUTPUT_DIR = Path(f\"../03_outputs/{trial_prefix}\")\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.357966Z",
     "iopub.status.busy": "2021-06-19T09:29:37.357569Z",
     "iopub.status.idle": "2021-06-19T09:29:37.369552Z",
     "shell.execute_reply": "2021-06-19T09:29:37.368312Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.357937Z"
    },
    "id": "NWrez3qyGJKH",
    "papermill": {
     "duration": 0.042944,
     "end_time": "2021-06-12T04:12:36.163441",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.120497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seed固定\n",
    "def set_seed(seed=2021):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.371915Z",
     "iopub.status.busy": "2021-06-19T09:29:37.371434Z",
     "iopub.status.idle": "2021-06-19T09:29:37.382691Z",
     "shell.execute_reply": "2021-06-19T09:29:37.381096Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.371871Z"
    },
    "id": "6O_5deZoGJKH",
    "papermill": {
     "duration": 0.042234,
     "end_time": "2021-06-12T04:12:36.243086",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.200852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# インプットフォルダ指定\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    DATA_DIR = '../input/commonlitreadabilityprize/'\n",
    "elif 'google.colab' in sys.modules:\n",
    "    DATA_DIR = '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input/commonlitreadabilityprize/'\n",
    "else:\n",
    "    DATA_DIR = '../00_input/commonlitreadabilityprize/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.388422Z",
     "iopub.status.busy": "2021-06-19T09:29:37.388077Z",
     "iopub.status.idle": "2021-06-19T09:29:37.497618Z",
     "shell.execute_reply": "2021-06-19T09:29:37.496534Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.38839Z"
    },
    "id": "KxOQu4miGJKI",
    "papermill": {
     "duration": 0.120829,
     "end_time": "2021-06-12T04:12:36.400695",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.279866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read_data\n",
    "train_base = pd.read_csv(DATA_DIR + 'train.csv')\n",
    "test_base = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "sample = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yA1vUylZGJKI",
    "papermill": {
     "duration": 0.036408,
     "end_time": "2021-06-12T04:12:36.47358",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.437172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 特徴作成_共通処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.500637Z",
     "iopub.status.busy": "2021-06-19T09:29:37.500167Z",
     "iopub.status.idle": "2021-06-19T09:29:37.506974Z",
     "shell.execute_reply": "2021-06-19T09:29:37.50533Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.500594Z"
    },
    "id": "5HIY2BfaGJKI",
    "papermill": {
     "duration": 0.043267,
     "end_time": "2021-06-12T04:12:36.554272",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.511005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ベースとなる継承元のクラス\n",
    "class BaseBlock(object):\n",
    "    def fit(self, input_df, y=None):\n",
    "        return self.transform(input_df)\n",
    "    def transform(self, input_df):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhC7R4DNGJKJ",
    "papermill": {
     "duration": 0.036407,
     "end_time": "2021-06-12T04:12:36.626866",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.590459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## テキスト特徴_共通処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.510018Z",
     "iopub.status.busy": "2021-06-19T09:29:37.509124Z",
     "iopub.status.idle": "2021-06-19T09:29:37.518493Z",
     "shell.execute_reply": "2021-06-19T09:29:37.517413Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.509971Z"
    },
    "id": "GA9ujg-PGJKJ",
    "outputId": "3a7079cf-3f14-4a8f-a893-e683ebed24b2",
    "papermill": {
     "duration": 0.043001,
     "end_time": "2021-06-12T04:12:36.706394",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.663393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ローカルの場合、stopwordsをダウンロード\n",
    "import nltk\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    pass\n",
    "else:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    os.listdir(os.path.expanduser('~/nltk_data/corpora/stopwords/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.521018Z",
     "iopub.status.busy": "2021-06-19T09:29:37.520418Z",
     "iopub.status.idle": "2021-06-19T09:29:37.545534Z",
     "shell.execute_reply": "2021-06-19T09:29:37.544347Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.520974Z"
    },
    "id": "2z0uB87rGJKJ",
    "papermill": {
     "duration": 0.061229,
     "end_time": "2021-06-12T04:12:36.803981",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.742752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# テキスト前処理\n",
    "# https://www.kaggle.com/alaasedeeq/commonlit-readability-eda\n",
    "\n",
    "#filtering the unwanted symbols, spaces, ....etc\n",
    "to_replace_by_space = re.compile('[/(){}\\[\\]|@,;]')\n",
    "punctuation = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "bad_symbols = re.compile('[^0-9a-z #+_]')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    '''\n",
    "    text: a string\n",
    "    returna modified version of the string\n",
    "    '''\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(punctuation, '',text)\n",
    "    text = re.sub(to_replace_by_space, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(bad_symbols, \"\", text)         # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = \" \".join([word for word in text.split(\" \") if word not in stopwords]) # delete stopwords from text\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.547916Z",
     "iopub.status.busy": "2021-06-19T09:29:37.547264Z",
     "iopub.status.idle": "2021-06-19T09:29:37.55595Z",
     "shell.execute_reply": "2021-06-19T09:29:37.554336Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.547869Z"
    },
    "id": "-JQ2zWeLGJKK",
    "papermill": {
     "duration": 0.042912,
     "end_time": "2021-06-12T04:12:36.883404",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.840492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_normalization(s:pd.Series):\n",
    "    x = s.apply(text_prepare)\n",
    "    return x\n",
    "\n",
    "# Counterオブジェクトを取得\n",
    "def get_counter(text:str):\n",
    "    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n",
    "    counter = collections.Counter(text_list)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49-Mfs-_GJKK",
    "papermill": {
     "duration": 0.037032,
     "end_time": "2021-06-12T04:12:36.956794",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.919762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 前処理_品詞変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.558391Z",
     "iopub.status.busy": "2021-06-19T09:29:37.557657Z",
     "iopub.status.idle": "2021-06-19T09:29:37.568172Z",
     "shell.execute_reply": "2021-06-19T09:29:37.5666Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.558344Z"
    },
    "id": "r0M4RfijGJKK",
    "papermill": {
     "duration": 0.044646,
     "end_time": "2021-06-12T04:12:37.037712",
     "exception": false,
     "start_time": "2021-06-12T04:12:36.993066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# テキスト情報を品詞に変換\n",
    "def get_pos_tag(text:str):\n",
    "    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n",
    "    pos_tag = [i[1] for i in nltk.pos_tag(text_list)]\n",
    "    return pos_tag\n",
    "\n",
    "def get_pos_tag_to_text(text:str):\n",
    "    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n",
    "    pos_tag = [i[1] for i in nltk.pos_tag(text_list)]\n",
    "    return \" \".join(pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypJnPS5SGJKK",
    "papermill": {
     "duration": 0.038386,
     "end_time": "2021-06-12T04:12:37.112346",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.07396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## テキスト特徴_シンプルなTF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.571273Z",
     "iopub.status.busy": "2021-06-19T09:29:37.570301Z",
     "iopub.status.idle": "2021-06-19T09:29:37.586411Z",
     "shell.execute_reply": "2021-06-19T09:29:37.585364Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.57121Z"
    },
    "id": "Ua4PSnv9GJKL",
    "papermill": {
     "duration": 0.05767,
     "end_time": "2021-06-12T04:12:37.213368",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.155698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参考: https://www.guruguru.science/competitions/16/discussions/556029f7-484d-40d4-ad6a-9d86337487e2/\n",
    "\n",
    "class TfidfSimpleBlock(BaseBlock):\n",
    "    \"\"\"シンプルなTF-IDF特徴を作成する block\"\"\"\n",
    "    def __init__(self, column: str, max_features=50, ngram_range=(1,1), use_idf=True):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.max_features=max_features\n",
    "        self.ngram_range=ngram_range\n",
    "        self.use_idf=use_idf\n",
    "        self.param_prefix=f\"col={column}_max_features={max_features}_\\\n",
    "                              ngram={ngram_range[0]}_{ngram_range[1]}_use_idf={use_idf}\"\n",
    "\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "\n",
    "    def get_master(self, _master_df):\n",
    "        \"\"\"tdidfを計算するための全体集合を返す.\"\"\"\n",
    "        return _master_df\n",
    "\n",
    "    def fit(self, \n",
    "            input_df, \n",
    "            _master_df=None, \n",
    "            y=None\n",
    "           ):\n",
    "        master_df = input_df if _master_df is None else self.get_master(_master_df)\n",
    "        text = self.preprocess(master_df)\n",
    "        self.vectorizer_ = TfidfVectorizer(max_features=self.max_features\n",
    "                                      ,ngram_range=self.ngram_range\n",
    "                                      ,use_idf=self.use_idf)\n",
    "\n",
    "        self.vectorizer_.fit(text)\n",
    "        self.prefix = 'tfidf' if self.use_idf == True else 'tf'\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        text = self.preprocess(input_df)\n",
    "        z = self.vectorizer_.transform(text)\n",
    "\n",
    "        out_df = pd.DataFrame(z.toarray())\n",
    "        out_df.columns = self.vectorizer_.get_feature_names()\n",
    "        return out_df.add_prefix(f'{self.prefix}_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_YjPLBcGJKN",
    "papermill": {
     "duration": 0.042255,
     "end_time": "2021-06-12T04:12:37.298884",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.256629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## テキスト特徴_学習済みモデル（gemsim経由）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.589098Z",
     "iopub.status.busy": "2021-06-19T09:29:37.58856Z",
     "iopub.status.idle": "2021-06-19T09:29:37.611789Z",
     "shell.execute_reply": "2021-06-19T09:29:37.610531Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.589027Z"
    },
    "id": "HkHVOrKQGJKN",
    "papermill": {
     "duration": 0.055099,
     "end_time": "2021-06-12T04:12:37.3917",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.336601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参考: https://zenn.dev/koukyo1994/articles/9b1da2482d8ba1\n",
    "# 参考: https://github.com/yagays/swem\n",
    "\n",
    "class GensimPreTrainedBlock(BaseBlock):\n",
    "    \"\"\"\n",
    "    文書をgemsim経由で学習済みモデルのベクトル表現へ変換するblock\n",
    "    モデルは別途入手し、インスタンス作成時に指定する。\n",
    "    モデル名は手動で入力する想定（デフォルトではgensim_pretrained で入る）\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 column: str,\n",
    "                 model:KeyedVectors,\n",
    "                 model_name='gensim_pretrained',\n",
    "                 swem='aver'):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.swem = swem # TODO:例外処理['aver', 'max', 'concat', 'hier']\n",
    "        self.param_prefix= f\"col={column}_model_name={model_name}_swem={swem}\"\n",
    "\n",
    "    def fit(self, input_df, y=None):\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        text = self.preprocess(input_df)\n",
    "        \n",
    "        if self.swem == 'aver':\n",
    "            feat = text.map(lambda x: self.average_pooling(x))\n",
    "        elif self.swem == 'max': \n",
    "            feat = text.map(lambda x: self.max_pooling(x))\n",
    "        elif self.swem == 'concat': \n",
    "            feat = text.map(lambda x: self.concat_average_max_pooling(x))\n",
    "        elif self.swem == 'hier': \n",
    "            feat = text.map(lambda x: self.hierarchical_pooling(x, n=3))\n",
    "            \n",
    "        out_df = pd.DataFrame(np.stack(feat.values))\n",
    "\n",
    "        return out_df.add_prefix(f'{self.model_name}_{self.column}_{self.swem}')\n",
    "    \n",
    "    # 前処理\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "    \n",
    "    # 文書ベクトルの取得\n",
    "    def get_sentence_vector(self, x: str):\n",
    "        ndim = self.model.vector_size\n",
    "        embeddings = [\n",
    "            self.model[word]\n",
    "            if word in self.model\n",
    "            else np.zeros(ndim)\n",
    "            for word in x.split()\n",
    "        ]\n",
    "\n",
    "        if len(embeddings) == 0:\n",
    "            return np.zeros(ndim, dtype=np.float32)\n",
    "        else:\n",
    "            return embeddings\n",
    "    \n",
    "    # SWEMの各処理\n",
    "    def average_pooling(self, text):\n",
    "        word_embeddings = self.get_sentence_vector(text)\n",
    "        return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "    def max_pooling(self, text):\n",
    "        word_embeddings = self.get_sentence_vector(text)\n",
    "        return np.max(word_embeddings, axis=0)\n",
    "\n",
    "    def concat_average_max_pooling(self, text):\n",
    "        word_embeddings = self.get_sentence_vector(text)\n",
    "        return np.r_[np.mean(word_embeddings, axis=0), np.max(word_embeddings, axis=0)]\n",
    "\n",
    "    def hierarchical_pooling(self, text, n):\n",
    "        word_embeddings = self.get_sentence_vector(text)\n",
    "\n",
    "        text_len = len(word_embeddings) # TODO: これで合ってるか要確認\n",
    "        if n > text_len:\n",
    "            raise ValueError(f\"window size must be less than text length / window_size:{n} text_length:{text_len}\")\n",
    "        window_average_pooling_vec = [np.mean(word_embeddings[i:i + n], axis=0) for i in range(text_len - n + 1)]\n",
    "\n",
    "        return np.max(window_average_pooling_vec, axis=0)    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg0N6qS4GJKO",
    "papermill": {
     "duration": 0.036269,
     "end_time": "2021-06-12T04:12:37.464408",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.428139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## テキスト特徴_fasttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.614485Z",
     "iopub.status.busy": "2021-06-19T09:29:37.613449Z",
     "iopub.status.idle": "2021-06-19T09:29:37.628471Z",
     "shell.execute_reply": "2021-06-19T09:29:37.627204Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.614391Z"
    },
    "id": "IDgINZMXGJKO",
    "papermill": {
     "duration": 0.045006,
     "end_time": "2021-06-12T04:12:37.546708",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.501702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参考: https://zenn.dev/koukyo1994/articles/9b1da2482d8ba1\n",
    "class FasttextBlock(BaseBlock):\n",
    "    \"\"\"文書をfasttextのテキスト表現へ変換する block\"\"\"\n",
    "    def __init__(self, column: str, ft_model:fasttext.FastText._FastText):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.ft_model = ft_model\n",
    "        self.param_prefix= f\"col={column}\"\n",
    "\n",
    "    # 前処理\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "        \n",
    "    def fit(self, input_df, y=None):\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        text = self.preprocess(input_df)\n",
    "        feat = text.map(lambda x: ft_model.get_sentence_vector(x))\n",
    "        out_df = pd.DataFrame(np.stack(feat.values))\n",
    "\n",
    "        return out_df.add_prefix(f'fasttext_{self.column}_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7A1XXT0GJKP",
    "papermill": {
     "duration": 0.036498,
     "end_time": "2021-06-12T04:12:37.619495",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.582997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## テキスト特徴_統計量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.631026Z",
     "iopub.status.busy": "2021-06-19T09:29:37.630429Z",
     "iopub.status.idle": "2021-06-19T09:29:37.648609Z",
     "shell.execute_reply": "2021-06-19T09:29:37.646913Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.630982Z"
    },
    "id": "R0qvzof6GJKP",
    "papermill": {
     "duration": 0.049015,
     "end_time": "2021-06-12T04:12:37.704812",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.655797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDescriptionBlock(BaseBlock):\n",
    "    \"\"\"テキストに関する統計量を返す block\"\"\"\n",
    "    def __init__(self, column: str):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.param_prefix = f'col={column}'\n",
    "\n",
    "    # 前処理\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "        \n",
    "    def fit(self, input_df, y=None):\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        # 前処理\n",
    "        self.text = self.preprocess(input_df)\n",
    "        self.counters = self.text.map(get_counter)\n",
    "\n",
    "        # 変換処理\n",
    "        _length = input_df[self.column].fillna('').map(lambda x: len(x) if x!='' else np.nan)\n",
    "        _wrd_cnt = self.counters.map(lambda x: sum(x.values()))\n",
    "        _wrd_nuniq = self.counters.map(lambda x: len(x))\n",
    "        _wrd_mean = self.counters.map(lambda x: np.mean(list(x.values())))\n",
    "        _wrd_max = self.counters.map(lambda x: np.max(list(x.values())))\n",
    "        \n",
    "        word_length = self.counters.map(lambda x: np.array([len(i) for i in x.keys()]))\n",
    "        word_length_desc = word_length.map(lambda x: pd.Series(x.ravel()).describe())\n",
    "        _word_length_desc_df = pd.DataFrame(word_length_desc.tolist()).iloc[:,1:]\n",
    "        _word_length_desc_df = _word_length_desc_df.add_prefix('word_length_')\n",
    "        \n",
    "        out_df = pd.concat([_length, _wrd_cnt, _wrd_nuniq, _wrd_mean, _wrd_max], axis=1)\n",
    "        out_df.columns = ['text_length', 'word_count', 'word_nunique', 'word_appearance_mean', 'word_appearance_max']\n",
    "        out_df = pd.concat([out_df, _word_length_desc_df], axis=1)\n",
    "        return out_df.add_suffix(f'_{self.column}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8KA35xKGJKP",
    "papermill": {
     "duration": 0.03615,
     "end_time": "2021-06-12T04:12:37.777053",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.740903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## テキスト特徴_CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.651214Z",
     "iopub.status.busy": "2021-06-19T09:29:37.650696Z",
     "iopub.status.idle": "2021-06-19T09:29:37.666587Z",
     "shell.execute_reply": "2021-06-19T09:29:37.665154Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.65116Z"
    },
    "id": "hUDuslqgGJKP",
    "papermill": {
     "duration": 0.052501,
     "end_time": "2021-06-12T04:12:37.86927",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.816769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CountVectorizerBlock(BaseBlock):\n",
    "    \"\"\"CountVectorizer x SVD による圧縮を行なう block\"\"\"\n",
    "    def __init__(self, column: str, master_df=None, n_components=50, ngram_range=(1,1)):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.master_df=master_df\n",
    "        self.n_components=n_components\n",
    "        self.ngram_range=ngram_range\n",
    "        self.param_prefix = f\"col={column}_comp={n_components}_ngram={''.join([str(i) for i in self.ngram_range])}\"\n",
    "\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "\n",
    "    def fit(self, \n",
    "            input_df, \n",
    "            y=None\n",
    "           ):\n",
    "        master_df = input_df if self.master_df is None else self.master_df\n",
    "        text = self.preprocess(master_df)\n",
    "        self.pileline_ = Pipeline([\n",
    "            ('tfidf', CountVectorizer(ngram_range=self.ngram_range)),\n",
    "            ('svd', TruncatedSVD(n_components=self.n_components, random_state=SEED)),\n",
    "        ])\n",
    "\n",
    "        self.pileline_.fit(text)\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        text = self.preprocess(input_df)\n",
    "        z = self.pileline_.transform(text)\n",
    "\n",
    "        out_df = pd.DataFrame(z)\n",
    "        return out_df.add_prefix(f'countvect_{self.column}_{\"_\".join([str(i) for i in self.ngram_range])}_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHad_XvWGJKP",
    "papermill": {
     "duration": 0.039945,
     "end_time": "2021-06-12T04:12:37.946997",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.907052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## テキスト特徴_TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.670696Z",
     "iopub.status.busy": "2021-06-19T09:29:37.669984Z",
     "iopub.status.idle": "2021-06-19T09:29:37.685339Z",
     "shell.execute_reply": "2021-06-19T09:29:37.683773Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.670663Z"
    },
    "id": "57eDPHwLGJKQ",
    "papermill": {
     "duration": 0.051996,
     "end_time": "2021-06-12T04:12:38.040718",
     "exception": false,
     "start_time": "2021-06-12T04:12:37.988722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参考: https://www.guruguru.science/competitions/16/discussions/556029f7-484d-40d4-ad6a-9d86337487e2/\n",
    "\n",
    "class TfidfBlock(BaseBlock):\n",
    "    \"\"\"tfidf x SVD による圧縮を行なう block\"\"\"\n",
    "    def __init__(self, column: str, master_df=None, n_components=50, ngram_range=(1,1)):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.master_df=master_df\n",
    "        self.n_components=n_components\n",
    "        self.ngram_range=ngram_range\n",
    "        self.param_prefix = f\"col={column}_comp={n_components}_ngram={''.join([str(i) for i in self.ngram_range])}\"\n",
    "\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "\n",
    "    def fit(self, \n",
    "            input_df, \n",
    "            y=None\n",
    "           ):\n",
    "        master_df = input_df if self.master_df is None else self.master_df\n",
    "        text = self.preprocess(master_df)\n",
    "        self.pileline_ = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(max_features=100000, ngram_range=self.ngram_range)),\n",
    "            ('svd', TruncatedSVD(n_components=self.n_components, random_state=SEED)),\n",
    "        ])\n",
    "\n",
    "        self.pileline_.fit(text)\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        text = self.preprocess(input_df)\n",
    "        z = self.pileline_.transform(text)\n",
    "\n",
    "        out_df = pd.DataFrame(z)\n",
    "        return out_df.add_prefix(f'tfidf_{self.column}_{\"_\".join([str(i) for i in self.ngram_range])}_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2agA8GJvGJKQ",
    "papermill": {
     "duration": 0.038549,
     "end_time": "2021-06-12T04:12:38.120283",
     "exception": false,
     "start_time": "2021-06-12T04:12:38.081734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## テキスト特徴_W2V(データセットから学習)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.692369Z",
     "iopub.status.busy": "2021-06-19T09:29:37.691962Z",
     "iopub.status.idle": "2021-06-19T09:29:37.708366Z",
     "shell.execute_reply": "2021-06-19T09:29:37.70705Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.6923Z"
    },
    "id": "AUTdZA3OGJKQ",
    "papermill": {
     "duration": 0.050697,
     "end_time": "2021-06-12T04:12:38.208101",
     "exception": false,
     "start_time": "2021-06-12T04:12:38.157404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.guruguru.science/competitions/16/discussions/2fafef06-5a26-4d33-b535-a94cc9549ac4/\n",
    "# https://www.guruguru.science/competitions/16/discussions/4a6f5f84-8491-4324-ba69-dec49dc648cd/\n",
    "\n",
    "def hashfxn(x):\n",
    "    return int(hashlib.md5(str(x).encode()).hexdigest(), 16)\n",
    "\n",
    "class W2VTrainBlock(BaseBlock):\n",
    "    \"\"\"Word2Vecを学習し、文書のベクトル表現を得るブロック。\n",
    "       学習済みモデルを使うパターンは、別に作成するものとする。\"\"\"\n",
    "    def __init__(self, \n",
    "                 column: str, \n",
    "                 master_df=None,\n",
    "                 model_size=50, \n",
    "                 min_count=1, \n",
    "                 window=5,\n",
    "                 n_iter=100\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.master_df = master_df\n",
    "        self.model_size = model_size\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.n_iter = n_iter\n",
    "        self.param_prefix = f\"col={column}_model_size={model_size}_min_count={min_count}_window={window}_n_iter={n_iter}\"\n",
    "\n",
    "    def preprocess(self, input_df):\n",
    "        x = text_normalization(input_df[self.column])\n",
    "        return x\n",
    "\n",
    "    def fit(self, \n",
    "            input_df, \n",
    "            y=None\n",
    "           ):\n",
    "        master_df = input_df if self.master_df is None else self.master_df\n",
    "        text = self.preprocess(master_df)\n",
    "        word_lists = text.map(lambda x: [i for i in x.split(' ') if i not in (' ')])\n",
    "        self.w2v_model = word2vec.Word2Vec(word_lists.values.tolist(),\n",
    "                                      vector_size=self.model_size,\n",
    "                                      min_count=self.min_count,\n",
    "                                      window=self.window,\n",
    "                                      seed=SEED,\n",
    "                                      workers=1,\n",
    "                                      hashfxn=hashfxn,\n",
    "                                      epochs=self.n_iter)\n",
    "\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        text = self.preprocess(input_df)\n",
    "        word_lists = text.map(lambda x: [i for i in x.split(' ') if i not in (' ')])\n",
    "\n",
    "        # 各文章ごとにそれぞれの単語をベクトル表現に直し、平均をとって文章ベクトルにする\n",
    "        sentence_vectors = word_lists.progress_apply(\n",
    "            lambda x: np.mean([self.w2v_model.wv[e] for e in x], axis=0))\n",
    "        sentence_vectors = np.vstack([x for x in sentence_vectors])\n",
    "        sentence_vector_df = pd.DataFrame(sentence_vectors,\n",
    "                                          columns=[f\"w2v_{self.column}_w{self.window}_{i}\"\n",
    "                                                   for i in range(self.model_size)])\n",
    "        \n",
    "        return sentence_vector_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.712323Z",
     "iopub.status.busy": "2021-06-19T09:29:37.711542Z",
     "iopub.status.idle": "2021-06-19T09:29:37.728435Z",
     "shell.execute_reply": "2021-06-19T09:29:37.727043Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.712276Z"
    },
    "id": "9u9hHhn-GJKQ",
    "papermill": {
     "duration": 0.049208,
     "end_time": "2021-06-12T04:12:38.293556",
     "exception": false,
     "start_time": "2021-06-12T04:12:38.244348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用モデル指定\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    _bert_model_name = '../input/huggingface-bert-variants/bert-base-cased/bert-base-cased/'\n",
    "else: # ローカル or Colab\n",
    "    _bert_model_name = 'bert-base-cased'\n",
    "\n",
    "# 学習済みBERT\n",
    "class BertSequenceVectorizer:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' # cudaが無いならcpuを使えばいいじゃない\n",
    "        self.model_name = _bert_model_name # 学習済みモデルの名前を指定\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name) # 指定したmodel_nameでtokenizerを作成\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(self.model_name) # 指定したmodel_nameで学習済みmodelを作成\n",
    "        self.bert_model = self.bert_model.to(self.device)\n",
    "        self.max_len = 176\n",
    "\n",
    "    def get_imp(self, sentence : str) -> np.array:\n",
    "        inp = self.tokenizer.encode(sentence)\n",
    "        len_inp = len(inp)\n",
    "        return [inp, len_inp]\n",
    "\n",
    "    def vectorize(self, sentence : str) -> np.array:\n",
    "        inp = self.tokenizer.encode(sentence)\n",
    "        len_inp = len(inp)\n",
    "\n",
    "        if len_inp >= self.max_len:\n",
    "            inputs = inp[:self.max_len]\n",
    "            masks = [1] * self.max_len\n",
    "        else:\n",
    "            inputs = inp + [0] * (self.max_len - len_inp)\n",
    "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
    "\n",
    "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
    "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
    "\n",
    "        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n",
    "        seq_out, pooled_out = bert_out['last_hidden_state'], bert_out['pooler_output']\n",
    "\n",
    "        if torch.cuda.is_available():    \n",
    "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
    "        else:\n",
    "            return seq_out[0][0].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.730949Z",
     "iopub.status.busy": "2021-06-19T09:29:37.73041Z",
     "iopub.status.idle": "2021-06-19T09:29:37.788774Z",
     "shell.execute_reply": "2021-06-19T09:29:37.787666Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.730903Z"
    },
    "id": "qCLEOnw-hp6C"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# 使用モデル指定\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    _roberta_model_name = '../input/roberta-base/'\n",
    "else: # ローカル or Colab\n",
    "    _roberta_model_name = 'roberta-base'\n",
    "\n",
    "# 学習済みRoBerta\n",
    "class RoBertaSequenceVectorizer:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' # cudaが無いならcpuを使えばいいじゃない\n",
    "        self.model_name = _roberta_model_name # 学習済みモデルの名前を指定\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name) # 指定したmodel_nameでtokenizerを作成\n",
    "        self.bert_model = transformers.RobertaModel.from_pretrained(self.model_name) # 指定したmodel_nameで学習済みmodelを作成\n",
    "        self.bert_model = self.bert_model.to(self.device)\n",
    "        self.max_len = 176\n",
    "\n",
    "    def get_imp(self, sentence : str) -> np.array:\n",
    "        inp = self.tokenizer.encode(sentence)\n",
    "        len_inp = len(inp)\n",
    "        return [inp, len_inp]\n",
    "\n",
    "    def vectorize(self, sentence : str) -> np.array:\n",
    "        inp = self.tokenizer.encode(sentence)\n",
    "        len_inp = len(inp)\n",
    "\n",
    "        if len_inp >= self.max_len:\n",
    "            inputs = inp[:self.max_len]\n",
    "            masks = [1] * self.max_len\n",
    "        else:\n",
    "            inputs = inp + [0] * (self.max_len - len_inp)\n",
    "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
    "\n",
    "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
    "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
    "\n",
    "        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n",
    "        seq_out, pooled_out = bert_out['last_hidden_state'], bert_out['pooler_output']\n",
    "\n",
    "        if torch.cuda.is_available():    \n",
    "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
    "        else:\n",
    "            return seq_out[0][0].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:37.79099Z",
     "iopub.status.busy": "2021-06-19T09:29:37.790432Z",
     "iopub.status.idle": "2021-06-19T09:29:51.701607Z",
     "shell.execute_reply": "2021-06-19T09:29:51.700453Z",
     "shell.execute_reply.started": "2021-06-19T09:29:37.790928Z"
    },
    "id": "5laImyczGJKR",
    "outputId": "1061de93-b7d2-4380-9951-f21c82b0478e"
   },
   "outputs": [],
   "source": [
    "class BERTPreTrainedBlock(BaseBlock):\n",
    "    \"\"\"\n",
    "    学習済みBERTモデルを用いて、文書をベクトル表現へ変換するblock\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 column: str,\n",
    "                 model=BertSequenceVectorizer(),\n",
    "                 model_name='bert-base-cased'\n",
    "                ):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            column: str\n",
    "                変換対象のカラム名\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.param_prefix = f\"col={column}_model_name={model_name}\"\n",
    "\n",
    "    def fit(self, input_df, y=None):\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df):            \n",
    "        bert_out = input_df[self.column].progress_apply(lambda x: self.model.vectorize(x))\n",
    "        out_df = pd.DataFrame(np.stack(bert_out))\n",
    "        return out_df.add_prefix(f'{self.model_name}_{self.column}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMuHGMgMGJKR",
    "papermill": {
     "duration": 0.036164,
     "end_time": "2021-06-12T04:12:38.365751",
     "exception": false,
     "start_time": "2021-06-12T04:12:38.329587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# make_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:51.703946Z",
     "iopub.status.busy": "2021-06-19T09:29:51.703338Z",
     "iopub.status.idle": "2021-06-19T09:29:51.710121Z",
     "shell.execute_reply": "2021-06-19T09:29:51.708898Z",
     "shell.execute_reply.started": "2021-06-19T09:29:51.703902Z"
    },
    "id": "62EQQyzcGJKR"
   },
   "outputs": [],
   "source": [
    "# 特徴量の出力先指定\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    FEAT_DIR = Path(\".\") # 仮で指定。吐き出さない処理を考えた方が良いか？\n",
    "elif 'google.colab' in sys.modules:\n",
    "    FEAT_DIR = Path('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/04_feature')\n",
    "else:\n",
    "    FEAT_DIR = Path(f\"../04_feature\")\n",
    "    FEAT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-19T09:29:51.712389Z",
     "iopub.status.busy": "2021-06-19T09:29:51.711951Z",
     "iopub.status.idle": "2021-06-19T09:31:59.808514Z",
     "shell.execute_reply": "2021-06-19T09:31:59.805583Z",
     "shell.execute_reply.started": "2021-06-19T09:29:51.712349Z"
    },
    "id": "BnVRE4lUGJKR",
    "outputId": "6503f90c-a256-4598-d498-cf3ef2c6958e",
    "papermill": {
     "duration": 232.489126,
     "end_time": "2021-06-12T04:17:40.020806",
     "exception": false,
     "start_time": "2021-06-12T04:13:47.53168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# モデル入手\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    fast_path = '../input/fasttext-pretrained-crawl-vector-en-bin/cc.en.300.bin'\n",
    "    gen_glv_wiki_path = '../input/stanfords-glove-pretrained-word-vectors/glove.6B.300d.txt'\n",
    "    gen_glv_twi_path = '../input/glovetwitter27b-in-gensim-kv-format/glove.twitter.27B.200d.kv'\n",
    "    gen_ggl_path = '../input/gensim-google-data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "    ft_model = fasttext.load_model(fast_path)\n",
    "    gen_glv_wiki_model = KeyedVectors.load_word2vec_format(gen_glv_wiki_path, binary=False)\n",
    "    #gen_glv_twi_model = KeyedVectors.load(gen_glv_twi_path)\n",
    "    #gen_ggl_model = KeyedVectors.load_word2vec_format(gen_ggl_path, binary=True)\n",
    "\n",
    "elif 'google.colab' in sys.modules:\n",
    "    fast_path = '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/cc.en.300.bin'\n",
    "    gen_glv_wiki_model = api.load('glove-wiki-gigaword-300')\n",
    "    ft_model = fasttext.load_model(fast_path)\n",
    "\n",
    "else: # ローカルまたは自前のクラウド環境を想定\n",
    "    fast_path = '../97_pre_trained/cc.en.300.bin'\n",
    "    gen_glv_wiki_path = '~/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300'\n",
    "    gen_glv_twi_path = '~/gensim-data/glove-twitter-200/glove-twitter-200.gz'\n",
    "    gen_ggl_path = '~/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'\n",
    "\n",
    "    ft_model = fasttext.load_model('../97_pre_trained/cc.en.300.bin')\n",
    "    gen_glv_wiki_model = KeyedVectors.load_word2vec_format(gen_glv_wiki_path, binary=False)\n",
    "    #gen_glv_twi_model = KeyedVectors.load_word2vec_format(gen_glv_twi_path, binary=False)\n",
    "    #gen_ggl_model = KeyedVectors.load_word2vec_format(gen_ggl_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.810183Z",
     "iopub.status.idle": "2021-06-19T09:31:59.811118Z"
    },
    "id": "t_EaEnThGJKR",
    "papermill": {
     "duration": 0.173573,
     "end_time": "2021-06-12T04:13:47.053264",
     "exception": false,
     "start_time": "2021-06-12T04:13:46.879691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初期化\n",
    "train_feat = pd.DataFrame()\n",
    "test_feat = pd.DataFrame()\n",
    "train_target = train_base['target'].copy()\n",
    "\n",
    "# tfidf作成用のdf作成\n",
    "whole_df = pd.concat([train_base[['id', 'excerpt']], test_base[['id', 'excerpt']]], axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.812679Z",
     "iopub.status.idle": "2021-06-19T09:31:59.81346Z"
    },
    "id": "Y2AS4-TuGJKS"
   },
   "outputs": [],
   "source": [
    "# 特徴量の出力\n",
    "import feather \n",
    "def save_feather(input_df:pd.DataFrame, filename_:str):\n",
    "    input_df.to_feather(FEAT_DIR/filename_)\n",
    "    \n",
    "def load_feather(filename_:str):\n",
    "    return pd.read_feather(FEAT_DIR/filename_)\n",
    "\n",
    "def save_pickle(obj, filename_:str):\n",
    "    with open(FEAT_DIR/filename_, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    \n",
    "def load_pickle(filename_:str):\n",
    "    with open(FEAT_DIR/filename_, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.81551Z",
     "iopub.status.idle": "2021-06-19T09:31:59.816255Z"
    },
    "id": "fajQ6ho4GJKS"
   },
   "outputs": [],
   "source": [
    "# https://www.guruguru.science/competitions/16/discussions/95b7f8ec-a741-444f-933a-94c33b9e66be/\n",
    "# https://github.com/nyk510/vivid/blob/master/vivid/utils.py\n",
    "\n",
    "from time import time\n",
    "\n",
    "# 文字列を「*」で囲っていい感じに見せる関数？\n",
    "def decorate(s: str, decoration=None):\n",
    "    if decoration is None:\n",
    "        decoration = '★' * 20\n",
    "\n",
    "    return ' '.join([decoration, str(s), decoration])\n",
    "\n",
    "# 時間計測用\n",
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' ', verbose=0):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time() \n",
    "        if self.verbose is None:\n",
    "            return\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.818098Z",
     "iopub.status.idle": "2021-06-19T09:31:59.819204Z"
    },
    "id": "b-Dzkz0CGJKS"
   },
   "outputs": [],
   "source": [
    "# 一旦これで進めるが、もうちょっとスマートにしたい。\n",
    "def run_blocks(input_df, blocks, y=None, test=False, overwrite=False):\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    print(decorate('start run blocks...'))\n",
    "\n",
    "    with Timer(prefix='run test={}'.format(test)): \n",
    "        for block in blocks:\n",
    "            with Timer(prefix='\\t- {}'.format(str(block))):\n",
    "                name = block.__class__.__name__\n",
    "                params = block.param_prefix\n",
    "                filename_df = f\"{'Test' if test else 'Train'}_{name}_{params}.ftr\"\n",
    "                filepath_df = FEAT_DIR/filename_df\n",
    "\n",
    "                if filepath_df.exists():\n",
    "                    out_i = pd.read_feather(filepath_df)\n",
    "                    print(f'skip -> read:{filename_df}')\n",
    "                else:\n",
    "                    if not test:\n",
    "                        out_i = block.fit(input_df, y=y)\n",
    "                    else:\n",
    "                        out_i = block.transform(input_df)\n",
    "\n",
    "                    save_feather(out_i, filename_df)\n",
    "\n",
    "                assert len(input_df) == len(out_i)\n",
    "                out_df = pd.concat([out_df, out_i.add_suffix(f'@{name}')], axis=1)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.820999Z",
     "iopub.status.idle": "2021-06-19T09:31:59.822173Z"
    },
    "id": "QoIfLx2aaNWM",
    "outputId": "4a032c5f-367c-49e0-f532-166bfd6af393"
   },
   "outputs": [],
   "source": [
    "type(FEAT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.824002Z",
     "iopub.status.idle": "2021-06-19T09:31:59.824788Z"
    },
    "id": "JGYTL_EkGJKS",
    "outputId": "7dea5172-f1cd-45c5-9d91-e549ee5c68c0"
   },
   "outputs": [],
   "source": [
    "# 特徴作成: 実行\n",
    "feat_blocks = [TextDescriptionBlock('excerpt')\n",
    "              ,TfidfBlock('excerpt', master_df=whole_df, ngram_range=(1,1))\n",
    "              #,TfidfBlock('excerpt', master_df=whole_df, ngram_range=(2,2))\n",
    "              #,TfidfBlock('excerpt', master_df=whole_df, ngram_range=(1,2))\n",
    "              ,FasttextBlock('excerpt', ft_model)\n",
    "              #,W2VTrainBlock('excerpt', master_df=whole_df, window=3)\n",
    "              #,W2VTrainBlock('excerpt', master_df=whole_df, window=10)\n",
    "              #,W2VTrainBlock('excerpt', master_df=whole_df, window=100)\n",
    "              ,GensimPreTrainedBlock('excerpt'\n",
    "                                     ,gen_glv_wiki_model\n",
    "                                     ,model_name='glove_wiki_giga300'\n",
    "                                     ,swem='aver')\n",
    "             ,BERTPreTrainedBlock('excerpt')\n",
    "             ,BERTPreTrainedBlock('excerpt', model=RoBertaSequenceVectorizer(), model_name='roberta-base')\n",
    "              ]\n",
    "\n",
    "overwrite = False\n",
    "train_feat = run_blocks(train_base, feat_blocks, overwrite=overwrite)\n",
    "test_feat = run_blocks(test_base, feat_blocks, test=True, overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.826339Z",
     "iopub.status.idle": "2021-06-19T09:31:59.827324Z"
    },
    "id": "Cj1JHlRsGJKS",
    "outputId": "72dd8831-0fd1-4553-a7a0-c42f64e94ed4"
   },
   "outputs": [],
   "source": [
    "print(train_feat.shape)\n",
    "print(test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLRP: RoBerta  + SVM\n",
    "- https://www.kaggle.com/maunish/clrp-roberta-svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-07T17:24:29.629096Z",
     "iopub.status.busy": "2021-06-07T17:24:29.628657Z",
     "iopub.status.idle": "2021-06-07T17:24:37.370591Z",
     "shell.execute_reply": "2021-06-07T17:24:37.36925Z",
     "shell.execute_reply.started": "2021-06-07T17:24:29.629009Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (AutoModel, AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "y_ = Fore.YELLOW\n",
    "r_ = Fore.RED\n",
    "g_ = Fore.GREEN\n",
    "b_ = Fore.BLUE\n",
    "m_ = Fore.MAGENTA\n",
    "c_ = Fore.CYAN\n",
    "sr_ = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:24:37.373346Z",
     "iopub.status.busy": "2021-06-07T17:24:37.372821Z",
     "iopub.status.idle": "2021-06-07T17:24:37.478863Z",
     "shell.execute_reply": "2021-06-07T17:24:37.477659Z",
     "shell.execute_reply.started": "2021-06-07T17:24:37.373303Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
    "test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "sample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n",
    "\n",
    "num_bins = int(np.floor(1 + np.log2(len(train_data))))\n",
    "train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n",
    "\n",
    "target = train_data['target'].to_numpy()\n",
    "bins = train_data.bins.to_numpy()\n",
    "\n",
    "def rmse_score(y_true,y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:24:37.481597Z",
     "iopub.status.busy": "2021-06-07T17:24:37.481249Z",
     "iopub.status.idle": "2021-06-07T17:24:37.493125Z",
     "shell.execute_reply": "2021-06-07T17:24:37.492023Z",
     "shell.execute_reply.started": "2021-06-07T17:24:37.481565Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size':128,\n",
    "    'max_len':256,\n",
    "    'nfolds':5,\n",
    "    'seed':42,\n",
    "}\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONASSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed=config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:24:37.495773Z",
     "iopub.status.busy": "2021-06-07T17:24:37.495265Z",
     "iopub.status.idle": "2021-06-07T17:24:37.506445Z",
     "shell.execute_reply": "2021-06-07T17:24:37.505418Z",
     "shell.execute_reply.started": "2021-06-07T17:24:37.495728Z"
    }
   },
   "outputs": [],
   "source": [
    "class CLRPDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.excerpt = df['excerpt'].to_numpy()\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n",
    "                                max_length=config['max_len'],\n",
    "                                padding='max_length',truncation=True)\n",
    "        return encode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:24:37.50812Z",
     "iopub.status.busy": "2021-06-07T17:24:37.507715Z",
     "iopub.status.idle": "2021-06-07T17:24:37.523034Z",
     "shell.execute_reply": "2021-06-07T17:24:37.521813Z",
     "shell.execute_reply.started": "2021-06-07T17:24:37.50809Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.middle_features = hidden_dim\n",
    "\n",
    "        self.W = nn.Linear(in_features, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        self.out_features = hidden_dim\n",
    "\n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "\n",
    "        score = self.V(att)\n",
    "\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:24:37.52717Z",
     "iopub.status.busy": "2021-06-07T17:24:37.526813Z",
     "iopub.status.idle": "2021-06-07T17:24:37.537055Z",
     "shell.execute_reply": "2021-06-07T17:24:37.535796Z",
     "shell.execute_reply.started": "2021-06-07T17:24:37.52713Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('../input/roberta-base')    \n",
    "        self.head = AttentionHead(768,768,1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.head.out_features,1)\n",
    "\n",
    "    def forward(self,**xb):\n",
    "        x = self.roberta(**xb)[0]\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:24:37.539408Z",
     "iopub.status.busy": "2021-06-07T17:24:37.53885Z",
     "iopub.status.idle": "2021-06-07T17:24:37.55375Z",
     "shell.execute_reply": "2021-06-07T17:24:37.552289Z",
     "shell.execute_reply.started": "2021-06-07T17:24:37.539364Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(df,path,plot_losses=True, verbose=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"{device} is used\")\n",
    "            \n",
    "    model = Model()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('../input/roberta-base')\n",
    "    \n",
    "    ds = CLRPDataset(df,tokenizer)\n",
    "    dl = DataLoader(ds,\n",
    "                  batch_size = config[\"batch_size\"],\n",
    "                  shuffle=False,\n",
    "                  num_workers = 4,\n",
    "                  pin_memory=True,\n",
    "                  drop_last=False\n",
    "                 )\n",
    "        \n",
    "    embeddings = list()\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in tqdm(enumerate(dl)):\n",
    "            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            embeddings.extend(outputs)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-07T17:24:37.557767Z",
     "iopub.status.busy": "2021-06-07T17:24:37.55726Z",
     "iopub.status.idle": "2021-06-07T17:27:52.081951Z",
     "shell.execute_reply": "2021-06-07T17:27:52.080087Z",
     "shell.execute_reply.started": "2021-06-07T17:24:37.557723Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings1 =  get_embeddings(train_data,'../input/clr-roberta/model0/model0.bin')\n",
    "test_embeddings1 = get_embeddings(test_data,'../input/clr-roberta/model0/model0.bin')\n",
    "\n",
    "train_embeddings2 =  get_embeddings(train_data,'../input/clr-roberta/model1/model1.bin')\n",
    "test_embeddings2 = get_embeddings(test_data,'../input/clr-roberta/model1/model1.bin')\n",
    "\n",
    "train_embeddings3 =  get_embeddings(train_data,'../input/clr-roberta/model2/model2.bin')\n",
    "test_embeddings3 = get_embeddings(test_data,'../input/clr-roberta/model2/model2.bin')\n",
    "\n",
    "train_embeddings4 =  get_embeddings(train_data,'../input/clr-roberta/model3/model3.bin')\n",
    "test_embeddings4 = get_embeddings(test_data,'../input/clr-roberta/model3/model3.bin')\n",
    "\n",
    "train_embeddings5 =  get_embeddings(train_data,'../input/clr-roberta/model4/model4.bin')\n",
    "test_embeddings5 = get_embeddings(test_data,'../input/clr-roberta/model4/model4.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_svm(X, y, X_test, bins=bins, nfolds=5, C=10, kernel='rbf'):\n",
    "    scores = list()\n",
    "    preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    X = pd.concat([X, train_feat], axis='columns')\n",
    "    X_test = pd.concat([X_test, test_feat], axis='columns')    \n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n",
    "    for k, (train_idx,valid_idx) in enumerate(kfold.split(X, bins)):\n",
    "        model = SVR(C=C,kernel=kernel,gamma='auto')\n",
    "        X_train,y_train = X[train_idx], y[train_idx]\n",
    "        X_valid,y_valid = X[valid_idx], y[valid_idx]\n",
    "        \n",
    "        model.fit(X_train,y_train)\n",
    "        prediction = model.predict(X_valid)\n",
    "        score = rmse_score(prediction,y_valid)\n",
    "        print(f'Fold {k} , rmse score: {score}')\n",
    "        scores.append(score)\n",
    "        preds += model.predict(X_test)\n",
    "        \n",
    "    print(\"mean rmse\",np.mean(scores))\n",
    "    return np.array(preds)/nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\n",
    "svm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\n",
    "svm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\n",
    "svm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\n",
    "svm_preds5 = get_preds_svm(train_embeddings5,target,test_embeddings5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.target = svm_preds\n",
    "sample.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTU9KEd-GJKT",
    "papermill": {
     "duration": 0.169598,
     "end_time": "2021-06-12T04:21:52.135361",
     "exception": false,
     "start_time": "2021-06-12T04:21:51.965763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# train & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.828831Z",
     "iopub.status.idle": "2021-06-19T09:31:59.829586Z"
    },
    "id": "qPcFm5qEGJKT",
    "papermill": {
     "duration": 0.175368,
     "end_time": "2021-06-12T04:21:52.481305",
     "exception": false,
     "start_time": "2021-06-12T04:21:52.305937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_cv(X, y, n_splits=5, random_state=0):\n",
    "    folds = KFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
    "    return list(folds.split(X, y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.83105Z",
     "iopub.status.idle": "2021-06-19T09:31:59.831944Z"
    },
    "id": "Dlho26FmGJKT",
    "papermill": {
     "duration": 0.176101,
     "end_time": "2021-06-12T04:21:52.825126",
     "exception": false,
     "start_time": "2021-06-12T04:21:52.649025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "target = 'target'\n",
    "cv = kfold_cv(train_feat, train_target)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.83339Z",
     "iopub.status.idle": "2021-06-19T09:31:59.834168Z"
    },
    "id": "Uid3tvmXGJKT",
    "papermill": {
     "duration": 0.176348,
     "end_time": "2021-06-12T04:21:53.175239",
     "exception": false,
     "start_time": "2021-06-12T04:21:52.998891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metrics': 'rmse',\n",
    "    'seed': SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.835617Z",
     "iopub.status.idle": "2021-06-19T09:31:59.836416Z"
    },
    "id": "HvI1sHBcGJKT",
    "papermill": {
     "duration": 0.173937,
     "end_time": "2021-06-12T04:21:53.517873",
     "exception": false,
     "start_time": "2021-06-12T04:21:53.343936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.837814Z",
     "iopub.status.idle": "2021-06-19T09:31:59.838614Z"
    },
    "id": "7vsazJ9BGJKT"
   },
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import BayesianRidge\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.840247Z",
     "iopub.status.idle": "2021-06-19T09:31:59.841202Z"
    },
    "id": "H29q6JW2GJKU",
    "outputId": "a7670408-748a-4f1e-80b8-d3eb862e3c1f",
    "papermill": {
     "duration": 155.785956,
     "end_time": "2021-06-12T04:24:29.472958",
     "exception": false,
     "start_time": "2021-06-12T04:21:53.687002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "oof_preds = np.zeros(len(train_feat))\n",
    "test_preds = np.zeros(len(test_feat))\n",
    "\n",
    "importances = pd.DataFrame()\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(cv):\n",
    "    print(f'\\nFold {i + 1}')\n",
    "    trn_x, trn_y = train_feat.iloc[train_index], train_target.iloc[train_index]\n",
    "    val_x, val_y = train_feat.iloc[valid_index], train_target.iloc[valid_index]\n",
    "    \n",
    "    model = BayesianRidge(n_iter=300, verbose=True)\n",
    "    model.fit(\n",
    "        X=trn_x,\n",
    "        y=trn_y\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict(val_x)\n",
    "    oof_preds[valid_index] = val_preds\n",
    "    test_preds += model.predict(test_feat) / 5\n",
    "    \n",
    "    val_score = np.sqrt(mean_squared_error(val_y, val_preds))\n",
    "    scores.append(val_score)\n",
    "    models.append(model)\n",
    "    \n",
    "mean_score = np.mean(scores)\n",
    "std_score  = np.std(scores)\n",
    "all_score  = np.sqrt(mean_squared_error(train_target, oof_preds))\n",
    "metrics_name = 'RMSE'\n",
    "print(f'Mean {metrics_name}: {mean_score}, std: {std_score}, All {metrics_name}: {all_score}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.842813Z",
     "iopub.status.idle": "2021-06-19T09:31:59.843675Z"
    },
    "id": "78Z6SPJ7GJKU",
    "papermill": {
     "duration": 0.193544,
     "end_time": "2021-06-12T04:24:29.845584",
     "exception": false,
     "start_time": "2021-06-12T04:24:29.65204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "sample['target'] = test_preds\n",
    "\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    sample.to_csv('submission.csv',index=False)\n",
    "else:\n",
    "    sample.to_csv(OUTPUT_DIR/'submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.845214Z",
     "iopub.status.idle": "2021-06-19T09:31:59.84612Z"
    },
    "id": "98bEBH-OGJKU",
    "outputId": "15ca030f-df12-4ea6-fc7f-6f498326dbb2",
    "papermill": {
     "duration": 0.193637,
     "end_time": "2021-06-12T04:24:30.23022",
     "exception": false,
     "start_time": "2021-06-12T04:24:30.036583",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "print(sample.shape)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-06-19T09:31:59.847579Z",
     "iopub.status.idle": "2021-06-19T09:31:59.848385Z"
    },
    "id": "LNX0XmiTGJKW",
    "papermill": {
     "duration": 0.185561,
     "end_time": "2021-06-12T04:24:37.921192",
     "exception": false,
     "start_time": "2021-06-12T04:24:37.735631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 分布(train_vs_oof)\n",
    "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
    "    pass\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sns.distplot(train_target, label='Train', ax=ax, color='C1')\n",
    "    sns.distplot(oof_preds, label='Out Of Fold', ax=ax, color='C2')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'train_vs_oof.png'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "n6Z7nq6YFak4"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
