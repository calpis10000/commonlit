{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"058-train-02.ipynb","provenance":[{"file_id":"1CEQK5FqKSwMO6MjCXmCCM9_BGeLzsJbO","timestamp":1627819646939},{"file_id":"17BUK8yRF7SDX0khlOXoHcFRTEFffkvRb","timestamp":1627488927996},{"file_id":"1wNpTEKAuuKP7ivTcm1f9j0sdmYU1RyzA","timestamp":1627306793279},{"file_id":"1uE__yBR1oxeYaUIrUTMEOffmeyuJBRAU","timestamp":1627305921964},{"file_id":"1PbEPh6kL5p5cdH5HC8iHoMVCIzA0MqvB","timestamp":1627284576770},{"file_id":"1TlxQ4e-ZX1Zy51dKLuhNdrBWg1qhojqP","timestamp":1627273765934},{"file_id":"17a4F4aC9L0QBqU8BRTrdqPn0WwJ0b08b","timestamp":1626746992716},{"file_id":"1G_W9irFTrEmDeHR0S6_u0bjpk8nxipXW","timestamp":1626689695352},{"file_id":"1bhhkorT--y8XXaVLM8hibVgC-tLqZ16P","timestamp":1626358153868},{"file_id":"1WtT2hX6O9Qbt_hb9sF50nM2QmDXFi-XA","timestamp":1626338366006},{"file_id":"1k_p5wftcUeo711Xho1-T5an2Xkneau-J","timestamp":1626323813472},{"file_id":"1Vz2GB2BNTWuefEFkCSh3TBPEIel7KG1t","timestamp":1626317426487},{"file_id":"1djoMWojeaIPopG5tS1jNMohn8ineblRh","timestamp":1626306831897},{"file_id":"1-6tlDO8158Pi6TpptIF884oFaEiT4Uxb","timestamp":1626276420047},{"file_id":"1js8eA3mDNS8mwSpCiHuzPeARFlUPAVrg","timestamp":1626272452526},{"file_id":"1yhcPgulwJtjJKUK9IuRKmNMhJ-4YXGol","timestamp":1626267205517},{"file_id":"1mnnSv0Pofn1QxArywV81VYqnZPB8uUWN","timestamp":1626180468522},{"file_id":"1RRdjt_UAeHmr5QQBAMyC82Fq1s31OWdK","timestamp":1625833136005},{"file_id":"1JPgg44HFemzwk8VSCXih3PejL0idy-C4","timestamp":1625825483466},{"file_id":"1Ye6wqVX71xAAAhmjXkw9IpRvTqeUyJDA","timestamp":1625812137500}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucCbvGD1XvG7","executionInfo":{"status":"ok","timestamp":1627821348453,"user_tz":-540,"elapsed":359,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"18c0fae5-f751-407d-e0ce-92f9d8cad7e6"},"source":["import sys\n","if 'google.colab' in sys.modules:  # colab特有の処理_2回目以降\n","  # Google Driveのマウント\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  # ライブラリのパス指定\n","  sys.path.append('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FACwJ6icpxrR"},"source":["# データセットをDriveから取得\n","!mkdir -p 'input'\n","!mkdir -p 'clrp-pre-trained'\n","\n","!cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input/commonlitreadabilityprize/' '/content/input'\n","!cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch5/pre-trained-roberta/clrp_roberta_large/' '/content/clrp-pre-trained'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RV9-VwbpZLZ9"},"source":["from pathlib import Path\n","\n","# input\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    DATA_DIR = Path('../input/commonlitreadabilityprize/')\n","\n","elif 'google.colab' in sys.modules: # Colab環境\n","    DATA_DIR = Path('/content/input/commonlitreadabilityprize')\n","\n","else:\n","    DATA_DIR = Path('../00_input/commonlitreadabilityprize/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5difyXe00UV"},"source":["from pathlib import Path\n","\n","# tokenizer\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    TOKENIZER_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    TOKENIZER_DIR = '/content/clrp-pre-trained/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    TOKENIZER_DIR = 'roberta-large'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKjsUxnOeDYl"},"source":["from pathlib import Path\n","\n","# pre-trained model\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    PRE_TRAINED_MODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    PRE_TRAINED_MODEL_DIR = '/content/clrp-pre-trained/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    PRE_TRAINED_MODEL_DIR = 'roberta-large'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLaT2V0ReoAZ"},"source":["UPLOAD_DIR = Path('/content/model')\n","EX_NO = '058-train-02'  # 実験番号などを入れる、folderのpathにする\n","USERID = 'calpis10000'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOGjAb4pAJ0F"},"source":["import subprocess\n","import shlex\n","\n","def gpuinfo():\n","    \"\"\"\n","    Returns size of total GPU RAM and used GPU RAM.\n","\n","    Parameters\n","    ----------\n","    None\n","\n","    Returns\n","    -------\n","    info : dict\n","        Total GPU RAM in integer for key 'total_MiB'.\n","        Used GPU RAM in integer for key 'used_MiB'.\n","    \"\"\"\n","\n","    command = 'nvidia-smi -q -d MEMORY | sed -n \"/FB Memory Usage/,/Free/p\" | sed -e \"1d\" -e \"4d\" -e \"s/ MiB//g\" | cut -d \":\" -f 2 | cut -c2-'\n","    commands = [shlex.split(part) for part in command.split(' | ')]\n","    for i, cmd in enumerate(commands):\n","        if i==0:\n","            res = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n","        else:\n","            res = subprocess.Popen(cmd, stdin=res.stdout, stdout=subprocess.PIPE)\n","    total, used = map(int, res.communicate()[0].decode('utf-8').strip().split('\\n'))\n","    info = {'total_MiB':total, 'used_MiB':used}\n","    return info\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g3-6m5MKXecB"},"source":["# Overview\n","This nb is based on copy from https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch .\n","\n","Acknowledgments(from base nb): \n","some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T06:26:32.834365Z","iopub.execute_input":"2021-07-04T06:26:32.834903Z","iopub.status.idle":"2021-07-04T06:26:40.143740Z","shell.execute_reply.started":"2021-07-04T06:26:32.834785Z","shell.execute_reply":"2021-07-04T06:26:40.142864Z"},"trusted":true,"id":"HRsRZ06WXecD"},"source":["import os\n","import math\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from transformers import AdamW # optimizer\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoConfig\n","from transformers import get_cosine_schedule_with_warmup # scheduler\n","from pytorch_memlab import profile\n","import pytorch_memlab\n","from pytorch_memlab import MemReporter\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gc\n","gc.enable()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.145217Z","iopub.execute_input":"2021-07-04T06:26:40.145539Z","iopub.status.idle":"2021-07-04T06:26:40.201326Z","shell.execute_reply.started":"2021-07-04T06:26:40.145504Z","shell.execute_reply":"2021-07-04T06:26:40.200136Z"},"trusted":true,"id":"omBfwshTXecE"},"source":["NUM_FOLDS = 5 # K Fold\n","NUM_EPOCHS = 5 # Epochs\n","BATCH_SIZE = 12 # Batch Size\n","MAX_LEN = 248 # ベクトル長\n","EVAL_SCHEDULE = [(0.55, 64), (-1., 32)] # schedulerの何らかの設定？\n","ROBERTA_PATH = PRE_TRAINED_MODEL_DIR # roberta pre-trainedモデル(モデルとして指定)\n","TOKENIZER_PATH = TOKENIZER_DIR # roberta pre-trainedモデル(Tokenizerとして指定)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # cudaがなければcpuを使えばいいじゃない"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.203398Z","iopub.execute_input":"2021-07-04T06:26:40.204055Z","iopub.status.idle":"2021-07-04T06:26:40.211572Z","shell.execute_reply.started":"2021-07-04T06:26:40.204015Z","shell.execute_reply":"2021-07-04T06:26:40.210762Z"},"trusted":true,"id":"4qcuXqwtXecF"},"source":["def set_random_seed(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n","\n","    torch.manual_seed(random_seed)\n","    torch.cuda.manual_seed(random_seed)\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","    torch.backends.cudnn.deterministic = True# cudnnによる最適化で結果が変わらないためのおまじない "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.214188Z","iopub.execute_input":"2021-07-04T06:26:40.214809Z","iopub.status.idle":"2021-07-04T06:26:40.309744Z","shell.execute_reply.started":"2021-07-04T06:26:40.214769Z","shell.execute_reply":"2021-07-04T06:26:40.308926Z"},"trusted":true,"id":"70PyLsJTXecF"},"source":["# read train_df(kfold)\n","train_kf_df = pd.read_csv(DATA_DIR/\"train_kfold.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.311021Z","iopub.execute_input":"2021-07-04T06:26:40.311347Z","iopub.status.idle":"2021-07-04T06:26:40.624393Z","shell.execute_reply.started":"2021-07-04T06:26:40.311314Z","shell.execute_reply":"2021-07-04T06:26:40.623347Z"},"trusted":true,"id":"xf0662k4XecF"},"source":["# tokenizerを指定\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6aaghNkXecG"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UU5uZKIcDjkV","executionInfo":{"status":"ok","timestamp":1627821358884,"user_tz":-540,"elapsed":546,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"cf357ec5-8677-4f50-9747-3c3586eb013d"},"source":["# 前処理用\n","import string\n","import re\n","import collections\n","\n","# ローカルの場合、stopwordsをダウンロード\n","import nltk\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    pass\n","else:\n","    import nltk\n","    nltk.download('stopwords')\n","    nltk.download('averaged_perceptron_tagger')\n","    os.listdir(os.path.expanduser('~/nltk_data/corpora/stopwords/'))\n","\n","# テキスト前処理\n","# https://www.kaggle.com/alaasedeeq/commonlit-readability-eda\n","\n","#filtering the unwanted symbols, spaces, ....etc\n","to_replace_by_space = re.compile('[/(){}\\[\\]|@,;]')\n","punctuation = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n","bad_symbols = re.compile('[^0-9a-z #+_]')\n","stopwords = set(nltk.corpus.stopwords.words('english'))\n","\n","def text_prepare(text):\n","    '''\n","    text: a string\n","    returna modified version of the string\n","    '''\n","    text = text.lower() # lowercase text\n","    text = re.sub(punctuation, '',text)\n","    text = re.sub(to_replace_by_space, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n","    text = re.sub(bad_symbols, \"\", text)         # delete symbols which are in BAD_SYMBOLS_RE from text\n","    text = \" \".join([word for word in text.split(\" \") if word not in stopwords]) # delete stopwords from text\n","    text = re.sub(' +', ' ', text)\n","    return text\n","\n","def text_normalization(s:pd.Series):\n","    x = s.apply(text_prepare)\n","    return x\n","\n","# Counterオブジェクトを取得\n","def get_counter(text:str):\n","    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n","    counter = collections.Counter(text_list)\n","    return counter\n","\n","# ベースとなる継承元のクラス\n","class BaseBlock(object):\n","    def fit(self, input_df, y=None):\n","        return self.transform(input_df)\n","    def transform(self, input_df):\n","        raise NotImplementedError()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ze-E2aCfgsfj"},"source":["class TextDescriptionBlock(BaseBlock):\n","    \"\"\"テキストに関する統計量を返す block\"\"\"\n","    def __init__(self, column: str):\n","        \"\"\"\n","        args:\n","            column: str\n","                変換対象のカラム名\n","        \"\"\"\n","        self.column = column\n","        self.param_prefix = f'col={column}'\n","\n","    # 前処理\n","    def preprocess(self, input_df):\n","        x = text_normalization(input_df[self.column])\n","        return x\n","        \n","    def fit(self, input_df, y=None):\n","        return self.transform(input_df)\n","\n","    def transform(self, input_df):\n","        # 前処理\n","        self.text = self.preprocess(input_df)\n","        self.counters = self.text.map(get_counter)\n","\n","        # 変換処理\n","        _length = input_df[self.column].fillna('').map(lambda x: len(x) if x!='' else np.nan)\n","        _wrd_cnt = self.counters.map(lambda x: sum(x.values()))\n","        _wrd_nuniq = self.counters.map(lambda x: len(x))\n","        _wrd_mean = self.counters.map(lambda x: np.mean(list(x.values())))\n","        _wrd_max = self.counters.map(lambda x: np.max(list(x.values())))\n","        \n","        word_length = self.counters.map(lambda x: np.array([len(i) for i in x.keys()]))\n","        word_length_desc = word_length.map(lambda x: pd.Series(x.ravel()).describe())\n","        _word_length_desc_df = pd.DataFrame(word_length_desc.tolist()).iloc[:,1:]\n","        _word_length_desc_df = _word_length_desc_df.add_prefix('word_length_')\n","        \n","        out_df = pd.concat([_length, _wrd_cnt, _wrd_nuniq, _wrd_mean, _wrd_max], axis=1)\n","        out_df = out_df.reset_index().drop('index', axis='columns')\n","        out_df.columns = ['text_length', 'word_count', 'word_nunique', 'word_appearance_mean', 'word_appearance_max']\n","        out_df = pd.concat([out_df, _word_length_desc_df], axis=1).fillna(-1)\n","        return out_df.add_suffix(f'_{self.column}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6lObnzFnYyK"},"source":["class SentenceDescriptionBlock(BaseBlock):\n","    \"\"\"テキスト(センテンス)に関する統計量を返す block\"\"\"\n","    def __init__(self, column: str):\n","        \"\"\"\n","        args:\n","            column: str\n","                変換対象のカラム名\n","        \"\"\"\n","        self.column = column\n","        self.param_prefix = f'col={column}'\n","\n","    # 前処理\n","    def get_delimiter_feats(self, sentences: list):\n","      deli_df = pd.DataFrame([{'comma': i.count(','),\n","                                'colon': i.count(':'),\n","                                'semi-colon': i.count(';'),\n","                                'dash': i.count('—'),\n","                                'total': i.count(',') + i.count(':') + i.count(';') + i.count('—')\n","                                } for i in sentences])\n","      deli_disc = deli_df.describe().drop('count', axis='index')\n","      deli_disc.loc['sum', :] = deli_df.sum()\n","      deli_values = deli_disc.T.values.reshape(-1)\n","      col_list = []\n","      for col_ in deli_disc.columns:\n","        for idx_ in deli_disc.index:\n","          col_list.append(f'sentences_delim_{col_}_{idx_}')\n","      return pd.Series(data=deli_values, index=col_list)\n","\n","    def transform(self, input_df):\n","        self.text = input_df.reset_index().drop('index', axis='columns')[self.column]\n","        self.sentences = self.text.map(lambda x: [i for i in x.split('.') if len(i) > 0])\n","        out_df = pd.DataFrame()\n","\n","        # センテンス数\n","        out_df['sentences_cnt'] = self.sentences.map(lambda x: len(x)) \n","\n","        # センテンス長 × 各種統計量\n","        sentences_len = self.sentences.map(lambda x: [len(i) for i in x])\n","        sentences_len_df = pd.DataFrame(\n","                                        sentences_len.map(\n","                                            lambda x: pd.Series(x).describe().drop('count')\n","                                            ).tolist()\n","                                        ).add_prefix('sentence_length_')\n","\n","        # センテンスの単語数(クリーニング)\n","        sentences_clean = self.sentences.map(lambda x: [text_prepare(i) for i in x])\n","        sentences_wrd_counter = sentences_clean.map(lambda x: [get_counter(i) for i in x])\n","\n","        sentences_wrd_cnt_df = pd.DataFrame( # 単語数\n","                                        sentences_wrd_counter.map(\n","                                            lambda x: pd.Series([sum(i.values()) for i in x]).describe().drop('count')\n","                                            ).tolist()\n","                                        ).add_prefix('sentence_word_cnt_') \n","\n","        sentences_wrd_uniq_df = pd.DataFrame( # 単語のユニーク数\n","                                        sentences_wrd_counter.map(\n","                                            lambda x: pd.Series([len(i) for i in x]).describe().drop('count')\n","                                            ).tolist()\n","                                        ).add_prefix('sentence_word_uniq_')\n","\n","        # 区切り文字の数を取得\n","        sentences_deli_df = self.sentences.apply(self.get_delimiter_feats)       \n","        \n","        out_df = pd.concat([out_df, \n","                            sentences_len_df, \n","                            sentences_wrd_cnt_df, \n","                            sentences_wrd_uniq_df, \n","                            sentences_deli_df], axis=1)\n","        \n","        out_df = out_df.reset_index().drop('index', axis='columns')\n","        return out_df.add_suffix(f'_{self.column}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAse0IDWDjho"},"source":["# Dataset用のClass。\n","class LitDataset(Dataset):\n","    def __init__(self, df, inference_only=False):\n","        super().__init__()\n","\n","        self.df = df        \n","        self.inference_only = inference_only # Testデータ用フラグ\n","        self.text = df.excerpt.tolist() # 分析対象カラムをlistにする。(分かち書きではなく、Seriesをlistへ変換するような処理)\n","        #self.text = [text.replace(\"\\n\", \" \") for text in self.text] # 単語単位で分かち書きする場合\n","        #self.text_len = text_normalization(df.excerpt).map(lambda x: [0 if i >= len(x.split(' ')) else len(x.split(' ')[i]) for i in range(132)])\n","        desc_block = TextDescriptionBlock('excerpt')\n","        sent_block = SentenceDescriptionBlock('excerpt')\n","        text_desc = desc_block.fit(self.df)\n","        text_sent = sent_block.fit(self.df)\n","        self.text_feats = pd.concat([text_desc, text_sent], axis='columns')\n","\n","        if not self.inference_only:\n","            self.target = torch.tensor(df.target.values, dtype=torch.float32) # trainのみ、targetをtensorに変換\n","            self.standard_error = torch.tensor(df.standard_error.values, dtype=torch.float32) \n","\n","        self.encoded = tokenizer.batch_encode_plus( # textをtokenize\n","            self.text,\n","            padding = 'max_length',            \n","            max_length = MAX_LEN,\n","            truncation = True, # 最大長を超える文字は切り捨て\n","            return_attention_mask=True\n","        )        \n"," \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, index): # 変換結果を返す\n","        input_ids = torch.tensor(self.encoded['input_ids'][index])\n","        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n","        #input_len = torch.tensor(self.text_len.iloc[index], dtype=torch.float32)\n","        input_desc = torch.tensor(self.text_feats.values[index], dtype=torch.float32)\n","\n","        if self.inference_only:\n","            return (input_ids, attention_mask, input_desc)            \n","        else:\n","            target = self.target[index]\n","            standard_error = self.standard_error[index]\n","            return (input_ids, attention_mask, input_desc, target, standard_error)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmEiwD6Dqk4Q"},"source":["# 検証\n","#litds = LitDataset(train_kf_df.loc[train_indices])\n","#litds[0][2].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhjZwnZNoU1i"},"source":["# 検証\n","#litds = LitDataset(train_kf_df)\n","#litds[0][2].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKtdy32wXecG"},"source":["# Model\n","The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.649629Z","iopub.execute_input":"2021-07-04T06:26:40.650066Z","iopub.status.idle":"2021-07-04T06:26:40.666374Z","shell.execute_reply.started":"2021-07-04T06:26:40.650002Z","shell.execute_reply":"2021-07-04T06:26:40.665211Z"},"trusted":true,"id":"BpkxjXEUXecH"},"source":["class LitModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(ROBERTA_PATH) # pretrainedからconfigを読み込み\n","        config.update({\"output_hidden_states\":True, # config更新: embedding層を抽出\n","                       \"hidden_dropout_prob\": 0.0, # config更新: dropoutしない\n","                       \"layer_norm_eps\": 1e-7}) # config更新: layer normalizationのepsilon                      \n","        \n","        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)\n","            \n","        self.attention = nn.Sequential(# attentionレイヤー            \n","            nn.Linear(config.hidden_size, 512),      \n","            nn.Tanh(),                       \n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","\n","        self.numeric_feats = nn.Sequential(\n","            nn.Linear(74, 74),\n","            nn.BatchNorm1d(74),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(74, 74),\n","            nn.BatchNorm1d(74),\n","            nn.ReLU(),\n","            nn.Dropout(0.2)\n","        )\n","\n","        self.regressor = nn.Sequential( # target、stderror                  \n","            nn.Linear(config.hidden_size + 74, 2),\n","        )\n","\n","        #self.bin_class = nn.Sequential( # target_sign\n","        #    nn.Linear(config.hidden_size + 64, 1),\n","        #    nn.Dropout(p=0.2),\n","        #    nn.Sigmoid()                       \n","        #)\n","\n","\n","    def forward(self, input_ids, attention_mask, input_desc):\n","        roberta_output = self.roberta(input_ids=input_ids, # robertaに入力データを流し、出力としてrobertaモデル(layerの複合体)を得る\n","                                      attention_mask=attention_mask)     \n","        # attention_pooling\n","        last_hidden_state = roberta_output.hidden_states[-1] # robertaモデルの最後のlayerを得る\n","        weights = self.attention(last_hidden_state) # robertaの最後のlayerをattentionへ入力し、出力として重みを得る                \n","        context_vector = torch.sum(weights * last_hidden_state, dim=1) # 重み×最後の層を足し合わせて文書ベクトルとする。\n","        # word_length_conv1d\n","        #input_chnl = input_len.unsqueeze(1)\n","        #conv1_layers = self.conv1_layers(input_chnl)\n","        #conv1_layers_v = conv1_layers.view(conv1_layers.size(0),-1)\n","\n","        # numeric_feats\n","        numeric_feats = self.numeric_feats(input_desc)\n","\n","        # https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently\n","        # last_hidden_state = roberta_output[0]\n","        # input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        # sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        # sum_mask = input_mask_expanded.sum(1)\n","        # sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        # mean_embeddings = sum_embeddings / sum_mask\n","        cat_layers = torch.cat([context_vector, numeric_feats], dim=1)\n","        return self.regressor(cat_layers)\n","        \n","        # Now we reduce the context vector to the prediction score.\n","        #return self.regressor(mean_embeddings) # 文書ベクトルを線形層に入力し、targetを出力する"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.672515Z","iopub.execute_input":"2021-07-04T06:26:40.672944Z","iopub.status.idle":"2021-07-04T06:26:40.684593Z","shell.execute_reply.started":"2021-07-04T06:26:40.672908Z","shell.execute_reply":"2021-07-04T06:26:40.683569Z"},"trusted":true,"id":"bB4jvQTxXecH"},"source":["def eval_mse(model, data_loader):\n","    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモードを選択。Batch Normとかdropoutをしなくなる           \n","    mse_mean_sum = 0\n","    mse_std_sum = 0\n","\n","    with torch.no_grad(): # 勾配の計算をしない(予測のみ行う)\n","        for batch_num, (input_ids, attention_mask, input_feats, target, standard_error) in enumerate(data_loader): # data_loaderからinput, attentin_mask, targetをbatchごとに取り出す\n","            input_ids = input_ids.to(DEVICE)   \n","            attention_mask = attention_mask.to(DEVICE)  \n","            input_feats = input_feats.to(DEVICE) \n","            target = target.to(DEVICE)\n","            standard_error = standard_error.to(DEVICE)\n","            \n","            output = model(input_ids, attention_mask, input_feats) # 取得した値をモデルへ入力し、出力として予測値を得る。\n","\n","            mse_mean_sum += nn.MSELoss(reduction=\"sum\")(output[:,0].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","            mse_std_sum += nn.MSELoss(reduction=\"sum\")(output[:,1].flatten(), standard_error).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","\n","\n","    del input_ids\n","    del attention_mask\n","\n","    mse_mean_result = mse_mean_sum / len(data_loader.dataset)\n","    mse_std_result = mse_std_sum / len(data_loader.dataset)\n","\n","    return mse_mean_result, mse_std_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.690155Z","iopub.execute_input":"2021-07-04T06:26:40.692530Z","iopub.status.idle":"2021-07-04T06:26:40.703425Z","shell.execute_reply.started":"2021-07-04T06:26:40.692488Z","shell.execute_reply":"2021-07-04T06:26:40.702366Z"},"trusted":true,"id":"47bDno_LXecI"},"source":["# 推論結果を返す\n","def predict(model, data_loader):\n","    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモード(dropout, batch_normしない)\n","\n","    result = np.zeros(len(data_loader.dataset)) # 結果をdataset長のzero配列として用意\n","    index = 0\n","    \n","    with torch.no_grad(): # 勾配の計算をしないblock(inputすると、現状の重みによる推論結果を返す)\n","        for batch_num, (input_ids, attention_mask, input_feats) in enumerate(data_loader): # data_loaderからbatchごとにinputを得る\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)\n","            input_feats = input_feats.to(DEVICE)\n","                        \n","            output = model(input_ids, attention_mask, input_feats) # modelにinputを入力し、予測結果を得る。\n","            output_target = output[:,0]\n","\n","            result[index : index + output_target.shape[0]] = output_target.flatten().to(\"cpu\") # result[index ~ predの長さ]へ、予測結果を格納\n","            index += output_target.shape[0] # indexを更新\n","\n","    return result # 全batchで推論が終わったら、結果を返す"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.708605Z","iopub.execute_input":"2021-07-04T06:26:40.709024Z","iopub.status.idle":"2021-07-04T06:26:40.730675Z","shell.execute_reply.started":"2021-07-04T06:26:40.708983Z","shell.execute_reply":"2021-07-04T06:26:40.729705Z"},"trusted":true,"id":"oInneuAmXecI"},"source":["# 学習\n","def train(model, # モデル\n","          model_path, # モデルのアウトプット先\n","          train_loader, # train-setのdata_loader\n","          val_loader, # valid-setのdata_loader\n","          optimizer, # optimizer\n","          scheduler=None, # scheduler, デフォルトはNone\n","          num_epochs=NUM_EPOCHS # epoch数、notebook冒頭で指定した値\n","         ):    \n","    \n","    best_val_rmse = None\n","    best_val_sign_bce = None\n","    best_epoch = 0\n","    step = 0\n","    last_eval_step = 0\n","    eval_period = EVAL_SCHEDULE[0][1] # eval期間(って何？) 冒頭で決めたEVAL_SCHEDULEの最初のtupleの[1]を取得\n","\n","    start = time.time() # 時間計測用\n","\n","    for epoch in range(num_epochs): # 指定したEpoch数だけ繰り返し\n","        val_rmse = None         \n","\n","        for batch_num, (input_ids, attention_mask, input_feats, target, standard_error) in enumerate(train_loader): # train_loaderからinput, targetを取得\n","            input_ids = input_ids.to(DEVICE) # inputをDEVICEへ突っ込む\n","            attention_mask = attention_mask.to(DEVICE)   \n","            input_feats = input_feats.to(DEVICE)\n","            target = target.to(DEVICE)\n","            standard_error = standard_error.to(DEVICE)\n","\n","            optimizer.zero_grad() # 勾配を初期化            \n","            model.train() # 学習モード開始\n","\n","            # https://www.kaggle.com/c/commonlitreadabilityprize/discussion/239421\n","            output = model(input_ids, attention_mask, input_feats) # input,attention_maskを入力し、予測結果を得る\n","            p = torch.distributions.Normal(output[:,0], torch.sqrt(output[:,1]**2))\n","            q = torch.distributions.Normal(target, standard_error)\n","            kl_vector = torch.distributions.kl_divergence(p, q)\n","\n","            loss = kl_vector.mean()\n","\n","            loss.backward() \n","            optimizer.step() # 重みを更新する\n","\n","            if scheduler:\n","                scheduler.step() # schedulerが与えられた場合は、schedulerの学習率更新\n","            \n","            if step >= last_eval_step + eval_period: # batchを回すごとにstepを増やしていって、「前回evalしたstep + eval_period(16)」を超えたら実行。\n","                print(gpuinfo())\n","                # Evaluate the model on val_loader.\n","                elapsed_seconds = time.time() - start # 経過時間\n","                num_steps = step - last_eval_step # 経過ステップ数\n","                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n","                last_eval_step = step # 前回stepの更新\n","                \n","                # valid-setによるrmse計算\n","                train_kldiv = loss\n","                \n","                val_mse_mean, val_mse_std = eval_mse(model, val_loader)\n","                val_rmse_mean = math.sqrt(val_mse_mean)                            \n","                val_rmse_std = math.sqrt(val_mse_std)                            \n","\n","                print(f\"Epoch: {epoch} batch_num: {batch_num}\")\n","                print(f\"train_kldiv: {train_kldiv:0.4}\"\n","                      )\n","                print(f\"val_rmse_mean: {val_rmse_mean:0.4}\",\n","                      f\"val_rmse_std: {val_rmse_std:0.4}\"\n","                      )\n","\n","                for rmse, period in EVAL_SCHEDULE: # eval_periodをvalid-rmseで切り替える処理\n","                    if val_rmse_mean >= rmse: # valid rmseをEVAL_SCHEDULEと比較し、0項 > valid rmseとなるまで回す : EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n","                        eval_period = period # eval_periodを更新\n","                        break                               \n","\n","                if not best_val_rmse or val_rmse_mean < best_val_rmse: # 初回(best_val_rmse==None), またはbest_val_rmseを更新したらモデルを保存する\n","                    best_val_rmse = val_rmse_mean\n","                    best_epoch = epoch\n","                    torch.save(model.state_dict(), model_path) # 最高の自分を保存\n","                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n","                else:       \n","                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\", # 更新されない場合は、元のスコアを表示\n","                          f\"(from epoch {best_epoch})\")      \n","\n","                start = time.time()\n","            \n","            # batchごとにメモリ解放\n","            del input_ids\n","            del attention_mask\n","            torch.cuda.empty_cache()                                            \n","            step += 1\n","    \n","    return best_val_rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.735798Z","iopub.execute_input":"2021-07-04T06:26:40.738398Z","iopub.status.idle":"2021-07-04T06:26:40.750876Z","shell.execute_reply.started":"2021-07-04T06:26:40.738356Z","shell.execute_reply":"2021-07-04T06:26:40.749635Z"},"trusted":true,"id":"rMY0fjXwXecJ"},"source":["# optimizerの作成\n","def create_optimizer(model):\n","    parameters = []\n","\n","    named_parameters = list(model.named_parameters()) # モデルパラメータの取得\n","    roberta_parameters = list(model.roberta.named_parameters())[:-2] # パラメータをroberta用、attention用、regressor用に格納。(直接引っ張ってくる形式に変更)\n","\n","    attention_parameters = list(model.attention.named_parameters())\n","    attention_group = [{'params': params, 'lr': 2e-5} for (name, params) in attention_parameters] # attention用パラメータをリストとして取得\n","    parameters += attention_group\n","\n","    regressor_parameters = list(model.regressor.named_parameters())\n","    regressor_group = [{'params': params, 'lr': 2e-5} for (name, params) in regressor_parameters] # reg用パラメータをリストとして取得\n","    parameters += regressor_group\n","\n","    numeric_feats_parameters = list(model.numeric_feats.named_parameters())\n","    numeric_feats_group = [{'params': params, 'lr': 2e-5} for (name, params) in numeric_feats_parameters] # reg用パラメータをリストとして取得\n","    parameters += numeric_feats_group\n","\n","    for layer_num, (name, params) in enumerate(roberta_parameters): # レイヤーごとにname, paramsを取得していろんな処理\n","        weight_decay = 0.0 if \"bias\" in name else 0.01\n","\n","        lr = 8e-6\n","\n","        if layer_num >= 69:        \n","            lr = 2e-5\n","\n","        if layer_num >= 133:\n","            lr = 4e-5\n","\n","        parameters.append({\"params\": params,\n","                           \"weight_decay\": weight_decay,\n","                           \"lr\": lr})\n","\n","    return AdamW(parameters) # 最終的に、AdamWにパラメータを入力する。\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PLKHwvKtNBn"},"source":["def train_and_save_model(train_indices, val_indices, model_path):\n","    train_dataset = LitDataset(train_kf_df.loc[train_indices]) # train, validのDataset\n","    val_dataset = LitDataset(train_kf_df.loc[val_indices])\n","        \n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              drop_last=True, shuffle=True, num_workers=2) # train, validのDataLoader\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                            drop_last=False, shuffle=False, num_workers=2)    \n","\n","    model = LitModel().to(DEVICE) # modelをDEVICEへぶち込む\n","    optimizer = create_optimizer(model) # optimizerをモデルから作成\n","    scheduler = get_cosine_schedule_with_warmup( # schedulerを作成\n","        optimizer,\n","        num_training_steps=NUM_EPOCHS * len(train_loader),\n","        num_warmup_steps=50)    \n","    rmse = train(model, model_path, train_loader, val_loader, optimizer, scheduler=scheduler)\n","\n","    del train_dataset\n","    del val_dataset\n","    del train_loader\n","    del val_loader\n","    del model\n","    del optimizer\n","    del scheduler\n","    gc.collect() \n","    torch.cuda.empty_cache()\n","    return rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.755813Z","iopub.execute_input":"2021-07-04T06:26:40.758373Z","iopub.status.idle":"2021-07-04T06:27:12.493221Z","shell.execute_reply.started":"2021-07-04T06:26:40.758265Z","shell.execute_reply":"2021-07-04T06:27:12.490139Z"},"trusted":true,"id":"k2LGJD3XXecK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627829982101,"user_tz":-540,"elapsed":8622732,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"1644768f-3483-4c21-c775-9ee038b1aeaa"},"source":["# 実行処理。 KFold & 学習\n","SEED = 1000\n","list_val_rmse = []\n","\n","for fold in sorted(train_kf_df['kfold'].unique()):\n","    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n","    print(gpuinfo())\n","    model_path = f\"model_{fold + 1}.pth\" # model_fold数_.pth\n","    set_random_seed(SEED + fold) # SEEDはfold別に変わるようにする\n","\n","    train_indices = (train_kf_df['kfold'] != fold)\n","    val_indices = (train_kf_df['kfold'] == fold)\n","    list_val_rmse.append(train_and_save_model(train_indices, val_indices, model_path))\n","    print(\"\\nPerformance estimates:\")\n","    print(list_val_rmse)\n","    print(\"Mean:\", np.array(list_val_rmse).mean())\n","    print(gpuinfo())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Fold 1/5\n","{'total_MiB': 16280, 'used_MiB': 2}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 83.7 seconds\n","Epoch: 0 batch_num: 64\n","train_kldiv: 0.5963\n","val_rmse_mean: 0.7148 val_rmse_std: 1.004\n","New best_val_rmse: 0.7148\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.2 seconds\n","Epoch: 0 batch_num: 128\n","train_kldiv: 0.5839\n","val_rmse_mean: 0.661 val_rmse_std: 1.01\n","New best_val_rmse: 0.661\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.4 seconds\n","Epoch: 1 batch_num: 4\n","train_kldiv: 0.4006\n","val_rmse_mean: 0.5594 val_rmse_std: 1.02\n","New best_val_rmse: 0.5594\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.3 seconds\n","Epoch: 1 batch_num: 68\n","train_kldiv: 0.4175\n","val_rmse_mean: 0.5601 val_rmse_std: 0.9534\n","Still best_val_rmse: 0.5594 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.2 seconds\n","Epoch: 1 batch_num: 132\n","train_kldiv: 0.3077\n","val_rmse_mean: 0.5178 val_rmse_std: 0.9932\n","New best_val_rmse: 0.5178\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 1 batch_num: 164\n","train_kldiv: 0.6328\n","val_rmse_mean: 0.5815 val_rmse_std: 0.9949\n","Still best_val_rmse: 0.5178 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.4 seconds\n","Epoch: 2 batch_num: 40\n","train_kldiv: 0.1318\n","val_rmse_mean: 0.5143 val_rmse_std: 0.9991\n","New best_val_rmse: 0.5143\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 2 batch_num: 72\n","train_kldiv: 0.1515\n","val_rmse_mean: 0.513 val_rmse_std: 0.9958\n","New best_val_rmse: 0.513\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 104\n","train_kldiv: 0.1363\n","val_rmse_mean: 0.5198 val_rmse_std: 0.9853\n","Still best_val_rmse: 0.513 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 136\n","train_kldiv: 0.2465\n","val_rmse_mean: 0.5045 val_rmse_std: 0.9903\n","New best_val_rmse: 0.5045\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 168\n","train_kldiv: 0.199\n","val_rmse_mean: 0.4874 val_rmse_std: 0.9962\n","New best_val_rmse: 0.4874\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.6 seconds\n","Epoch: 3 batch_num: 12\n","train_kldiv: 0.05238\n","val_rmse_mean: 0.4919 val_rmse_std: 1.0\n","Still best_val_rmse: 0.4874 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 3 batch_num: 44\n","train_kldiv: 0.1474\n","val_rmse_mean: 0.5057 val_rmse_std: 0.9775\n","Still best_val_rmse: 0.4874 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 3 batch_num: 76\n","train_kldiv: 0.123\n","val_rmse_mean: 0.4959 val_rmse_std: 0.9978\n","Still best_val_rmse: 0.4874 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 3 batch_num: 108\n","train_kldiv: 0.07083\n","val_rmse_mean: 0.4984 val_rmse_std: 0.9784\n","Still best_val_rmse: 0.4874 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 3 batch_num: 140\n","train_kldiv: 0.1876\n","val_rmse_mean: 0.4943 val_rmse_std: 0.9705\n","Still best_val_rmse: 0.4874 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 3 batch_num: 172\n","train_kldiv: 0.09354\n","val_rmse_mean: 0.4873 val_rmse_std: 0.9674\n","New best_val_rmse: 0.4873\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.6 seconds\n","Epoch: 4 batch_num: 16\n","train_kldiv: 0.05018\n","val_rmse_mean: 0.4879 val_rmse_std: 0.9841\n","Still best_val_rmse: 0.4873 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 48\n","train_kldiv: 0.07286\n","val_rmse_mean: 0.4852 val_rmse_std: 0.9908\n","New best_val_rmse: 0.4852\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 80\n","train_kldiv: 0.06349\n","val_rmse_mean: 0.4872 val_rmse_std: 0.9883\n","Still best_val_rmse: 0.4852 (from epoch 4)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 112\n","train_kldiv: 0.08151\n","val_rmse_mean: 0.488 val_rmse_std: 0.9761\n","Still best_val_rmse: 0.4852 (from epoch 4)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 144\n","train_kldiv: 0.06127\n","val_rmse_mean: 0.489 val_rmse_std: 0.9804\n","Still best_val_rmse: 0.4852 (from epoch 4)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 176\n","train_kldiv: 0.05596\n","val_rmse_mean: 0.4892 val_rmse_std: 0.9789\n","Still best_val_rmse: 0.4852 (from epoch 4)\n","\n","Performance estimates:\n","[0.48516161186118684]\n","Mean: 0.48516161186118684\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 2/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 83.0 seconds\n","Epoch: 0 batch_num: 64\n","train_kldiv: 1.006\n","val_rmse_mean: 0.6825 val_rmse_std: 0.1145\n","New best_val_rmse: 0.6825\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.1 seconds\n","Epoch: 0 batch_num: 128\n","train_kldiv: 0.4322\n","val_rmse_mean: 0.5805 val_rmse_std: 0.1052\n","New best_val_rmse: 0.5805\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.2 seconds\n","Epoch: 1 batch_num: 4\n","train_kldiv: 0.5034\n","val_rmse_mean: 0.5493 val_rmse_std: 0.06945\n","New best_val_rmse: 0.5493\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 1 batch_num: 36\n","train_kldiv: 0.5642\n","val_rmse_mean: 0.5774 val_rmse_std: 0.06919\n","Still best_val_rmse: 0.5493 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.0 seconds\n","Epoch: 1 batch_num: 100\n","train_kldiv: 0.8354\n","val_rmse_mean: 0.5198 val_rmse_std: 0.1033\n","New best_val_rmse: 0.5198\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 1 batch_num: 132\n","train_kldiv: 0.8586\n","val_rmse_mean: 0.5014 val_rmse_std: 0.07313\n","New best_val_rmse: 0.5014\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 1 batch_num: 164\n","train_kldiv: 1.372\n","val_rmse_mean: 0.5478 val_rmse_std: 0.08054\n","Still best_val_rmse: 0.5014 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.5 seconds\n","Epoch: 2 batch_num: 8\n","train_kldiv: 0.2894\n","val_rmse_mean: 0.4939 val_rmse_std: 0.06308\n","New best_val_rmse: 0.4939\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 2 batch_num: 40\n","train_kldiv: 0.3597\n","val_rmse_mean: 0.5003 val_rmse_std: 0.06216\n","Still best_val_rmse: 0.4939 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 2 batch_num: 72\n","train_kldiv: 0.3123\n","val_rmse_mean: 0.4858 val_rmse_std: 0.07704\n","New best_val_rmse: 0.4858\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 2 batch_num: 104\n","train_kldiv: 0.3705\n","val_rmse_mean: 0.5007 val_rmse_std: 0.06899\n","Still best_val_rmse: 0.4858 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 2 batch_num: 136\n","train_kldiv: 0.1101\n","val_rmse_mean: 0.4845 val_rmse_std: 0.05844\n","New best_val_rmse: 0.4845\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 2 batch_num: 168\n","train_kldiv: 0.254\n","val_rmse_mean: 0.4879 val_rmse_std: 0.0625\n","Still best_val_rmse: 0.4845 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 3 batch_num: 12\n","train_kldiv: 0.1695\n","val_rmse_mean: 0.502 val_rmse_std: 0.06575\n","Still best_val_rmse: 0.4845 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 3 batch_num: 44\n","train_kldiv: 0.1483\n","val_rmse_mean: 0.4721 val_rmse_std: 0.06398\n","New best_val_rmse: 0.4721\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 3 batch_num: 76\n","train_kldiv: 0.3472\n","val_rmse_mean: 0.4731 val_rmse_std: 0.06697\n","Still best_val_rmse: 0.4721 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.1 seconds\n","Epoch: 3 batch_num: 108\n","train_kldiv: 0.2426\n","val_rmse_mean: 0.4716 val_rmse_std: 0.06453\n","New best_val_rmse: 0.4716\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.1 seconds\n","Epoch: 3 batch_num: 140\n","train_kldiv: 0.104\n","val_rmse_mean: 0.4788 val_rmse_std: 0.06328\n","Still best_val_rmse: 0.4716 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.1 seconds\n","Epoch: 3 batch_num: 172\n","train_kldiv: 0.1992\n","val_rmse_mean: 0.4714 val_rmse_std: 0.06341\n","New best_val_rmse: 0.4714\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 16\n","train_kldiv: 0.0961\n","val_rmse_mean: 0.4724 val_rmse_std: 0.06227\n","Still best_val_rmse: 0.4714 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.1 seconds\n","Epoch: 4 batch_num: 48\n","train_kldiv: 0.127\n","val_rmse_mean: 0.4724 val_rmse_std: 0.06145\n","Still best_val_rmse: 0.4714 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 4 batch_num: 80\n","train_kldiv: 0.1055\n","val_rmse_mean: 0.4716 val_rmse_std: 0.05934\n","Still best_val_rmse: 0.4714 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.1 seconds\n","Epoch: 4 batch_num: 112\n","train_kldiv: 0.1361\n","val_rmse_mean: 0.4706 val_rmse_std: 0.06173\n","New best_val_rmse: 0.4706\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.1 seconds\n","Epoch: 4 batch_num: 144\n","train_kldiv: 0.09246\n","val_rmse_mean: 0.4705 val_rmse_std: 0.0603\n","New best_val_rmse: 0.4705\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.1 seconds\n","Epoch: 4 batch_num: 176\n","train_kldiv: 0.06752\n","val_rmse_mean: 0.4691 val_rmse_std: 0.05929\n","New best_val_rmse: 0.4691\n","\n","Performance estimates:\n","[0.48516161186118684, 0.4691287610685]\n","Mean: 0.4771451864648434\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 3/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 83.3 seconds\n","Epoch: 0 batch_num: 64\n","train_kldiv: 1.299\n","val_rmse_mean: 0.689 val_rmse_std: 0.9287\n","New best_val_rmse: 0.689\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.1 seconds\n","Epoch: 0 batch_num: 128\n","train_kldiv: 0.6615\n","val_rmse_mean: 0.6695 val_rmse_std: 0.9992\n","New best_val_rmse: 0.6695\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.3 seconds\n","Epoch: 1 batch_num: 4\n","train_kldiv: 0.5136\n","val_rmse_mean: 0.5414 val_rmse_std: 0.9971\n","New best_val_rmse: 0.5414\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 1 batch_num: 36\n","train_kldiv: 0.1943\n","val_rmse_mean: 0.5226 val_rmse_std: 0.9832\n","New best_val_rmse: 0.5226\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 1 batch_num: 68\n","train_kldiv: 0.4642\n","val_rmse_mean: 0.5512 val_rmse_std: 0.965\n","Still best_val_rmse: 0.5226 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.1 seconds\n","Epoch: 1 batch_num: 132\n","train_kldiv: 0.6624\n","val_rmse_mean: 0.5097 val_rmse_std: 1.011\n","New best_val_rmse: 0.5097\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 1 batch_num: 164\n","train_kldiv: 0.6556\n","val_rmse_mean: 0.5065 val_rmse_std: 0.9815\n","New best_val_rmse: 0.5065\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.6 seconds\n","Epoch: 2 batch_num: 8\n","train_kldiv: 0.3123\n","val_rmse_mean: 0.5232 val_rmse_std: 1.006\n","Still best_val_rmse: 0.5065 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 40\n","train_kldiv: 0.1581\n","val_rmse_mean: 0.5185 val_rmse_std: 0.9881\n","Still best_val_rmse: 0.5065 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 2 batch_num: 72\n","train_kldiv: 0.2128\n","val_rmse_mean: 0.4945 val_rmse_std: 0.9612\n","New best_val_rmse: 0.4945\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 2 batch_num: 104\n","train_kldiv: 0.1871\n","val_rmse_mean: 0.4805 val_rmse_std: 0.9915\n","New best_val_rmse: 0.4805\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 136\n","train_kldiv: 0.2591\n","val_rmse_mean: 0.4911 val_rmse_std: 0.9944\n","Still best_val_rmse: 0.4805 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 2 batch_num: 168\n","train_kldiv: 0.2438\n","val_rmse_mean: 0.4796 val_rmse_std: 0.9969\n","New best_val_rmse: 0.4796\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.5 seconds\n","Epoch: 3 batch_num: 12\n","train_kldiv: 0.09576\n","val_rmse_mean: 0.4787 val_rmse_std: 1.004\n","New best_val_rmse: 0.4787\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 3 batch_num: 44\n","train_kldiv: 0.09755\n","val_rmse_mean: 0.4862 val_rmse_std: 0.9778\n","Still best_val_rmse: 0.4787 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 3 batch_num: 76\n","train_kldiv: 0.1841\n","val_rmse_mean: 0.4833 val_rmse_std: 0.9729\n","Still best_val_rmse: 0.4787 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 3 batch_num: 108\n","train_kldiv: 0.07744\n","val_rmse_mean: 0.4805 val_rmse_std: 0.9976\n","Still best_val_rmse: 0.4787 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 3 batch_num: 140\n","train_kldiv: 0.1366\n","val_rmse_mean: 0.4737 val_rmse_std: 1.005\n","New best_val_rmse: 0.4737\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 3 batch_num: 172\n","train_kldiv: 0.1383\n","val_rmse_mean: 0.4815 val_rmse_std: 0.9763\n","Still best_val_rmse: 0.4737 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.5 seconds\n","Epoch: 4 batch_num: 16\n","train_kldiv: 0.06742\n","val_rmse_mean: 0.4756 val_rmse_std: 0.9819\n","Still best_val_rmse: 0.4737 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 48\n","train_kldiv: 0.121\n","val_rmse_mean: 0.4781 val_rmse_std: 0.9827\n","Still best_val_rmse: 0.4737 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 80\n","train_kldiv: 0.06301\n","val_rmse_mean: 0.4765 val_rmse_std: 0.9916\n","Still best_val_rmse: 0.4737 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 112\n","train_kldiv: 0.1241\n","val_rmse_mean: 0.4758 val_rmse_std: 0.977\n","Still best_val_rmse: 0.4737 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 144\n","train_kldiv: 0.05762\n","val_rmse_mean: 0.4779 val_rmse_std: 0.9815\n","Still best_val_rmse: 0.4737 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 176\n","train_kldiv: 0.05479\n","val_rmse_mean: 0.4781 val_rmse_std: 0.9904\n","Still best_val_rmse: 0.4737 (from epoch 3)\n","\n","Performance estimates:\n","[0.48516161186118684, 0.4691287610685, 0.47370595559878237]\n","Mean: 0.4759987761761564\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 4/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 83.4 seconds\n","Epoch: 0 batch_num: 64\n","train_kldiv: 0.6181\n","val_rmse_mean: 0.7383 val_rmse_std: 1.099\n","New best_val_rmse: 0.7383\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.3 seconds\n","Epoch: 0 batch_num: 128\n","train_kldiv: 0.7273\n","val_rmse_mean: 0.5922 val_rmse_std: 1.001\n","New best_val_rmse: 0.5922\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.5 seconds\n","Epoch: 1 batch_num: 4\n","train_kldiv: 0.5208\n","val_rmse_mean: 0.6875 val_rmse_std: 1.019\n","Still best_val_rmse: 0.5922 (from epoch 0)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.2 seconds\n","Epoch: 1 batch_num: 68\n","train_kldiv: 0.4424\n","val_rmse_mean: 0.5242 val_rmse_std: 0.9928\n","New best_val_rmse: 0.5242\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 1 batch_num: 100\n","train_kldiv: 0.6418\n","val_rmse_mean: 0.5486 val_rmse_std: 1.027\n","Still best_val_rmse: 0.5242 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 1 batch_num: 132\n","train_kldiv: 0.5431\n","val_rmse_mean: 0.5625 val_rmse_std: 0.9775\n","Still best_val_rmse: 0.5242 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.5 seconds\n","Epoch: 2 batch_num: 8\n","train_kldiv: 0.4981\n","val_rmse_mean: 0.5085 val_rmse_std: 0.9836\n","New best_val_rmse: 0.5085\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 40\n","train_kldiv: 0.3493\n","val_rmse_mean: 0.5074 val_rmse_std: 1.028\n","New best_val_rmse: 0.5074\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.5 seconds\n","Epoch: 2 batch_num: 72\n","train_kldiv: 0.4012\n","val_rmse_mean: 0.5054 val_rmse_std: 0.9689\n","New best_val_rmse: 0.5054\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 104\n","train_kldiv: 0.4928\n","val_rmse_mean: 0.5222 val_rmse_std: 1.029\n","Still best_val_rmse: 0.5054 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 136\n","train_kldiv: 0.4328\n","val_rmse_mean: 0.486 val_rmse_std: 0.9758\n","New best_val_rmse: 0.486\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 168\n","train_kldiv: 0.4105\n","val_rmse_mean: 0.4998 val_rmse_std: 0.984\n","Still best_val_rmse: 0.486 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.6 seconds\n","Epoch: 3 batch_num: 12\n","train_kldiv: 0.1589\n","val_rmse_mean: 0.487 val_rmse_std: 1.005\n","Still best_val_rmse: 0.486 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 3 batch_num: 44\n","train_kldiv: 0.2339\n","val_rmse_mean: 0.4921 val_rmse_std: 0.9941\n","Still best_val_rmse: 0.486 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 3 batch_num: 76\n","train_kldiv: 0.1198\n","val_rmse_mean: 0.4819 val_rmse_std: 0.9818\n","New best_val_rmse: 0.4819\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 3 batch_num: 108\n","train_kldiv: 0.105\n","val_rmse_mean: 0.5031 val_rmse_std: 0.9907\n","Still best_val_rmse: 0.4819 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 3 batch_num: 140\n","train_kldiv: 0.2586\n","val_rmse_mean: 0.4961 val_rmse_std: 0.9963\n","Still best_val_rmse: 0.4819 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 3 batch_num: 172\n","train_kldiv: 0.1564\n","val_rmse_mean: 0.4793 val_rmse_std: 1.012\n","New best_val_rmse: 0.4793\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.5 seconds\n","Epoch: 4 batch_num: 16\n","train_kldiv: 0.1258\n","val_rmse_mean: 0.4784 val_rmse_std: 0.985\n","New best_val_rmse: 0.4784\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 4 batch_num: 48\n","train_kldiv: 0.06226\n","val_rmse_mean: 0.4774 val_rmse_std: 1.011\n","New best_val_rmse: 0.4774\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 4 batch_num: 80\n","train_kldiv: 0.09729\n","val_rmse_mean: 0.4806 val_rmse_std: 0.9896\n","Still best_val_rmse: 0.4774 (from epoch 4)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 4 batch_num: 112\n","train_kldiv: 0.1017\n","val_rmse_mean: 0.4763 val_rmse_std: 0.9987\n","New best_val_rmse: 0.4763\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 4 batch_num: 144\n","train_kldiv: 0.1557\n","val_rmse_mean: 0.485 val_rmse_std: 0.9896\n","Still best_val_rmse: 0.4763 (from epoch 4)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.2 seconds\n","Epoch: 4 batch_num: 176\n","train_kldiv: 0.2023\n","val_rmse_mean: 0.4815 val_rmse_std: 0.9994\n","Still best_val_rmse: 0.4763 (from epoch 4)\n","\n","Performance estimates:\n","[0.48516161186118684, 0.4691287610685, 0.47370595559878237, 0.4762812679965918]\n","Mean: 0.4760693991312652\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 5/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 83.3 seconds\n","Epoch: 0 batch_num: 64\n","train_kldiv: 1.902\n","val_rmse_mean: 0.6224 val_rmse_std: 0.07399\n","New best_val_rmse: 0.6224\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.3 seconds\n","Epoch: 0 batch_num: 128\n","train_kldiv: 0.9011\n","val_rmse_mean: 0.5684 val_rmse_std: 0.07525\n","New best_val_rmse: 0.5684\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.5 seconds\n","Epoch: 1 batch_num: 4\n","train_kldiv: 0.3445\n","val_rmse_mean: 0.5279 val_rmse_std: 0.0672\n","New best_val_rmse: 0.5279\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.5 seconds\n","Epoch: 1 batch_num: 36\n","train_kldiv: 0.2316\n","val_rmse_mean: 0.5588 val_rmse_std: 0.06343\n","Still best_val_rmse: 0.5279 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","64 steps took 82.3 seconds\n","Epoch: 1 batch_num: 100\n","train_kldiv: 0.3927\n","val_rmse_mean: 0.4955 val_rmse_std: 0.05692\n","New best_val_rmse: 0.4955\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.5 seconds\n","Epoch: 1 batch_num: 132\n","train_kldiv: 1.382\n","val_rmse_mean: 0.5359 val_rmse_std: 0.06004\n","Still best_val_rmse: 0.4955 (from epoch 1)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.5 seconds\n","Epoch: 1 batch_num: 164\n","train_kldiv: 0.3201\n","val_rmse_mean: 0.4953 val_rmse_std: 0.06614\n","New best_val_rmse: 0.4953\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.8 seconds\n","Epoch: 2 batch_num: 8\n","train_kldiv: 0.2768\n","val_rmse_mean: 0.4862 val_rmse_std: 0.05548\n","New best_val_rmse: 0.4862\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.5 seconds\n","Epoch: 2 batch_num: 40\n","train_kldiv: 0.2258\n","val_rmse_mean: 0.4981 val_rmse_std: 0.06753\n","Still best_val_rmse: 0.4862 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 72\n","train_kldiv: 0.295\n","val_rmse_mean: 0.4753 val_rmse_std: 0.05878\n","New best_val_rmse: 0.4753\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 104\n","train_kldiv: 0.4968\n","val_rmse_mean: 0.5251 val_rmse_std: 0.05675\n","Still best_val_rmse: 0.4753 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 136\n","train_kldiv: 0.1675\n","val_rmse_mean: 0.48 val_rmse_std: 0.05817\n","Still best_val_rmse: 0.4753 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 2 batch_num: 168\n","train_kldiv: 0.3333\n","val_rmse_mean: 0.4903 val_rmse_std: 0.05138\n","Still best_val_rmse: 0.4753 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.6 seconds\n","Epoch: 3 batch_num: 12\n","train_kldiv: 0.07627\n","val_rmse_mean: 0.4825 val_rmse_std: 0.05703\n","Still best_val_rmse: 0.4753 (from epoch 2)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 3 batch_num: 44\n","train_kldiv: 0.07435\n","val_rmse_mean: 0.4714 val_rmse_std: 0.05413\n","New best_val_rmse: 0.4714\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 3 batch_num: 76\n","train_kldiv: 0.1367\n","val_rmse_mean: 0.4868 val_rmse_std: 0.06049\n","Still best_val_rmse: 0.4714 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.3 seconds\n","Epoch: 3 batch_num: 108\n","train_kldiv: 0.05948\n","val_rmse_mean: 0.498 val_rmse_std: 0.04895\n","Still best_val_rmse: 0.4714 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 3 batch_num: 140\n","train_kldiv: 0.1341\n","val_rmse_mean: 0.4712 val_rmse_std: 0.04983\n","New best_val_rmse: 0.4712\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 3 batch_num: 172\n","train_kldiv: 0.07707\n","val_rmse_mean: 0.4816 val_rmse_std: 0.05385\n","Still best_val_rmse: 0.4712 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.6 seconds\n","Epoch: 4 batch_num: 16\n","train_kldiv: 0.07765\n","val_rmse_mean: 0.4778 val_rmse_std: 0.04881\n","Still best_val_rmse: 0.4712 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 4 batch_num: 48\n","train_kldiv: 0.03593\n","val_rmse_mean: 0.4764 val_rmse_std: 0.04957\n","Still best_val_rmse: 0.4712 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 4 batch_num: 80\n","train_kldiv: 0.07558\n","val_rmse_mean: 0.4745 val_rmse_std: 0.04882\n","Still best_val_rmse: 0.4712 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 4 batch_num: 112\n","train_kldiv: 0.09482\n","val_rmse_mean: 0.4796 val_rmse_std: 0.05256\n","Still best_val_rmse: 0.4712 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 4 batch_num: 144\n","train_kldiv: 0.1172\n","val_rmse_mean: 0.4798 val_rmse_std: 0.05289\n","Still best_val_rmse: 0.4712 (from epoch 3)\n","{'total_MiB': 16280, 'used_MiB': 15093}\n","\n","32 steps took 41.4 seconds\n","Epoch: 4 batch_num: 176\n","train_kldiv: 0.06975\n","val_rmse_mean: 0.4784 val_rmse_std: 0.04678\n","Still best_val_rmse: 0.4712 (from epoch 3)\n","\n","Performance estimates:\n","[0.48516161186118684, 0.4691287610685, 0.47370595559878237, 0.4762812679965918, 0.47122576231820734]\n","Mean: 0.4751006717686536\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"YSFnleU5vZS8","executionInfo":{"status":"ok","timestamp":1627829982103,"user_tz":-540,"elapsed":46,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"918b828a-f58a-46bc-a4ba-d06a1e2cf3c5"},"source":["\"\"\"desc_block = TextDescriptionBlock('excerpt')\n","desc_feats = desc_block.fit(train_kf_df.loc[train_indices])\n","desc_block_src = TextDescriptionBlock('excerpt')\n","desc_feats_src = desc_block_src.fit(train_kf_df.head(10))\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"desc_block = TextDescriptionBlock('excerpt')\\ndesc_feats = desc_block.fit(train_kf_df.loc[train_indices])\\ndesc_block_src = TextDescriptionBlock('excerpt')\\ndesc_feats_src = desc_block_src.fit(train_kf_df.head(10))\""]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"D0biwiMjq5KP","executionInfo":{"status":"ok","timestamp":1627829982103,"user_tz":-540,"elapsed":39,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"765c0c34-3e99-427d-d47d-cf869acbab28"},"source":["\"\"\"train_dataset = LitDataset(train_kf_df.loc[train_indices]) # train, validのDataset\n","val_dataset = LitDataset(train_kf_df.loc[val_indices])\n","    \n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                          drop_last=True, shuffle=True, num_workers=2) # train, validのDataLoader\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                        drop_last=False, shuffle=False, num_workers=2)    \n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'train_dataset = LitDataset(train_kf_df.loc[train_indices]) # train, validのDataset\\nval_dataset = LitDataset(train_kf_df.loc[val_indices])\\n    \\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\\n                          drop_last=True, shuffle=True, num_workers=2) # train, validのDataLoader\\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\\n                        drop_last=False, shuffle=False, num_workers=2)    \\n'"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"m4v-cGx-Mv7S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627829982104,"user_tz":-540,"elapsed":39,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"d363339e-8527-4f37-8f34-8c993d39e0eb"},"source":["print(list_val_rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.48516161186118684, 0.4691287610685, 0.47370595559878237, 0.4762812679965918, 0.47122576231820734]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XU4gRXHCBEpC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAb99KSKBEmd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jH0aFzWxBEkG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2CdCMuIKDMP"},"source":["#rep = MemReporter(model)\n","#rep.report()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLl1yDOOKIe7"},"source":["#rep = MemReporter(model.roberta)\n","#rep.report()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qkqnknA_m9D"},"source":["#gpuinfo()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwrqSMdYA6Pu"},"source":["#del model\n","#del optimizer \n","#del train_loader\n","#del val_loader\n","#del scheduler \n","#del list_val_rmse\n","#del train_indices\n","#del val_indices\n","#del tokenizer\n","#torch.cuda.empty_cache()\n","#gpuinfo()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wXcHyUSJXecL"},"source":["# upload models"]},{"cell_type":"code","metadata":{"id":"YIV6UllSIGoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627830096059,"user_tz":-540,"elapsed":113982,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"0c5df65d-38fb-472e-f217-18da6e2b6ed2"},"source":["%cd\n","!mkdir .kaggle\n","!mkdir /content/model\n","!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\n","\n","!cp -r /content/model_1.pth /content/model/model_1.pth\n","!cp -r /content/model_2.pth /content/model/model_2.pth\n","!cp -r /content/model_3.pth /content/model/model_3.pth\n","!cp -r /content/model_4.pth /content/model/model_4.pth\n","!cp -r /content/model_5.pth /content/model/model_5.pth"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y9OpHdmWTPLE"},"source":["import shutil\n","model_path_out = Path('/content/model/')\n","dir_ = f'/content/drive/MyDrive/Colab_Files/kaggle/commonlit/98_model_inf/{EX_NO}'\n","!mkdir {dir_}\n","tgdir = Path(dir_)\n","\n","for file_ in list(model_path_out.iterdir()):\n","  shutil.copy(file_, tgdir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"14ddOZH4IMam","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627830233300,"user_tz":-540,"elapsed":137257,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"8f4b03ca-e25d-41c8-f499-d8efb14481ee"},"source":["def dataset_upload():\n","    import json\n","    from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","    id = f'{USERID}/{EX_NO}'\n","    print(id)\n","\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = id\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = f'{EX_NO}'\n","\n","    with open(UPLOAD_DIR / 'dataset-metadata.json', 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","\n","    api = KaggleApi()\n","    api.authenticate()\n","\n","    # データセットがない場合\n","    if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n","        api.dataset_create_new(folder=UPLOAD_DIR,\n","                               convert_to_csv=False,\n","                               dir_mode='skip')\n","    # データセットがある場合\n","    else:\n","        api.dataset_create_version(folder=UPLOAD_DIR,\n","                                   version_notes='update',\n","                                   convert_to_csv=False,\n","                                   delete_old_versions=True,\n","                                   dir_mode='skip')\n","dataset_upload()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["calpis10000/058-train-02\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Starting upload for file model_4.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:26<00:00, 53.6MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_4.pth (1GB)\n","Starting upload for file model_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:26<00:00, 53.1MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_1.pth (1GB)\n","Starting upload for file model_5.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:26<00:00, 54.3MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_5.pth (1GB)\n","Starting upload for file model_3.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:26<00:00, 53.9MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_3.pth (1GB)\n","Starting upload for file model_2.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:26<00:00, 53.1MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_2.pth (1GB)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"huJwVMSAPuDO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627830439693,"user_tz":-540,"elapsed":206398,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"02245d33-0f71-4c80-bf2f-7ee439f8ca5c"},"source":["# validation再実行_予測結果取得\n","all_predictions = np.zeros(len(train_kf_df)) # 推論結果について、「fold　× 推論df」のzero行列で枠を作る\n","\n","for fold_ in sorted(train_kf_df['kfold'].unique()):\n","    model_path = UPLOAD_DIR/f\"model_{fold_ + 1}.pth\" # 対応するモデルを読む\n","    print(f\"\\nUsing {model_path}\")\n","\n","    val_idx = train_kf_df['kfold'] == fold_\n","    val_df = train_kf_df[val_idx]\n","    val_dataset = LitDataset(val_df, inference_only=True) # TestのDataset(何で、もう一回作るのだろう？)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                          drop_last=False, shuffle=False, num_workers=2) # TestのDataLoader\n","\n","    model = LitModel()\n","    model.load_state_dict(torch.load(model_path))    # 対応するモデルから、重みを読み込む\n","    model.to(DEVICE) # モデルをDEVICEへぶち込む\n","\n","    all_predictions[val_idx] = predict(model, val_loader) # 推論結果行列の対象列に、推論結果を入力(以後、繰り返し)\n","\n","    del model\n","    gc.collect()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Using /content/model/model_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/model/model_2.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/model/model_3.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/model/model_4.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/model/model_5.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"0zzuBPobmLFu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627830439694,"user_tz":-540,"elapsed":25,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"cadfd038-7141-4723-a77d-b5edb15f3e39"},"source":["from sklearn.metrics import mean_squared_error\n","import math\n","np.sqrt(mean_squared_error(train_kf_df.target.values, all_predictions))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4751343137650183"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"Wpc8ro9hmNci","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627830439695,"user_tz":-540,"elapsed":23,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"9d982804-9b18-4658-f375-3b075128376c"},"source":["train_kf_df['pred'] = all_predictions\n","fold = 1\n","tg_true = train_kf_df[train_kf_df['kfold']==fold]['target'].values\n","tg_pred = train_kf_df[train_kf_df['kfold']==fold]['pred'].values\n","np.sqrt(mean_squared_error(tg_true, tg_pred))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.46912876169040524"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"ceDI72NumT5-","colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"status":"ok","timestamp":1627830440267,"user_tz":-540,"elapsed":590,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"bc43fc5a-d404-4db4-de34-742a5962227a"},"source":["train_kf_df['pred'] = all_predictions\n","train_kf_df['diff_sq'] = (train_kf_df['target'] - train_kf_df['pred'])**2\n","train_kf_df.plot(kind='scatter', x='target', y='diff_sq')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f339bc9ea90>"]},"metadata":{"tags":[]},"execution_count":38},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYMAAAEGCAYAAACHGfl5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOyde3wV5Z3/P9+Zc0lIIGBAMSSB2oiWUEFNRYt1FWuLithd0Vq0btu1bvcnttt66w1v/Lq7itqq0PaH1t1aqRWxFQR3vYGLUEWDDUgiYsQLIcolQEhCci4zz++POXMyZ+aZc8uZnHOS7/v18iU5Z845z8w883y/z/dKQggwDMMwwxsl3wNgGIZh8g8LA4ZhGIaFAcMwDMPCgGEYhgELA4ZhGAaAL98DyIaxY8eKSZMm5XsYDMMwRcWWLVsOCCHGyd4rSmEwadIkNDY25nsYDMMwRQURfeT2HpuJGIZhGBYGDMMwDAsDhmEYBiwMGIZhGLAwYBiGYcDCgGGYAqCjO4Stuw+jozuU76EMW4oytJRhmKHDqqY9uPXpbfArCiK6jnsuOwVzp08YlN/u6A6h7VAvqseUorI8OCi/WaiwMGAYJm90dIdw69Pb0BfR0QcdAHDL09sws26s54tzPoVQIcJmIoZh8kbboV74lcRlyK8oaDvU6+nvWoVQVyiKvoiOW57eNqzNVJ4KAyIqIaI3iGgrETUT0Z2SY4JE9CQRtRLRZiKa5OWYGIYpHKrHlCKi6wmvRXQd1WNKPf3dfAmhQsbrnUEIwCwhxDQA0wHMJqIzbcf8E4BDQog6AL8EcLfHY2IYpkCoLA/instOQYlfwcigDyV+BfdcdornJqJ8CaFCxlOfgTB6anbH/vTH/rP32bwUwB2xf68EsISISHA/ToYZFsydPgEz68YOqiPXFEK32HwGw9mJ7LkDmYhUAFsA1AFYKoTYbDtkAoDdACCEiBJRJ4BKAAds33MdgOsAoLa21uthMwwziFSWBwd9Ic6HECpkPHcgCyE0IcR0ANUAziCiqVl+zzIhRIMQomHcOGkFVoZhmIyoLA9iWs3oYS8IgEGMJhJCHAawHsBs21t7ANQAABH5AFQA6BiscTEMwzDeRxONI6LRsX+XArgAwA7bYasB/GPs3/MArGN/AcMwzODitc/geAC/j/kNFAArhBBriOguAI1CiNUAfgfgD0TUCuAggCs9HhPDMAxjw+toom0ATpW8fpvl330ALvdyHAzDMExyOAOZYRiGYWHAMAzDsDBgGIZJynApr81VSxmGYVwYTpVNeWfAMAwjYbhVNmVhwDAMI2G4VTZlYcAwDCNhuFU2ZWHAMAwjIV/ltfMFO5AZhmFcGE6VTVkYMAzDJCEf5bXzAZuJGIZhGBYGDMMwDAsDhmEYBiwMGIZhGLAwYBiGYcDCYMAMlyJWDMMMbTi0dAAMpyJWDMMMbXhnkCXDrYjVYMK7LYYZfHhnkCVmEas+9NcuMYtYDYcEFa/g3RbD5AfeGWTJcCtiNRjwboth8gcLgywZbkWsBoPhVjKYYQoJNhMNgOFUxGow4N0Ww+QP3hkMkMryIKbVjGZBkAN4t8Uw+cPTnQER1QB4DMBxAASAZUKIB2zHnAtgFYAPYi/9WQhxl5fjYgoX3m0xTH7w2kwUBXCjEOItIhoJYAsRvSiEaLEd96oQYo7HY2GKhOFSMphhCglPzURCiE+EEG/F/t0F4B0AHCfI5ATOR2CY3DFoDmQimgTgVACbJW+fRURbAbQDuEkI0TxY42KKE85HYJjcMigOZCIqB/A0gH8VQhyxvf0WgIlCiGkAHgLwjMt3XEdEjUTUuH//fm8HzBQ0nI/AMLnHc2FARH4YgmC5EOLP9veFEEeEEN2xfz8HwE9EYyXHLRNCNAghGsaNG+f1sJkChvMRGCb3eCoMiIgA/A7AO0KI+12OGR87DkR0RmxMHV6OiyluOB+BYXKP1zuDmQC+CWAWETXF/ruIiL5HRN+LHTMPwPaYz+BBAFcKIYTH42KKGM5HYJjcQ8W47jY0NIjGxsacf29Hd4jj24sIvl8MkxlEtEUI0SB7j8tRxODolOKD8xEYJndwOQpwdArDMAwLA3B0CsMwDAsDcHQKwzAMCwNwdArDMAw7kGN4XS2TI18YhklFPtcJFgYWvIpO4UglhmFSke91gs1EHsORSgzDpKIQ1gkWBh7DkUoMw6SiENYJFgYew5FKDMOkohDWCRYGHsORSgzDpKIQ1gmuTTRIcDQRwzCp8Hqd4NpEBQDX0WEYJhX5XCfYTMQwDMOwMGAYhmFYGDAMwzBgYcAwDMOAhQHDMAwDFgYMwzAMWBgwDMMwYGHAMAzDgIUBwzAMAxYGDMMwDFgYMAzDMPBYGBBRDRGtJ6IWImomoh9IjiEiepCIWoloGxGd5uWYGIZhGCdeF6qLArhRCPEWEY0EsIWIXhRCtFiOuRDAibH/ZgD4Tez/DMMwzCDh6c5ACPGJEOKt2L+7ALwDwN7U81IAjwmD1wGMJqLjvRwXwzCMnY7uELbuPjxsW9IOWglrIpoE4FQAm21vTQCw2/J3W+y1T2yfvw7AdQBQW1vr1TCzgnsVMExxk+9m9IXAoAgDIioH8DSAfxVCHMnmO4QQywAsA4zmNjkc3oDgScQwxY21GX0fjNaTtzy9DTPrxg4r5c7zaCIi8sMQBMuFEH+WHLIHQI3l7+rYawWPdRJ1haLoi+i45eltw3abyTDFSCE0oy8EvI4mIgC/A/COEOJ+l8NWA7gmFlV0JoBOIcQnLscWFDyJGGbwybVtvxCa0RcCXpuJZgL4JoC3iagp9tpPAdQCgBDitwCeA3ARgFYARwF82+Mx5QyeREyhMdT9V16YZc1m9LfYvncoXr9kkBAFY35Pm4aGBtHY2JjXMZgP3fY9nVi0toV9BkzeGer+q47uEGbevQ59kX4FrMSvYNOts3KycA91QQoARLRFCNEge2/QoomGEvaHbuGcKZhaVTGkJxFT2AwHJ6hpljXPD+g3y+biHPPZjL4Q4HIUGSJzGi9a08KCgMkrMv+VSoT1O/YNmYAGNst6CwuDDGGnMVOIyBbKnrCGO55txsy712F1U1EE6CXFtO2X+BWMDPpQ4leGpW3fK9hMlCGsnTCFiNUJqhKhJ6wBALpDxv+Hislo7vQJmFk3dsjb9vMB7wwyhLUTplCZO30CNt06C3fOrUd5UE14byjtXivLg5hWM5qfuRzDO4MsYO2EKVQqy4M47+Rj8fNV2xNe590rkwreGWQJaydMocK7VyYbeGfAMEMQ3r0ymcLCgCl4hkMykBcM97j5bBmu842FAVPQDPWsWqawGM7zLW1hkKodpdnEhmFyxXDIqmUKh+E+3zLZGfwawGkAtgEgAJ8HsAVAHwABYFbOR8cUNF5vp70uP8AwVgZjvhWyCSoTYdAO4LtCiLcBgIimArhDCDHPk5ExBc1gbKc5wY8ZTLyeb4VugsoktPQkUxAAgBBiO4DP5X5ITKFhrx8/WE19hmuI5HDvxZsvvJxvxdAIK5OdwTYiegTA47G/r4JhMmKGMDJtZmJl2aCZb4ZbiGSha4+FglfmFq/mWzGYPDMRBt8G8C8AfhD7ewOA3+R8RINMIdvw8o2bQ23NgrMH1XwzXEIkh7sDM128FphezLdiMHmmbSYSQvQJIX4phPh7AP8E4GUhRJ93Q/OeVU17MPPudbj6kc1DprJjLnGr0NoT1oal+cZrpGWoFRoyNYVyQTGYW2QUg8kzk9DSVwDMjX1mC4B9RPRXIcQPPRqbp7AWlppk2sy0mtEpt9O868oMaRnqkIbtezoxrWZ0nkZVWBSDucWNQjd5ZuJArhBCHAHwDwAeE0LMAHC+N8PyHi/6EpiOv9a9XUPCAZhKm0lWn8nrXddQdLJWlgexcM4Ux+uL1rYMqfMcCMVgbklGIdc0y8Rn4COi4wFcAeBnHo1n0Mj1pDLtmEIXCGkCAVUBEbB4XnE7ALPRZrzedQ1lJ+vUqgqUBdR4PwKgeDTfwYCb13tHJsLgLgDPA9gohHiTiE4A8J43w/KeXE4q6+JnEtaMf9/41NaiNz1l6lBrO9QLlSjhtVwtaEPdvFc9phSaEAmvFZPmmwnZmhEL3dxSrKQtDIQQTwF4yvL3LgCXmX8T0U+EEP+e2+F5S64mlcyOaRLRBJrbO3HO5GMHOtyiYfuezgTNFsjdglbMNuN0GC6a70B3d8MlwmwwyWWhussBFJUwAHIzqWQmp0QoyXtDi9a9XbhzTYvj9YUXT8nJw1vsNuN0GOqa71Df3RUruWxu41jxiOhRItpHRNulHyA6l4g6iagp9t9tORxPUnLpgDS1uaDPeTl9ClBfNSrr7y4mR+mqpj246MFXEY4mLtZlQRVTJ1Tk5DfcnNoAiuY6pUMhOxqzxZzLze2dOQ/eYAZOLncGQvLafwFYAuCxJJ97VQgxJ4fjSIkXDkhTm/vj5o/x0Lqd8CkqNKFj8bxpWT/QxeQoNbW9sOacBlEtt5q7XXPe2HoAM+9eVxTXaaiQqb3fOpfDmgbdNk2G2u4uFYUYdp1LYeDYGQghNhDRpBz+xoAZyBY11Q2sLA/ihvNPxPwZtQO+0anGWWiTKZnfRBfAptYDnmSJsslh8MlUSZHdI58CBH0KAurQ9Yu4UahKXkphQER3CyFuJaLLY05kN5K9l4yziGgrjKqoNwkhml3GcR2A6wCgtrY2y5/K3gGZyQ3MhR8i2Tg3th7wbDJlKmTM48sCqqvfJKIJz6Kqit2hXGhCPRXZCF/ZPSr1+7D0qtNQUeovmnPPBYWsvKSzM7iIiH4M4CdIsuALIf4ti99/C8BEIUQ3EV0E4BkAJ7p8/zIAywCgoaFBZpJKi2wckPm4gW7jLAuono0lU43FfvwVDdVY0dgGCKDP5jfwKqqqmB3KhaohJiMb4et2j+qrRnm6ABaioC1k5SUdB/L/ADgE4BQiOmL5r4uIjgzkx4UQR4QQ3bF/PwfAT0RjB/KdqcimRkiqbGUvnLxu4+wJa5443zKt+SI7fkVjGx7/zhm4+syJLr+S+6iqYqj5IpsfxVpjJxvhm497VKh1xwpZeUlnZ/BzIcTNRLRKCHFpLn+ciMYD2CuEEER0Bgzh1JHL35CRaeheshvopXYnG2dHd8gxllBUQ1lAHdBvZaqxuPkI5j+yGX4191FVySjkUEy3+VHIGmIyss2DGMx7lGwnDyCv86SQ80jSEQavwWh3mfEugIieAHAugLFE1AbgdgB+ABBC/BbAPAD/QkRRAL0ArhRCZG0CyoRM7PpuNxCA5+Yj+zitYzFLXygKYc6SjQMSRJlqLLLjzQzssNafcFbiVyCEGFBUVTrb/UJMQkq2KBWyhpiKbBf2wbpHboJ2+eaP8etXWvNulitU5SUdYRAgovkAvkhE/2B/UwjxZ7cPCiG+keyLhRBLYISeFjyyG7h19+G8aHdzp0/AlONH4aKHNgIQ8UV4IIIoU43FfnxI00HCEE4mZQEVd86tx3knHzsswmvtJNP+p9WMLlgNMR0KUfiayARtWNOxdP17CEVFQThuC/H6pSMMvgejq9loAJfY3hMAXIXBUMN+A3Oh3WXr5OoJawiqSkKC10AFUaYai/X4soCKOUs2AhZhoAkxIEFQyJEX6ZAsCGDr7sOYWTcWm26dVXAaYrEjU2yuP7cOyzbsQigajR9XDGa5wSSlMBBCbASwkYgahRC/G4Qx5Y1MF+aB2v8GovV6ZWbIVGOxHp9rTbdY7eomsvlxRUM15izZKL3nhRj9UqzYFRsAWPpKa8IxxWKWGywolYmeiGYJIdbJTERAcjORVzQ0NIjGxsacfudAFuZsHuKO7hBm3r0uodJpiV/Bpltnpf0dq5v2OBbffJtQcrmg5eIaFQLWXIw5SzZKz8fL3JHhQrK519Edwh83f4wl699DQFXTvsZDTUAT0RYhRIPsvXTMROcAWAfDRCRgxAda/1/0ZqKBmiOysf/lQustREdULm2hhRx5kQnmNXHzMTW3Hylqc1imeLHAJlPmrO8BhOvOOQHzZ9Sm/O1i9ldlQzrCoIuIfgRgO/qFACCvRVSUeGWOSDbpc2XmKURHVC4pRIGXLW73HBBFbQ7LhFwvsB3dITS3d+KWlVulzmHAGfG39JVWzJ+RvIpBsfursiEdYVAe+/9JAL4AYBUMgXAJgDc8Gteg4oX9PdWkHypab66RCdB8C7xcabJu97y+qqJow0wzIdcLrPmMKUQIRRN1U2siZjaCttj9VdmQjgP5TgAgog0AThNCdMX+vgPAWk9HN0jkemFOd9K7JZXlUgsuJptnIW7Lcz0mt53OcFAMcrnAyroLWrEK02wEbTHngWRLJlVLjwMQtvwdjr02JMilOSKTSW/VenO98BTi4upGIW7LvRqTbKczlMxhbuRygXXLgB/hV6FDJAjTbATtcNy5ZyIMHgPwBhH9Jfb312D0Kxgy5MocUQjF8ApxcU1GIW7LB3tM+TaHeU1leRALL56CO59thl9VoAmR9QIre8aCPgW//ebpjgJ42Qra4SCgraTd6UwI8QsA34ZRtO4QgG8XW8/jwcLUKnJZDC9Tcv19XjMY2/JMCwoOdVPBYHfRW9W0B4vWtiDgUxDRBRbOmZL1TlX2jC2edwrOmTxO+pxVlmfXOS7bzxUjGTW3EUK8BaPsNJOCXBbDy4ZiW8i83pZnYzIbyqaCwTYhymz8i9a0YHb9+EHLmGeSk8tOZ4yNTLb9uV54inEh8+rhztZk1tEdwsTKMqxZcDZ6wlpRLDjpBAzkw4TolcmtkExrxRSsIYOFQQGR68WwGDUnLx7ubBYimeY8rWZ0TseVa9LV9vPhnym2nWqmFFOwhhtp+wyYwSHXNsrhZPO0Y9rEZS05ky1Exdh4JpMx52NhzsaPlm/S9akU43yRwTsDZkji1pIzHZNZIUY2pSLTcOZcmRAzMY0U0041E02/GOeLDBYGzJBDZhNf0diWtu2/GE0amY7ZbWG2Lu5A8q5g2TrlC32BzNSnUozzRQYLAyZrCtVh5qap9YS1tOz+xeh8z2bM9oXZurj3RqIgIpT45BU+iy2PJRMy1fQzufaF+swALAxySqY3upAnRioK2WFWPaYUYW1gmpqpOTe3dwIgz/o355KBmGFkizsgENGMZjD2hX6omEZkyDT9kKYn7TOezrUv5GcGYGGQMzK90fmaGLkQQAMJ1RwM4bex9QA0y8PsVykrzT7bHgP5FPLZmmGa249AiRckdmJf6AdiGil0Jciq6QNGb28SImWf8WTXvhh2UsNOGHgxETO90fmaGLkSQLkK1fRC+JnX1tINFAohXs440+/J9B4VuvYnY1XTHtyychtCUd31GPtCn60prViuT7zP+IOvAoDR21sTWT+nxbCTGlbCwKuJmOmNzsfEyKUAylQrHEzhJ7u2AVXN+Npmc4+8PM90lZhsTJW3Pu0UBCoBipLoMxhocb1i0I6t9IQ1BH0qwtrA+yYXg5N52AiDju5QXPvJ9UTM9EYP5sQwF4fO3nDOBFCmWuFgCr9cXdtsvser80xXiclG2ZGNeURAxW+vPg31VRUpF/pMzFLFoB1byeVzWgxBCcNGGCzf/LFD+xlILXXrQ+J2owFg6+7DjodpsCaGdXEIaxp0W2+6gQggq1ZYFlDRE9bQ0R3Ke+hdrq5tNt/jxXmmq01ncpx17srGrAuB+qoK14U+W1OrV/PAKx9Erp/TQs+z8FQYENGjAOYA2CeEmCp5nwA8AOAiAEcBfCtWDC+ndHSHsHT9e47Xw7EIAdmC7Yab9mW/0RtbD2Dm3etctTSvJ4ZscfApRpnfgGqMaeGcKfEqpplGnpjj/rCjJ6U2mouHKh/JTZl+j/U8VSJENB0LL54yoHsrc+zKlJi2Q73wKcmPc5u7mdybgZhavVCCvPZB5Po5LeQ8CxLCu1bGRHQOgG4Aj7kIg4sA3ABDGMwA8IAQYkaq721oaBCNjY1pj2Pr7sO4+pHN6ApFE16/aOp4rHt3X3wiLbx4CqZOqHC96R3dIcy8e11C5cUSv4JNt85yaF/pHOclsnMeGfRh6VWnoaLUj+17OrFobUvGD9Hy1z/CnWtaEFCNxU4XQETrn0PJzjNbDa4QnY7JzsV6jaK6yHq8bo5d2TVe/vpH+Nkz212PSzUn0y1wl4t5nStNvhCes2KDiLYIIRpk73m6MxBCbCCiSUkOuRSGoBAAXiei0UR0vBDik1yOQ7Y9DajAyzv2JfgQfvbMdpQHVdcHOF2bZ7ranJe4bcnNePmvL3stY0eedcEJR+XHpErOsb+eamEoRKdjMuHU0R3CorUtCEf1+DXKZrxujt2gzxkma/6mnYVzpgAwFIPO3kjSuZuOxporm3+utONi80EUOvkuVDcBwG7L322x1xwQ0XVE1EhEjfv378/oR8ztqbVI1g2zJiOgOk+/O6S5FppKx+a5qmkPvvtYI45GtKTHeY3snM1FJJvGNx3dIdz5bHPK383kPFc17cHMu9fh6kc2Y+bd67C6aY/jmFw36RloQ5dURclyNV7Z9wDAd2Z+xlVJsVIWUNHRHY5f3+8+1ojeSKIEz3ROFlpETKGNp9gpGgeyEGIZgGWAYSbK9PN22x8ALH2l1fV4mYaRyuaZiTY3GLjZO7OOlFENR7QVlQCf2u+HSPc809X4c/nA58LclEobTWe86ZhJZFnUAPDopg9w7ZdOSJhznb1hx32JaBqWrm9N2PmqCgHof3SuaKjOWKPPxObvdXJZMUToFBP5FgZ7ANRY/q6OveYJ9u2p1dnXE05Pk0/mUEoWpnfO5GM9OKPUyLbkCY5OhRDRjBaEqSJlNIl/6a6vTcXs+vEZP/TpbvEry4O4oqEaj732cfy1TBcxIHfmplSLfaoFKl2BVFkexILz6nDfizsTXrfmTFi/SxeAdanXBUFF4v3SbOFkKxrb8IPzJ2d0/uk6VAfLz1PoETrFRL6FwWoAC4joTzAcyJ259hckwzqRtrd3YtGalrQ0DDebZ7IwvULAqqnNnT4BXX3ReHPyRWtaMDLokz6w5ucWzpmCRWta4pEyt19Sj6tmTASQWTQSkHpRNX+zLKBiRWNbwnHZLGK5tHen0kaTVQTNRCDNn1GLJevfQyjav4ib10heS6ifqC7g4tYZ0Pmb1yDZZwbbz5PrENhCL5fhFV6Hlj4B4FwAY4moDcDtAPwAIIT4LYDnYEQStcIILf22l+ORYU6kaTWjMbt+/IAKkxXyttWuqS2cM8VwdGoibmKQPbCOz6WIuEqXZNfK+pshzagLYyWbRSyX5qZk2qh1IbFXSM2mGubiedOk12jr7sOO77JT4leg6yKWRatD0/WEMh1hTUdnb8Q1PyQbOrpDWL9jX8ow11zhtnC77UysSoasnHkhRq4NFl5HE30jxfsCwPVejiETsi1MZiUX29bWvV1o2n0Y02tGo+64kRl/3o5MU7vz2Rb4UzywrXu7cPNTWxHWRPxzi9a25Cx0T3atUmm8QHaLeDr+nmS1/WWmNvtrqRaSbARSJn4fGc99/0vxRW9T64H4+fdFNWi6juuXv5WzRc88/0zMrrn4PbuiUhZQpTuTrr4oFq1tgdAFQppAid9wulsFRaFFrg0m+TYTFQy5nAgDCZ277Zm38djr/fbxa86qxV2Xfj6r7zKRaqQqIRJ1X5hWNe3BzSu3IawNXCtPhv1aycZq1XDT2W25LeJuC6tsERdARopBOvMn251jKr+P+V2ybm5WZWJm3Vgs+2YDjvRGcONTTQhpiOehDHTRs56/lbKgCi0Wqp3LBVV2vc3Q8LAmHLtJlQh3rjFCfk3MsZrnPtxDVVkYxJBNBFUhrN+xD+edfOygTIbWvV0JggAAHnvtY1xz5iTHDiETu6ZUI9UEbvrqSbj/xZ2Ohcl80MKSKpaZlClubj+CI70RjCr1o75qVFrX0E3jtWq4yb4nlXZuX1hli8rNK7cCoIzqWKW7kLjthrLZScq+6wfnT05pNglFNSi2yKKBLnqy8y8LqLjzkvqMnp90d2iy3wOM0HAZEU1HwKdI82PMcx/uoaosDGLIJkJPSMPtq5vx81XbXTXDXDqbmnYfdn3dKgwytWtatUigvz77/S/ulPoA3B60QJp9AVY17cGNK5oS7NM+Bbj/iukpTWhu2nM65rJsdneyBEGVFNhL+6daLDNZSKwCaaA2artwkwm75vZO3LJyK0LRfnMfbDu+gS56svPXhMhIEGSyQ0tlJgv6FGi6jqBPhSYEFl48RZqYB/Sfey58fl5Vlx0MWBjEsIdb9sQ0DNP2mY5zdaB21+kuLRmtr2drznKrzy7zAUgztn0Knrvh7JSLslEddivsm4qoDvzwySZHToI1c9ca6ZSN3yXTbb5buQdN6IBIlAapFstsFhKvbdTm/FSIEiKSACCoEgQRghnmh7gx0IU0mx1astDwUFRHWUBBRBe4/ZIpuGrGRIws8eGWp7dJfQbmOAfi8/OyuuxgwMLAgmlT3bW/C4uf35kwwVRKNBl58SDXHTcS15xVmxBTf81ZtQkL8EDsmunWZ89WOzcjSQiJJggTTQBaVI8vvub1Mh33PoUQ1vofXrfzcWvanol2nixBcPG8afHxZbKwZbqQpHsv060b5OaIl0EKYe2Cs9MyvaX6rWzP34rUTJtihyYLDU9U5GJBD2taMLt+vLTSrltwgDkm69+prkkuq8vmAxYGMVKVe+4Ja7jj2X6T0cTKMk+cTXdd+nlcc+Yk12iidBc82QObyWKZ6YNtXj+fQuhL0jHLil9R0Nx+xLFo/ewv2wEBXHXmRNffMSNihBAo9fviC3a62mk6CYLZLGyZBA+kW97EOi8XnHci5s+oTblDlc1PABjhV6FDpG16s2ONGIrnmljuU7bBE3IzU+odmj00fP2Ofbh9dXOCIpdpDaZc9YVwqy5bqE7qfNcmKgjs9WZCUQEhBII+JaEJtrVuUVlA9czZVHfcSMxrqJE+rKbWLqs5ZGKv+bN880fYGvNHyD4LIKFej1m/BwCm1YxOOUmt18/NgacSYC8FZVw/4YhJB4A7n2121A8yGxSZ9ymiCUR1JNQImlk3FptunYXHr52BNQvOxsTKMun3dPZGHOUe7AmC5iLj1UPqdi8B47ze0KoAACAASURBVH607u1yzMv7XtyJL/5Hfx0nt1pJsvkZUAm3zD4JaxacnTIySla/yfpbPWENYU3gZ89sx/LNH2V9DaxzzX4tFs+bhsXzks91+/U87+RjHZnymTyXqWpPuZGuoiUrM1IoTmreGUAurUv9RrnnfUf6cMezzQmLnF9R0BPW8pZglirpyRFy95ftKAsYjrR7LjsFm26dFf+sve+CLDwxG62oLKji5q+chBKfgjc+PIi1b38KIhhOPZVAiuGMrq+qcISvAoBfdWpLsgZFCZ+JaVjTaka75oxYtT5N1+FXk7d2lJFL559hmjwdZqKj9X7IEu4Awx6eKhzSPj97I1EIAPe9sBP/8T87srJntx3qhUoywW2YYTK9FrLfss5N8/uy7SmRzXOZreae7u9ubD2AiE0YZFNexQtYGCB5uef6qlH4+artjvfMDNN81UVx2+66RQJZHeGbbp2FaTWjpYLD9FfI7Jlui6B0i68L+FUFtz/b7HBeCjLs1ebO5/ZLphimIevnhXAUd1uyztmgyEqyUg23PL0NVRUljiS6oA9YetWp8c5eqRb6XDr/lr/+UbwciCZEvNxHsoQ7E6EDr73fgbM+W+mqkZrzs7n9CL77WCNCUR0RzT2vIJU9u3pMqWMhA4yclXTNHNYMYNlvmXPTSqamp4H4LgYSXprqd82drd0E/eSbuxPKq+Qr0oiFAVJL9WTvZWsj9QpjGyo31QCJWo6b4LCiAFgf6/vg1gxHdv3Mhc0uCAAgqCoJNt2rZkwEBBIWRrtWtXzzx9IdhErAiIAv4b7ISjUIXeAbj2xOaMQDGIXfKkoDqCwPplzoc+n8S+gNEbtft69uRsBmMivxGzsYW0V0hDQdC574G645qzbl/Kwo9SOgKgm7Kru2m04ZicryIG6/pN7RREfTRVqLpTzXQf5bqUi1YGb7XA50Z5Hsd9sO9cYqxyaiUv955zPSiIVBjGRSPRtNI1/SfWPrAYfmYcWq5aRT0uBoRMdP//J2fCF2WwTt1yiZoJFpWledORGzp8qrn7q1LfUpwJ++eyb8PjWlozwkESTWsaSz0GebmGiPfmpuPyLtDRHVBKKScf7398/Bf2//FA++vBP24CAzKVFmXjFJpe1mUkbiqjMnAhQrZ6JS2tnFrXu7cNOKJkT0/jmUba7DYLa6TNXfOxOqx5Q6KscChqM83TnoJSwMLCST6ploGvmS7uZksmq/qkJQSSDo80m1RrsWNHdalaNKqNSmn0USFmAkA7ktHtZrbF1A2w71IqCqCEUT00cVUnD1o2/gnstOSTAt2M/L1EJlYZazTh4HID1bcTaJibLop4CqSq+pjCsaqlF33EjccNxIKAQsfmGn45im3Ycxr6Emfh7mNXC7HrJs80zKSFw1Y2JGZctXNe3Bj55ssq/9WeU6DNaCWVkezEmtMvt3Lp53Cm58amv8GfUpwOJ501x3tIMZacTCIMdkMlkzLY6WCtmCpukCikq47pwTHCGJgFyjf+7tT1yjgkwiuo6ygIqtuw/HHdH2B8e6ALmFRcqQFSCTCZawpgOa/Pratbs5SzZKf+u5t/fi5Xf247ZLnL8hC2PMJDHRreheVE9+ba08+aZRrntj6wH86iWnIACASZUjUiogbrvbbMtIJFOO7DuhW1ZudQgCAAABz92QWa6DPB8hfZ9FungldMz7YK2OnEzZsM9BLy0OLAxsuCU05ToaIRfF0ey4aeMRTWDpK62YP6PWtYSvdWzRJHYmU2O84vRqzFmyMSEvI2JxzN7y9DasWXA2ln2zAYCIO2hTIXsIF61tifsgFJCjpajM/m1NRGs7ZPRiuPPZFmm9pVBUx6I1LfjRlyfj3hfedfVbAP0Ps1s8e3N7JypKAylNZSZBnzM72D62R17dhUc3feAwEQGGz+RoRE9r4ZIt4LI5E9V1HDvKea9kiW2piv5df26dkTwGpwBccN6JGec6SHdnYQ3b2zsdjueBYO6wZK8PdBGuLA9Km12l8ld4bXEYlsIgnRrosoSmdC58KuluFnAzyyAMpDiaHXMy3bTSWWTOryhYvvlj/DrW6rMvkhjiaXcG3xyrZWPFpwC/ueo0VFWUYs6SjSmjXi568NWESqPpXD83YTq1qgKbbp2VEBljIrN/m+GURP2hozd9ZTLuff5dqYlG13Tc++JOo5hZLAt6Zt3Y+M7Hvqied/KxjiizvqiG7z7WiICqJt3RmKgE6ALwK5Au9Ca//d9dknzu2LXxKQBEUgXEnHMyoWxfgHojUegCuH753+L3bWbdWCzf/DGWrm+NlxKRhSDPrBvrEEpL1rdCCLm5cP6MWveTdqGyPIiFF09xOLHNLONcactlAdVhOuuL6LHe0sm184Fo77lqjJQNw04YJGt6IdvSZ1riN5l0j9eKiS36VrIpjibDWoPIuuiFLT1xTcz6RLc8vQ1Tjh8V3ynMnT4Bo0f48c9/2IJeywNR6vehojSAnrCWUuM1H6RwklBGGcmEqaFRjcPieant3/1jE/Fwyvtf3InbL6nHXWuc4a5hHYCux6ta3rG6GXc92xxf2GXVT684vTqhymxUExBA3Ldx15oWfGfmJDy66UME1EQFw7qbSoXbEWZv7fqqCtdrZi8aaBYMtNaEmlhZhjULzkZ7Z58jBPXGp7aCIGBugMz5IwtBXvbNBse8CKgKrjunDg+8vDM+Br9KWDwv+5ycqRMqUB5UHbk/qZ6VTMyyPWENQZUSAg+CKuG57Z/i16+0umrnudDeZTu4wchcHlbCIJl0TbWlz2SymVmwmdSKyaY4mht1x43EvZcndsi6/tw6LNuwS5q0JXSBix7amODI6+qLJggC+3j6ou52b5UAn5L4ICUz5dg11VQ9j02BZy/ZIatAasWvKJg6oQJ//fH5eOTVXfjN/+5yPdZcpM2F/aantmLK8aPiv9XRHcKKLYmOdvuiHYrq+P1fPwIg4j4bwHiwO3vDhvatJTrFS/yK6xyxctmpVbj01AlxTV+mgABwFA00CwZaa0KZn5l10rGO+ZGOsAKMawsIx7zoi2qYP6MW82fUJuxOAEh3Xekgy3dI9ayYRQlVxYiAWjwvuVm2ekwpSKHEiCciLI21IXXLw8lWe0+1mxhI/kO6DCthkEy6pgqzDEW1hNIUdlJpBG7CZkRAhR6zTwPuxdEy3Xrat5sAsDRmInKcmyYAiLhp6aantoIkmaY/umByfNILSWasiU81FgYrbqYc+7Xq6A45opn+9GYbrjlzEsaUBYyiZHs6cVesKJn1wZZVILWPoSxgNJQ/67OV+MPrH6V0lJuENUNg3jvPGGs6/gAAcf+G6bMxtb6O7pBjvgV9CpZ9swGvvX8gqaDyq4Q1b3+CF1r2JVw/u3lh6+7DMeGYeC80YSSs2Reu57Z/mta1kBGKahjhVx3zwvy7sjyI+qpRaG7vxMOv7sJ/bvrAddcFJNfit+/pTAif9imQ+nes33WTJYIHMHY8CsF1YZcJ2H6FSl7oMVvtPZ3dxEDzH9JhWAmDVCYI68U2t/QqGRquohDmLNnoOnFTaQSy3w76CL+92rDBx1sTSuLFs9162reb9p4GQZUgAEfYZVgTCKhOYXDv8+9i/KgSTKwsQ6nfFzeh2TFMAydgqW07nUx7Ms1Unb0RxwMVjur46q82QFEIQZ/iWMCtD7YVlYxzM30GV5xejYsf2ghVIUQ1XWp+CSgxk5GEsKUMhOx++hSCQoYwPGqL1xe6cCwK159bhyXr34NPVRDRBG67ZArOmTwO9VWj8OimD10Fm64LRET/rsW+iJn1fsoCqut3HOjuS0uYKYDjiKBPwde/YPgMzHLQikKY/7s3oBIharmypX4f2g71YmPrAceCLBs/IC/Od0xZAIvWtkhzIVRFwcy6sa7n0Nze6djhRDSBUn+i0qIQobm9M+7cTUehSpW3k0p7z2Q3kYuWuskYVsIglXS1X+xDPWFc9NBGAMLRIs96I9LRCNx++9DRCK77w5ZByXqVJdOUBVRc/JAz7NJeTMt4TcSjhJLtoiK6jvkzanHh1PEOU47sWlnNVGbjdjuaADRNICLJrpY92GYF0vqqivj5XvjgqwmLAsFY2AKqseh84wu1WP7Gx3C30ieWMrcLV4KApgMXff5YPNP0ScLnQpqI7yyti50ugN6whqBPwV3PtmBk0Ie50ydg8bxTcFOsdIaVUp8CUihB2FijmLbv6Yxnioc1DbJq4n6VcHbdOPziuR2u52mcqyFM9dgYVAL+9cuT4zuca86c5Hg+7PRGoigLqLhl5TZXk5Pd2W2f7/e9KA+pNQmoiZ93LpZy06E9au5o2AgAWDxvWkJAhUyhcsv2zlR7z3Q3kSykd6AMK2EApJau1ovddqjXWKCSpPED6WsE9qJkADDz7nUZZ70OxHFkn0yte7tw7uSxeL5lX8JxQZWgw2kzNougLbx4Srx8RCiqJUTt3HPZKdjYegA3P9UEggIBHfdePt21Q5XdTOVXCQqJpJnUduwPtlmB1DzfDTv3Oc5FwCic96WTj8XLO/ZhxZa2lDZyeynzNQvOjjcMMtfDZ5o+cazBJX4lnskqC1Qw/TM3PrUVM+vGYu70CaiqKMGVj2xOyErWhADZ1l0zismv9u+akmn83591IsaUBaC7XOCygIqorjsc3JoAjikLxOeP4WRVpOG6JkSE9k55GQYT67OSrvlN9nm3HXR91SjH/SAAd8ytx13PtiQGVUSdARVu+Suy9SPTtqaD4QtIl2EnDID0pWuyG2W/wak0gnRrzqeT9ZqryXLbM287ei6bkEJ44jtnYP4jmxO001BUw+ZdHbj/pf4wzDsvnZqQkQoADf/3pdjDZ4z9B39qcnSocssO9sdMVHqazkufYjzYZnOTiGYUfUu8x27aYea2cnPBNSNozKY8VmQjTyf3IKIJNLd34tDRSLw/hFUY6EJg/hk18bDOsKYjqukxs1Fq/0fQR5g/o9ZQdPyqw5xV4ldw59x6HDsqiP+z/C3HTuy2Vdsxe6oRwplOOZMSnwqApGUYzPFYn5V0vtOk1G/srO657BQc6gnj5lhItV2xAgCfSgmCzacSZtePR82YEfjeH7Yk5K7IAiqS9dE2sa4JZs5DOj2581X92M6wFAbp4naj3DpzZVJW2s3ckizrNZeTpXVvl1QQjPAr0GE8YA2fqYxHJZm2YSLCv/13onnBjPE2H4A1W9sdi6FAf5XNiZVlePw7Z+DDjqOYVDkCVz/6RsKxR8M6gj6nE1qGXyXcd3lsW28pdrdoTb/JBQDqq0bBp8DRjtONgGrc26BKABHmnVaNlW+1SWzwQmpSA4wFp9SyWwKAzt5I0kgsADjSG3WNPIvqwIrGNqxZcDae2/4pHnp5pzy7V4JK/aUPAEgXaBHrWwwAYUkynCaMqK1zJo9zzE3TxGe9RBHdqP57m6QyrUrA2hu+lJB4lizPxfo5IsQ66ulo/Oig1KRmKlaAIZSskVslPiOQoL5qlKOwo32nmo5ZVrboy/IuUmXLD3YtMyueCwMimg3gAQAqgEeEEP9he/9bABYD2BN7aYkQ4hGvx5UuMieSadoxsXbmkt1ImTaoxLbP6Sz0mUwWty2p/fWmWEMRO9d+6QT84xcnJfhRphw/Km4bljkk7buZA9190u9ev2Mvblq51dGDVlYPKVlUkJWbv3pSPE9k0doWhDURf7hvXrkVo0f44+ai+WfUuu6ErAR9Ch6+pgFVFSVxU8GhnnDMn9BPX0THCL+Kq2dMxH+95mzw8v+uOg2HjkYwvWY0mj85gpl3r4NPoaSmKNNvnyrMub2zF79+pTVpsprjuxWKa8qV5cnr5ADAP509SRrVtGt/d7yMgn1uPvDSzoRrbIYFd3SHHd8zIuBzOIOB/vl+3wvv4o9v7E54r9SnIKoLRHQRL+thDUO2EtZ07D54FDKBbVW6jMg593uSyizrpuzJ8i7cvstLX0C6eCoMiEgFsBTABQDaALxJRKuFEC22Q58UQizwciwDwXqjtu4+7NqZy9w+25FtfY+GNVz7+0bcMOtErEmjF206k8VtSyp7fVLlCOl3EJw9X1PZhu27mbPrxgF4x3Hcs9vaYX32TYH6TFM7RvgVHLWZiuyLpkqOQpe4/8WduOy0aqnADUUFvvf4W9BjvQKetAkcwDBTfP0LNY5s2nMmj0NHdwg94V7LNUjMnVAAzH/kDQR9ClSihA5bX6qrxPVP/M1VY7Zj7lp8CuFHK/4GTSS3sQOUsW1dCIH1O/ZhUuUIfNhxFNNrRuP1n5wvrZMDGIrBw6/ucox78fPvJjTIsUYx2XMvVjQaYcFL1zvDmsOa5mrurCwP4savnISn32pL2CGEND0tX5JPIYSjRplvAFAI0kZGW3cfduwa7KQyyza3d0KxhWKbeReF2tVMhtc7gzMAtAohdgEAEf0JwKUA7MKgaKgeU5q0MxfgrGckS6QCjOic+17ciSXr38PiedNca6uk2xDdLWRTrrWcbtijbU/Wb/73fVwT2xlY6xjJK5ASiMixm9n8wUGHw+7LJ4/DX98/KO214FcJEduKoxAhoIoE4SFTqFPliZg28dtXNTvO1drz+AfnT064xo5ieXOmOJKQdBjap/nAB30KfnLhyZhaNQpXP/pGWk1qTMxv7Rc27ivewounoL5qVFLbup+AiO0rIrqRZGW9jtecVYu7Lv289Dsqy4O4/4rpuHnlNijU7+S2FuazOlrdFsWm3Ycd/RQAozZRMgXH2L1Mw48sGdTpCAK/CkCIhCuoC2OxszYyAuSKmhHAgIQ8CLdxGslsTpNWRNex+2BvQmScX3U+K4WE18JgAgDrPq8NwAzJcZcR0TkAdgL4oRBit+SYQUe2CFeWB107c23f04mvL3tNWurCbgaxYkYwyDpP2WvCWL/TOja3qKOm3YcdrQoNrYWM5iK2p8sUavbs1OnVFXj9g0MJxwoBrL3h7ASbr7Vpi4lPATa8d8C1bHNUE/insz+DR22JSABwY6wGvhvWPBF7eYiE35DVkdeNiCOjdo+hHbvVlV+0pgULL54iLWVhYm4Yj0ZSl+uwE1QVHE3DcToioGLqhMSsY1nsPSmEIDnLnthvgdkPwa1gXLLCfEB//aneiKFZ23cREV3H9JrR0gQ7a20iN4VnZt1YqIqCaJJrc9Hnj8PL7+yDSgo0oWPBeSfiN6+8j4itOqxPVeKNjExkfo8F59XhwqnjU+7WzXlinw9BnxIvrGi9HgohaT5EvikEB/KzAJ4QQoSI6J8B/B7ALPtBRHQdgOsAoLa21v52zkkWBSDrzCVrWZhuqQvAaUu0axwhizOrqy/q6Do2s26s1Bnd0ROWNiyprxrlKtRkLQntggAwiqRZv7ujOyRv2qIDMk3X6OIloOk6Hn/9YwCEq8+sxVmfrUR9VQU2th6AkAXKx7BGonR0h/DEm5npEAvOq3MkQ/kU4AfnT5YXy5tQgYevacD3Hn/LEYUDGJrzHc+2QAGgKInvqQrg4mcGgAQTWTIimh7PV7Da67e3d2LRmv45sfDiKVi0Nr0NeNPuw6g7biRa93Y58kKAJIX5bPWn7Ji9K+qOG4mGiWOwsbUj/t6pNRXxnfT/bP/U0eXO2ndZtquw/sbMz47Dy+/sj+VUEI4pDxjlXWzYO7LZy8cYitd7WLZhF5a+0urok5GOAgYA35k5CVOrKiR1mtRB602QDV4Lgz0Aaix/V6PfUQwAEEJ0WP58BMA9si8SQiwDsAwAGhoaMohAz5x0Er3snblSlbowNSc37JVNZRoHYFTXvHNNiyOEbtOts5ytJ10WhIUXG2GXs+vHo+3gUTyycRcCqhp/ENMpRAcYYZDWh6vtUC/8sQSuVPgU4NtfnIRHN32AkNZfEPA3/7sLj732UTzO3a2cdomPcO/l01BzTFlcs5c5Zs3wQ7vN3q8SzjrhGFz1uzcSPhfVgQdf3gnFtpqb96d6TKk0Kc6KDkDXE1tyGruKlrQd425ENIELH3w1HkFl2uun1Yx2NJwZWeKTVrC1M71mtCPM2G4+coQEazpICNcOckFVwX2XT8OcaVVo3duVIAgAQ7mY//Br6Ivo8d2KOW8cfZeTXO8bL5iMRWsTr+uiNS24bU49bl+9PaE43m1zpsQFkH3nu3DOFPz6lVaEokKaGe0WLSSLJHt00we47LRqx7jDmo7O3khOuqZ5gdfC4E0AJxLRZ2AIgSsBzLceQETHCyHMdM25kHkeBxlpAw1Je0O7UzdZmKg9YoHIMLOYWIuxJdtJhHWgzEewxmaYGahm9Ulze9t2qNdhIir1K6g5JjFBRyEF3/u7z8YzS2V1c2R8bXqVI/lOk9Qskjl+ozrw8Ku7EPQpCNk0f1mEiZ2oLnDjU9vi5rMrG2qkxy04rw5XnlGLTa0HErKFDefvZsjyDyI68OXJldj4foc0ymvBeSemzIoFjDIJS686Le6YPdgTTutzqYhowlE4D3DORzMSzJ55beejjh6HeU1mPpI2DHL53pCm48antkIXwlUY9bjU/YhGdUf4qix0FAA+7Ohx3cVt/umX48Xxdh/sTcjMtvffuPPZFvhdejIDkCqHm26dhQXn1TnuaUA1svvt5W00Xcf1y9/Kupqp1yipD8keIUQUwAIAz8NY5FcIIZqJ6C4imhs77PtE1ExEWwF8H8C3vBxTOkgbaMTaG868ex1WN+1xfMactCV+BSODPpT4+9s7th3qjSXf9GNfM1c0tqGjO+T6+yYB1WkHNjNQr35kM+Ys2YiPOnpQWR7E9j2djoW1N6Lj2t834sYVTeiL6OgKRRGK6lj6SisO9YSxNRZyuvDiKQiohLKAihK/gisaqh1jeaqxDRt27ouP23SUW7miYQJ++fXpCPicUy2quy8IbpT4+h3Xoagx/r6ILg3tBIAHXm7F/zR/irnTJ2DNgrPjWbchTcTCUOW//9KO/fjRBZPx+LUzsOnWWQkP7vwZtQioqR8dn0rYd6Q/zHb+jFpISj5lRVgTuOjBVxPmYkd3CBt27sOGnfvj96TuuJG47/JpSDbc7z62Rfr6r17aia27D8e/C0BcWzez0M35rpJTrIZitZzcItfciArgu481xs9t7vQJeO77X4onI1p5akubox2qveR5fVUFFq1tscx34RCOBDgqoZrRTm2Heh0RhKagmD+jFkGfvNrw3OkTsOnWWVh61alQyJjv5ny95eltCde1EPDcZyCEeA7Ac7bXbrP8+ycAfuL1ODIhwTmXRntDILEuvN3xlE5WpdVnkCzxxq4dqUQQQiCkWcotr9yGqooSV5uxTMMSApj9wAYEfWq8vETQ0uilosTvcIILANc9tgUgxLfN9mNWNbXj1tmfw3M3nI2v/mqDa4LUiICCoykEg18l/PjCzwEQWPz8TmmdIue56vE8kJpjRjiqaprri2xc976wE6/9eFb8Plptxm4x+FaOhhP7I8+sGwtFIWjpZomlPLf+wAOZ78PsWzB3+gR82tnnSBY0cbvqa97+FK/s3I+o3m/Hl0VZdXSHHb0yTFSF4PepuOasWtd8ABmmIDGfs7rjRuKqM2odQj+iCaiUaJKzR+yk47MLRXV864sT8YfXPorPBV0Am1oPoKsv6iiOaBU4i+dNc80VqiwPoqI04OjhneteBLmgEBzIBUmq9oZ2Z++tsaiOiKbj9kvqXRu0A5Bmltrjj83f/+Pmj7FkfSt8ar9QsqIJgRE+NaGvbjiq48qHX4dq92ImwXyQo/Hz7C8Kt2hNC244r076uT6LY3vZNxsgbDb+UFTgj5s/xvwZta4LYVnQ6Ln7xocHE4SJPdgpogn837UtKPGraZmSrNy2ajtURXFEJmnC6DQmDVtVKW4msEZ1hTUdF3zO2bbQxEeGdgv0KxA3r9yGn1x4smHWSiLEvnzyOJz/ueOwbsc+vPTOvpQ52KaJ0F4ILqoDNz7VhCnHj8KYsgDud+mfnApr6Q1ZmPJdz7bASEaUjzQcNRzed136eVxz5iQ07T6Mg0fDuP/FnfArCrpDUddztD5nyUqnmKd99Zm1uPZLJziUtN0Hj6b02QFGv2nNNt9uXrkNsgAGa7mTVEmhhVR/KBksDJLgFkUhc/YmZCQ/sx2gWNRRDNN++9VfbXD8jhmKZi48Vq3ihvONJvLrd+zDT//8NsISh6q9JzBgLAbJwvFM0inR4FcUTEyx1fcrCo70RqQOxYfWvYdpNaNdk3vCUSP80H6dZb7jqI4ELc2erOaGUfXULWlO/plQRMP2PZ24/LebHJ2+1rztXs9Iti6Gojp+sbYlZcbwSzv246Ud+5MfZMFMQJMVgotowEUPbsQNs+ocvqNM8SsKnm/+1LEuqgrFmjLJBZwCJJR+N30Ql51Wjeb2zlhnNbk4MJ8zt9Ipdn636UNc+6UT4n+vatrjKJsNSAu5AgB6Jc+RQoCuJ36iLKBiaiwk2S4AZO1FC6n+UDJYGKQg1Y2UOWkBwyFl78na3tkn1UCv/EIN7nq2OR4nbS2ha47hvJOPBSj3QVTpBLeENQ1nfXZsvO+tjIiuY1SpP645W1GIsGt/l2shNSEE2jt7HX6UVJQFVdz8lZPQ+OHBpItztkR14OfPbE+jQlJ6ZFI6IhWBWIGehXOMBDS3QnBhTccvX9qZMllr1knjsO5ddyF0NKxh8QvO3YXxu+5fbrZWvXnlVkcl0IrSgPSjZsMnU0FatuH95IOPYe7kzCAIt7LZmdxPe7c/QJ5TdEVDNf64+eOE6KV4zSwUTv2hZLAwSINkN1LWgg9InJj9yKfhH17/KPawGovlj1Y0OXwSleVB3Hv5dPzgT03SbxkRMMpFDDByUcqC807ExtYDWL3VKBsR1nRc9Pnj8ULL3gQBWV81CjIFtC+q49//W96IHjAaoOw51Jtx2GU4quPf//sd+NNw5mbLQAWBLMs7GxQCSvwqorrAhfXHYe3bn0ABcOfqZowM+rB43in41yebpIt+qp/3K8A5k8diw3v7pfPHUCs5uQAAFUBJREFUSLx2fklAJSw4rw7HlAfiFWNlpkzAMBd+9VcbEuz6U44fJd1J3nbx56AJwzwpS6hzw5pH0HYoednsVPgVufAO+igesm01l9n9IaaJyfocF0L9oWSwMEgTtxtpZCTXO7Ju7QkuAFBfVeGoueMWdmmG1lkxhdILzZ9ioa28gi6Ai085HqssTVWSl99Kj6BPwVknHINvPPI6rLvoF1r2Sp3lbotSsoW+JxzFbau3u75vhQCUBlREorqlAX1m/oPBRORoX/GPZ03E106tRllAxVd+tSF2fY3v/tcnm7DiujOTdH9OTkQH7nr2HVf3qpswCWsi3vlr4cVTEPQp0ixlE03055OYJVFkNagWrm6GECKlYmN9duylHsoCqlRJSxsilAUoIdrNLF9SURpIKw9HVWQKYeHiaWjpcOGqMyfiF38/FQGfgrKgmhBWaqWyPIj7Lp+GoE/BiICKoE/BdX93gsu39j8gZhtDMxRtSlUFbvnqSQmhn3OnJQoCwMh69Wd5h83z+HpDNa58eDMc5lRhmL3M0DtzbDPrxmasqRsx3+kdK2CYLCJ6rpZZj8nRIE+feAym1YzGjk+POBZnXQBXLHs97VLWMrJdNvsiOvoiOhatbcH0mtHSHYQMv6Jgz6FeqRknoqUWBD6b099a6mFV0x7MWbJxQDuDgE9xjM1smGTUJ0vDTyVRCAsZ3hlkid15dNWMiY4MUBmyktiPvPpBYuMNBTjSG8GGnfsSkmXsfZmDPgURXeCmCybj3hfedfxWVAe+9cWJWL7545Rlk61vlwWM6J7pNaNx8UMbpWaOvqiOb/3nG1CI4FOMXsqL5xkNe5KVDxgqqIp7wxYrOYoixahSHzq6QzggKQUNpFfAzUvMDnjJ6iVZ6Ytqae8GZQTUxHpFZqkHAK69IBI/b94/kgowTRe4/ZJ6R9kXMxs5mkIY+FXC4nmF5yROBguDLHCrWyQzJbkVu7Med9/l03DzSiOnIRQxsiMXPNGU8D3WLakZjGcuuPe+uFPabQsAnnhjN649+zNYtmGX68Jkf70vouHYUUH84fWPki7qujC0JVNY/HDFVjz53RkFbbbJFekIglzhUwjXL/9bvE9zIRKKGv203eol9UU16LpASUBFVBOIanpaglJmRgWMDGcrZuSRLKegxK9A1wVURYlHDJnPStAHPHD5qWhu78Sjmz6MB0Bcf24dZk8dn1ByxnRM2yu/mpgBDSeMK0uIJioWyJ6EUww0NDSIxsbGvPx2R3fI0dymxK9g061Gbb1kZZCTpaAb9XWO4Nrfv+nqaHWjLKgiHNVcTS0j/EZPW00XaWuqQV922n2qgmyMOyoBnxs/Ets/6UpxnFybzSdmdJPZ9Q9AvGbUX9/vwH9u+gA+RUFE0/GlE8fi5TTDZ912YD+YVYf/9+ouaYVg2fP5oy9Pxj3P73CYn0YGfXj82hmYVjPaUiX4PQRUFWFNw4LzToyXaQGMfiZXP7I57vuwYq4DhSwEiGiLEKJB9h7vDDLE3IraWb75Y/z6ldb45PzRBZNx7ws7pT1Z3RzRFaV+w86ZoTDoC+vSKB4TWR5CKrI18+RCEOTC8T1YKMje3u74LkJKQQAYkT1+lXDJ58fjL02fFMS1CsdaRZrZ3uUlvnhrWDMvJBSLlktXEADyHdhptRX44VdOwjVfnCQ1y15/bh2WWMq+y8pJm4QsVWABOIrVGf1GWrF4niFs3KoJuOUKFRMsDDKkLKA67JF9ER1L1u1EWOs35/zbc87U/1Qp6NVjSl3DEN1C3YBY2J/tY9lq9vnGR0Yt/mQ+DpMvTBqNL0w6Bo+8+kHGu6lckUtBMHfaBDz9N2fdKxkRTeDPtoABK/kUqEa2t9xsmQve3nMED738HubPqE3I9LfuxAGB6845AfNn1KLtUK8jM96EhIgnxU2sLJMeE4rquGnltnhhwMT+B8bu4ZiyQEIJ8UIsRJcKjibKELP9oRW/SvApqssn+umLurf5Awxt4o659dL3Mk1Y0tPIPi5EBNJ3hm5rO4J/OLU6VhG2OAmoRsDAvNOr8ew298U9UwSyjyQbKJqQ17/KFZFYh8Av/sfL8WJ21koAZjE6M+y1LKC6ltoOaSJeOC4S1Vwdz+GoHi8MaBage/zaGfjrj8/H/Bm1CYXwCrUQXSpYGGRI9ZhSo/2hBYVI2kzDjhAiXhnUbaJcNWMifnrhyQMep3VO+1V5uYJCRBPpO2dDUR2/eaUVN14w2eNReUdYM6K+VjS2pRWumAm5zHj2mtNrKwy/QwaEokZilxmk4bfV4lJAaG4/gp6whpIUklElwob3DiCQ5DCzMKDZj2BazeiEJjf271u/Y19RCQQWBhkiK1W9eN4pWDxvGkr8Rp6BGyoRLnpoI65+ZLNrKeyO7hCOKQuknLwy3B6miCYGNfplMHn6b+2u1TiZ4mHLx51Z7SZCUR1/3Pyx1JZ/NGKUdt/e3pnye3rCGn638QOkqqhu7XFgIi15H9Zwx7PuJe8LEY4myhBrk/j2zl4AFG9eYr63fU9nWl2t7NEH1uqnmVTlDKiEf/m7z2LpK+/npPTBYFBMTmKmsAn6CH/98fnY1HoAP3yyyRF/YUYTZao0KADIFh3nFjG0ummPa35FIUUZcTRRjrA6qMwEsFK/z5FrUD2mFDXHlOK19zviTd5DUQ2KQgk2SatDWVb9NF3CmsCD61qLanEtprEyhY2ZcDbl+FGG/8jRs4JwTFkAZYHMSp/7fQpuu2SKwzEsW9Rn1o3Fsm+ejl37e3DvC+8mVNYtxN4FMlgYpImsLzKQWGvFbDJizS247ZJ61IwZgSO9Edz4VGIimbUUtlsDjqBKiOhGW3ifQq6OMF5cmeFKbySKde/sxW/+V74zjmg6DvaEM+6BEVAVTK2qwKZbZyWtLGBVEsOa7uiRXYi9C2SwMEiTVN2S/IqC197vwM2xXq3mcbfHmqoEVKMxu08xqnSGNQ3Xn9vfMMYtftlc/AMq5SyMkWEKiYHmakR14IF1ra7v/8u5n5WWa0mFtZuZm1YvUxL9KiHoM3Yshdq7QAY7kNMkVevK3kgUP1rR5HCCRXXEe/VGNCMl/qozawEQlm3YFXcwWR3TI/xOJ7RK5MnNylVPXobJFq+VnN+88r7juSxRydHX2I61m5kbskiiEp+Kh69pkPbPLmSGpTCwVwFNh8ryIBbOmRKvTGrkFhjp7GaD9nSiIXwq4dGNHyQ0czfD1cz45d9+83QEbQ3ke6O6q4loIOQpV4thBg1pz29CynDrgy5FAa24tbSsr6qIh54WC8NOGKxq2oOZd69LGt7p9jnDkUSIRHXccUk9Nv/0y3j82hl4+JoGlPic2rxPMbaMViKacJR4NuueA4bQOWfyONw2Z0rG58ZKPsOkRlWArzfUpOx38NC699C6tyup4igLNS8Ws5CdYeUzkNn3ktULkn3OZNHaFsyeOj5e4EpmQlIIuPKMGqxobIs7lM06KVZ6QkavXWtq/dQJFSgPqglRCalgJZ9hUqPrwBNv7k6Z6R7WBGY/8CpK/aqjxIS1GrG1UqsZsWQmphUTw0oYyJzA6YR9pfqcqR3cvHJbQm5BWDMyS+0dwXr6oo6Y57vWtKDmmBHxnIVkdYoYhskeAaRV+woAorqQRgzeEis5r+kiXsTOHklYbPWJhpWZyM2+lyrsS/a5sKahszcc3z7OnT4BD1/T4HD+qoqRhGLaD1c17cG9Lzobi4eiOv75sUbM+MVLuPHJv+FQTxj3XHaKw8yULWxCYpiB4VcUNLd34qantiIU1XE0rCEU1XHjU1vRurcroTZSMdYn8lwYENFsInqXiFqJ6MeS94NE9GTs/c1ENMmrsWRr37N/zqcYxdSuX/63BL9DfdUo6DZjjWkCAvrNTWGXzOTeqI6oMEosfPmXG7CxdT9u+spJOThzNiExzECJ6DqO9EYcu4qIJrCxdb8jqkhWuqKQ8dRMREQqgKUALgDQBuBNIlothLAazf8JwCEhRB0RXQngbgBf92pM9raT6dr1zM81tx/Bdx9rRCiqI6Ilbh/NiKOf/SWxnZ/pX0iVq2BnReMeDv1kmAIg6DMUx1GlAen7Y8tLsrI6FBJe7wzOANAqhNglhAgD+BOAS23HXArg97F/rwRwPnlck9hacTDTz1WU+hFQ3TWAqVUVCc0yrO+nylWQwaGfDJNfSnwKHr6mAXOnT0B91SjYor7hU4CzPltZ9FFFXjuQJwDYbfm7DcAMt2OEEFEi6gRQCeCA9SAiug7AdQBQW5u/PrCp/A7VY0odLQmtmYyGo3krQlFe5RmmKCDDBAwYCuH9V0zHzSu3QiUFmtCxeN40VJYHs7Y6FApF40AWQiwTQjQIIRrGjRuXt3Gk8juken/u9An464/Px40XTEbQ13/MFQ3V0t/zKZwlzDBe41cJZQEVAZUwd9rxCMT+lmn45jP8xHVn4q8/Pj8hYihbq0Mh4GkJayI6C8AdQoivxv7+CQAIIf7dcszzsWNeIyIfgE8BjBNJBpbPEtYm1jhj2Y1P9b7smI7uEB5a9x4ef/1jBHwKdCFwz2WnxH0VgMCOT7tw7ws7oevCEXpaPTqItsOJ0QslPgV9Eof17CnHIaTpWP9u+v1oM4FLVDMD4YqGCVjR6J4Q6lcARSFcOr0Kz/ytHRACYd1IKFOJcPsl9TjYE8YDL78Xf04I/e+HNIGgT4EAcPslUzC7frzjWSxWDT8ZyUpYey0MfAB2AjgfwB4AbwKYL4RothxzPYDPCyG+F3Mg/4MQ4opk31sIwsBL0hU0kaiGzR8cBAB8tX486o4bida9XWjafRjTa0ZjTFkg/j2HesLY2HoAY8sDOOuz/Ul25vGTKkfA71Nj39mBA7FU/L5IFKqi4ryTxqE3ouOjjh5MrByB8aNKsL39CKKajj2He9HVF0XDxDG4oH48ACSMr3VfF3ojOk46tgyfdoVx1gnH4JjyErQfOordh46iN6yhNKAiqgm8t68LEyvLMK26AhtbOwAITK8ejR17uxHVdYQiGg50hQES6A1rOH3iMZg0tgxvfNCBlvYjKAv6UDeuHAd6wlAIONhjnMfEyjKU+BV0dIcR9CmYVjMa4ytKABA+6ezF1t2HcbA7DFUhNEwag2NHlWBb22G8+2kXesNRjCoN4KTjRuLQ0TA6esLo6gsDIBxTFsTUCaNw/KgSPN+8Fwe6+4ybREZBwuMrgohoRuP1oEqoLA9iTFkANWNGYGSJH119ERzsCaGrN4o3PuyArhPmnT4BPlXByzv2oS+soSygYk9nLz5TWYYD3SF8dPAoVFXByIAP9VWjcLg3jAPdYYSjOjQh8JXPHYuA34ej4SiqE34njIBPQfvhXnT1RVAzZgRCmo6+cBQChMNHw/iw4ygCKuHUmjEYWepDZ28ULe1H4FOA40eXoi+iYX9XCDXHjEBFiR8793WjotSwNnf2Gt/ZE45i75E+HOgKY/Jx5dCEQInfhwunjoffp+BIbwSt+7rR+OFB+FTCZ8aWY3SpH/u7Q6ivqsBX6sfHF+TX3u/ARx09CPgUhKM6JlaOwMnjRyXk7Vh7jFhfN5+V5vZOmH1HzLkpO3Y4kDdhEPvxiwD8CoAK4FEhxC+I6C4AjUKI1URUAuAPAE4FcBDAlUKIXcm+c6gLA4ZhGC/Ia3MbIcRzAJ6zvXab5d99AC73ehwMwzCMO0XjQGYYhmG8g4UBwzAMw8KAYRiGYWHAMAzDYBCiibyAiPYD+Cjf4/CAsbBlXg8xhvr5AUP/HPn8ipuJQghp1m5RCoOhChE1uoV9DQWG+vkBQ/8c+fyGLmwmYhiGYVgYMAzDMCwMCo1l+R6Axwz18wOG/jny+Q1R2GfAMAzD8M6AYRiGYWHAMAzDgIVBQUFEi4hoGxE1EdELRFSV7zHlGiJaTEQ7Yuf5FyIane8x5RIiupyImolIJ6IhFaJIRLOJ6F0iaiWiH+d7PLmEiB4lon1EtD310UMTFgaFxWIhxClCiOkA1gC4LdUHipAXAUwVQpwCo9fFT/I8nlyzHcA/ANiQ74HkEiJSASwFcCGAKQC+QURT8juqnPJfAGbnexD5hIVBASGEOGL5swxDsFmYEOIFIUQ09ufrAOT9PosUIcQ7Qoh38z0ODzgDQKsQYpcQIgzgTwAuzfOYcoYQYgOMfirDFs/7GTCZQUS/AHANgE4A5+V5OF7zHQBP5nsQTFpMALDb8ncbgBl5GgvjASwMBhkiegnAeMlbPxNCrBJC/AzAz2L9ohcAuH1QB5gDUp1j7JifAYgCWD6YY8sF6ZwfwxQbLAwGGSHEl9M8dDmMDnFFJwxSnSMRfQvAHADniyJMdMngHg4l9gCosfxdHXuNGSKwz6CAIKITLX9eCmBHvsbiFUQ0G8AtAOYKIY7mezxM2rwJ4EQi+gwRBQBcCWB1nsfE5BDOQC4giOhpACcB0GGU6P6eEGJIaV9E1AogCKAj9tLrQojv5XFIOYWI/h7AQwDGATgMoEkI8dX8jio3ENFFAH4FQAXwqBDiF3keUs4goicAnAujhPVeALcLIX6X10ENMiwMGIZhGDYTMQzDMCwMGIZhGLAwYBiGYcDCgGEYhgELA4ZhGAYsDBhGChGNJqL/Mwi/87UhVvCNKVJYGDCMnNEA0hYGZJDN8/Q1GFVAGSavcJ4Bw0ggIrMq57sA1gM4BcAYAH4APxdCrCKiSQCeB7AZwOkALoJRZPBqAPthFHbbIsT/b+/uWaOKoigMv0sEhxARRLGwsBDrBFQEBUFrrYKVhYXoPxD8BVbWFunsBG1EwV4IDIgRDVYiaisIfhOw2BbnJKJcmRRhRuF9YODCnftVLc7suXvXzSSHaS2g9wPfgSvAXlqr8k/9s1RVr6f0iNJv7E0kDbtOm7uwmGQnMFdVn5PsA8ZJNloxHAEuVdU4yXFgCVighcYq8LR/b5n2RvmrJCeAW1V1tp/nYVXdm+bDSX8yDKTJAtxIcprWKuQgcKDve1dV4759CrhfVevAepIHAEnmgZPA3SQb59w1rZuXtsIwkCa7SPt552hV/UjyFhj1fd+2cPwO4GOfYCf9kywgS8O+ALv79h7gfQ+CM8ChvxyzApxPMuqrgXOwOcHuTZILsFlsXhi4jjQzhoE0oKo+ACt9QPoicCzJGq1APNhavKqe0No6vwAeAWu0wjC01cXlJM+Bl/waGXkHuJbkWS8ySzPhv4mkbZRkvqq+JpkDHgNXq2p11vclTWLNQNpey/0lshFw2yDQ/8KVgSTJmoEkyTCQJGEYSJIwDCRJGAaSJOAnl9r9rVf/nFoAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"PvRi_JQgwcKI","colab":{"base_uri":"https://localhost:8080/","height":807},"executionInfo":{"status":"ok","timestamp":1627830440268,"user_tz":-540,"elapsed":18,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"6ac676de-64ad-4e8b-9e49-9d32bbf7955c"},"source":["# 二乗誤差が2.0を超える列\n","thr_ = 2.0 \n","train_kf_df[train_kf_df['diff_sq'] > thr_]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>url_legal</th>\n","      <th>license</th>\n","      <th>excerpt</th>\n","      <th>target</th>\n","      <th>standard_error</th>\n","      <th>kfold</th>\n","      <th>bins_tg</th>\n","      <th>bins_std</th>\n","      <th>bins</th>\n","      <th>pred</th>\n","      <th>diff_sq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>141</th>\n","      <td>bcd734621</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Midas was enjoying himself in his treasure-roo...</td>\n","      <td>0.943021</td>\n","      <td>0.537713</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>105</td>\n","      <td>-0.802351</td>\n","      <td>3.046322</td>\n","    </tr>\n","    <tr>\n","      <th>990</th>\n","      <td>afeb324bd</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>On the morning of the 20th of March, a long se...</td>\n","      <td>0.401053</td>\n","      <td>0.481889</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>92</td>\n","      <td>-1.348900</td>\n","      <td>3.062335</td>\n","    </tr>\n","    <tr>\n","      <th>1314</th>\n","      <td>85b41606e</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>As soon as the plate is dry, a positive cliché...</td>\n","      <td>-3.543987</td>\n","      <td>0.609348</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>-2.000015</td>\n","      <td>2.383851</td>\n","    </tr>\n","    <tr>\n","      <th>1412</th>\n","      <td>8f35441e3</td>\n","      <td>https://www.africanstorybook.org/#</td>\n","      <td>CC BY 4.0</td>\n","      <td>Every day, Emeka's father took him to school i...</td>\n","      <td>1.583847</td>\n","      <td>0.624776</td>\n","      <td>1</td>\n","      <td>11</td>\n","      <td>10</td>\n","      <td>1110</td>\n","      <td>0.071463</td>\n","      <td>2.287303</td>\n","    </tr>\n","    <tr>\n","      <th>1628</th>\n","      <td>4cf4a2fa3</td>\n","      <td>https://kids.frontiersin.org/article/10.3389/f...</td>\n","      <td>CC BY 4.0</td>\n","      <td>Although anyone, from kids to the elderly, can...</td>\n","      <td>-1.802185</td>\n","      <td>0.518239</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>44</td>\n","      <td>-0.191276</td>\n","      <td>2.595029</td>\n","    </tr>\n","    <tr>\n","      <th>1944</th>\n","      <td>04ade0eb2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>While I was hailing the brig, I spied a tract ...</td>\n","      <td>-3.315282</td>\n","      <td>0.544735</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>-1.693418</td>\n","      <td>2.630444</td>\n","    </tr>\n","    <tr>\n","      <th>2124</th>\n","      <td>76f92b721</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>The biggest desert in the world is in Africa, ...</td>\n","      <td>1.103341</td>\n","      <td>0.553751</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>106</td>\n","      <td>-0.578314</td>\n","      <td>2.827965</td>\n","    </tr>\n","    <tr>\n","      <th>2277</th>\n","      <td>7c732b8bb</td>\n","      <td>https://en.wikipedia.org/wiki/Environmental_sc...</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>Environmental science is an interdisciplinary ...</td>\n","      <td>-3.137143</td>\n","      <td>0.555843</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>16</td>\n","      <td>-1.622923</td>\n","      <td>2.292862</td>\n","    </tr>\n","    <tr>\n","      <th>2611</th>\n","      <td>034bfda3f</td>\n","      <td>https://www.commonlit.org/texts/everyday-life-...</td>\n","      <td>CC BY-NC-SA 2.0</td>\n","      <td>Even the clothes we wear every day are scrupul...</td>\n","      <td>-1.624428</td>\n","      <td>0.484176</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>43</td>\n","      <td>-0.050776</td>\n","      <td>2.476382</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id  ...   diff_sq\n","141   bcd734621  ...  3.046322\n","990   afeb324bd  ...  3.062335\n","1314  85b41606e  ...  2.383851\n","1412  8f35441e3  ...  2.287303\n","1628  4cf4a2fa3  ...  2.595029\n","1944  04ade0eb2  ...  2.630444\n","2124  76f92b721  ...  2.827965\n","2277  7c732b8bb  ...  2.292862\n","2611  034bfda3f  ...  2.476382\n","\n","[9 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cL4lTGKjSAA5","executionInfo":{"status":"ok","timestamp":1627830440268,"user_tz":-540,"elapsed":16,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"fb1cb63f-9211-43ec-b3b4-782ba4f9db36"},"source":["# 二乗誤差が2.0を超える文章\n","thr_ = 2.0 \n","tmp_df = train_kf_df[train_kf_df['diff_sq'] > thr_].copy()\n","for i in tmp_df.index:\n","  print(tmp_df.loc[i].target)\n","  #print(tmp_df.loc[i].standard_error)\n","  print(tmp_df.loc[i].pred)\n","  print(tmp_df.loc[i].excerpt)\n","  print('--------------------------')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.943020903\n","-0.8023508191108704\n","Midas was enjoying himself in his treasure-room, one day, as usual, when he perceived a shadow fall over the heaps of gold; and, looking suddenly up, what should he behold but the figure of a stranger, standing in the bright and narrow sunbeam! It was a young man, with a cheerful and ruddy face. Whether it was that the imagination of King Midas threw a yellow tinge over everything, or whatever the cause might be, he could not help fancying that the smile with which the stranger regarded him had a kind of golden radiance in it. Certainly, although his figure intercepted the sunshine, there was now a brighter gleam upon all the piled-up treasures than before. Even the remotest corners had their share of it, and were lighted up, when the stranger smiled, as with tips of flame and sparkles of fire.\n","--------------------------\n","0.401052549\n","-1.3489004373550415\n","On the morning of the 20th of March, a long series of earthquakes spread alarm throughout all the cities and numerous villages that are scattered over the sides of Mt. Etna. The shocks followed each other at intervals of a few minutes; dull subterranean rumblings were heard; and a catastrophe was seen to be impending. Toward evening the ground cracked at the lower part of the south side of the mountain, at the limit of the cultivated zone, and at four kilometers to the north of the village of Nicolosi. There formed on the earth a large number of very wide fissures, through which escaped great volumes of steam and gases which enveloped the mountain in a thick haze; and toward night, a very bright red light, which, seen from Catania, seemed to come out in great waves from the foot of the mountain, announced the coming of the lava.\n","--------------------------\n","-3.5439874060000003\n","-2.0000147819519043\n","As soon as the plate is dry, a positive cliché of the drawing to be reproduced is laid upon it, and the whole exposed to the sun for a minute, or to the electric light for three minutes. The reaction produced is the same as with the citrate of iron, but much quicker; the exposed parts are no longer hygroscopic, but in the parts protected by the lines of the drawing the sensitive coating has retained its stickiness, and will hold any powder that may be passed over it, thus producing a very clear image of the drawing. The coating being excessively thin, the little moisture it holds and the powder applied suffice to break its continuity, especially if the powder be slightly alkaline. If the rest of the surface were sufficiently resisting, the plate might be bitten at once; but light alone is not enough to produce complete impermeability: the action of heat must be combined with it. The plate is, therefore, placed on a grating, with wide openings, a large flame is applied underneath, and it is heated till the borders where the copper is bare show iridescent colors.\n","--------------------------\n","1.583846826\n","0.07146348059177399\n","Every day, Emeka's father took him to school in his car. He also brought Emeka home after school. One afternoon on their way home, Emeka's father stopped to buy something at a big shop. From the car, Emeka looked across the road and saw an old man. He was carrying a big load on his head. He was tired and walked slowly. Emeka kept looking at him. The old man sat under the shade of a tree on the walkway and opened his bag. He had two flat plastic water bottles, which he was making into shoes. Emeka thought about that old man for a long time. He felt sad. When he got home, he could not eat. He thought about what he could do. He got up and took some money from his money bag. He called Chita and jumped on his bicycle. Emeka rode to the shop where his father had shopped. The boy ran into the shop and came out with a bag. He went to where the old man was resting against a tree. Emeka called out, \"Good afternoon, sir.\" The man answered, \"Peace to you, my child.\"\n","--------------------------\n","-1.802185475\n","-0.1912762075662613\n","Although anyone, from kids to the elderly, can come into contact with the bacterium Mycobacterium tuberculosis, the vast majority of people (about 90%) who are infected with this bacterium will eliminate it through the work of the immune system. However, some people (about 10%) who come into contact with M. tuberculosis cannot fully control the bacteria. These people can develop a disease called active tuberculosis (ATB), with fever, coughing, and weight loss. In these cases, M. tuberculosis makes its home within the lungs and can be transmitted to other people when the sick people expel the bacteria through coughing. In some people, another scenario occurs in which their immune system keeps the bacteria in a dormant or \"sleeping\" state. This is called latent TB. In these people, no fever, coughing, or weight loss will be apparent until their immune system stops working properly.\n","--------------------------\n","-3.31528229\n","-1.6934179067611694\n","While I was hailing the brig, I spied a tract of water lying between us, where no great waves came, but which yet boiled white all over and bristled in the moon with rings and bubbles. Sometimes the whole tract swung to one side, like the tail of a live serpent; sometimes, for a glimpse, it would all disappear and then boil up again. What it was I had no guess, which for the time increased my fear of it; but I now know it must have been the roost or tide-race, which had carried me away so fast and tumbled me about so cruelly, and at last, as if tired of that play, had flung out me and the spare yard upon its landward margin.\n","I now lay quite becalmed, and began to feel that a man can die of cold as well as of drowning. The shores of Earraid were close in; I could see in the moonlight the dots of heather and the sparkling of the mica in the rocks.\n","--------------------------\n","1.103341259\n","-0.5783140659332275\n","The biggest desert in the world is in Africa, and is called the Sahara. It is almost as large as the Atlantic Ocean, but instead of water it is all sands and rocks. Like the ocean, it is visited with storms; dreadful gales, when the wind scoops up thousands of tons of sand and drives them forward, burying and crushing all they meet. And it has islands, too—small green patches, where springs bubble through the ground, and ferns and acacias and palm-trees grow. When a traveler sees one of these fertile spots afar off, he feels as a tempest-tossed sailor does at sight of land. It is delightful to quit the hot, baking sun, sit in shadow under the trees, and rest the eyes, long wearied with dazzling sands, on the sweet green and the clear spring. Oases, these islands are called. Long distances divide them. It is often a race for life to get across from one to the other.\n","--------------------------\n","-3.1371432610000003\n","-1.6229232549667358\n","Environmental science is an interdisciplinary academic field that integrates physical, biological and information sciences (including ecology, biology, physics, chemistry, zoology, mineralogy, oceanology, limnology, soil science, geology, atmospheric science, and geodesy) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment. Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.\n","Related areas of study include environmental studies and environmental engineering. Environmental studies incorporate more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect. Environmental scientists work on subjects like the understanding of earth processes, evaluating alternative energy systems, pollution control and mitigation, natural resource management, and the effects of global climate change. Environmental issues almost always include an interaction of physical, chemical, and biological processes.\n","--------------------------\n","-1.624428478\n","-0.050776053220033646\n","Even the clothes we wear every day are scrupulously patterned after Victorian antiques and nineteenth-century fashion plates. Clothes are incredibly intimate. They influence how we move, and at the same time record tiny details about us that seem too mundane to write down — things like whether the items in our pockets are light or heavy, or what we do with our hands when we don't have pockets at all. I sew all my own clothes by hand, and Gabriel's are made for him by a seamstress in Seattle.\n","I'm an author; as with any true writer it's not just my profession but how I experience the world. I keep a diary every day, using an antique mother-of-pearl fountain pen I bought with part of my first book advance. I draft a lot of my manuscripts the same way: I enjoy this tangible connection to my words. (There have been some really interesting studies done showing the human brain processes information more thoroughly when it's written by hand as opposed to typed.) When I take notes from antique books and magazines I use a pencil to avoid dribbling ink on irreplaceable antique volumes.\n","--------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d89ElwoOUPDx"},"source":[""],"execution_count":null,"outputs":[]}]}