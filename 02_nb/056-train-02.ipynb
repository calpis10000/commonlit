{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":432,"status":"ok","timestamp":1627654558798,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"ucCbvGD1XvG7","outputId":"7c0a4768-0a9c-45ed-f973-74a29ed4935e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import sys\n","if 'google.colab' in sys.modules:  # colab特有の処理_2回目以降\n","  # Google Driveのマウント\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  # ライブラリのパス指定\n","  sys.path.append('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4556,"status":"ok","timestamp":1627654563894,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"FACwJ6icpxrR"},"outputs":[],"source":["# データセットをDriveから取得\n","!mkdir -p 'input'\n","!mkdir -p 'clrp-pre-trained'\n","\n","!cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input/commonlitreadabilityprize/' '/content/input'\n","!cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch5/pre-trained-roberta/clrp_roberta_large/' '/content/clrp-pre-trained'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1627654563895,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"RV9-VwbpZLZ9"},"outputs":[],"source":["from pathlib import Path\n","\n","# input\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    DATA_DIR = Path('../input/commonlitreadabilityprize/')\n","\n","elif 'google.colab' in sys.modules: # Colab環境\n","    DATA_DIR = Path('/content/input/commonlitreadabilityprize')\n","\n","else:\n","    DATA_DIR = Path('../00_input/commonlitreadabilityprize/')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1627654563896,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"x5difyXe00UV"},"outputs":[],"source":["from pathlib import Path\n","\n","# tokenizer\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    TOKENIZER_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    TOKENIZER_DIR = '/content/clrp-pre-trained/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    TOKENIZER_DIR = 'roberta-large'"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1627654563896,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"tKjsUxnOeDYl"},"outputs":[],"source":["from pathlib import Path\n","\n","# pre-trained model\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    PRE_TRAINED_MODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    PRE_TRAINED_MODEL_DIR = '/content/clrp-pre-trained/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    PRE_TRAINED_MODEL_DIR = 'roberta-large'"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1627654563896,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"ZLaT2V0ReoAZ"},"outputs":[],"source":["UPLOAD_DIR = Path('/content/model')\n","EX_NO = '056-train-02'  # 実験番号などを入れる、folderのpathにする\n","USERID = 'calpis10000'"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1627654563897,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"hOGjAb4pAJ0F"},"outputs":[],"source":["import subprocess\n","import shlex\n","\n","def gpuinfo():\n","    \"\"\"\n","    Returns size of total GPU RAM and used GPU RAM.\n","\n","    Parameters\n","    ----------\n","    None\n","\n","    Returns\n","    -------\n","    info : dict\n","        Total GPU RAM in integer for key 'total_MiB'.\n","        Used GPU RAM in integer for key 'used_MiB'.\n","    \"\"\"\n","\n","    command = 'nvidia-smi -q -d MEMORY | sed -n \"/FB Memory Usage/,/Free/p\" | sed -e \"1d\" -e \"4d\" -e \"s/ MiB//g\" | cut -d \":\" -f 2 | cut -c2-'\n","    commands = [shlex.split(part) for part in command.split(' | ')]\n","    for i, cmd in enumerate(commands):\n","        if i==0:\n","            res = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n","        else:\n","            res = subprocess.Popen(cmd, stdin=res.stdout, stdout=subprocess.PIPE)\n","    total, used = map(int, res.communicate()[0].decode('utf-8').strip().split('\\n'))\n","    info = {'total_MiB':total, 'used_MiB':used}\n","    return info\n"]},{"cell_type":"markdown","metadata":{"id":"g3-6m5MKXecB"},"source":["# Overview\n","This nb is based on copy from https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch .\n","\n","Acknowledgments(from base nb): \n","some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3392,"status":"ok","timestamp":1627654567273,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"HRsRZ06WXecD"},"outputs":[],"source":["import os\n","import math\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from transformers import AdamW # optimizer\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoConfig\n","from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup # scheduler\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gc\n","gc.enable()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1627654567277,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"omBfwshTXecE"},"outputs":[],"source":["NUM_FOLDS = 5 # K Fold\n","NUM_EPOCHS = 8 # Epochs\n","BATCH_SIZE = 12 # Batch Size\n","MAX_LEN = 248 # ベクトル長\n","EVAL_SCHEDULE = [(0.55, 64), (-1., 32)] # schedulerの何らかの設定？\n","ROBERTA_PATH = PRE_TRAINED_MODEL_DIR # roberta pre-trainedモデル(モデルとして指定)\n","TOKENIZER_PATH = TOKENIZER_DIR # roberta pre-trainedモデル(Tokenizerとして指定)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # cudaがなければcpuを使えばいいじゃない"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1627654567278,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"4qcuXqwtXecF"},"outputs":[],"source":["def set_random_seed(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n","\n","    torch.manual_seed(random_seed)\n","    torch.cuda.manual_seed(random_seed)\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","    torch.backends.cudnn.deterministic = True# cudnnによる最適化で結果が変わらないためのおまじない "]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1627654567279,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"70PyLsJTXecF"},"outputs":[],"source":["# read train_df(kfold)\n","train_kf_df = pd.read_csv(DATA_DIR/\"train_kfold.csv\")"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1627654567279,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"xf0662k4XecF"},"outputs":[],"source":["# tokenizerを指定\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"]},{"cell_type":"markdown","metadata":{"id":"N6aaghNkXecG"},"source":["# Dataset"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1627654567720,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"UU5uZKIcDjkV","outputId":"b61e2bc8-5cea-469b-9ce0-9dc32a7a2dca"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}],"source":["# 前処理用\n","import string\n","import re\n","# ローカルの場合、stopwordsをダウンロード\n","import nltk\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    pass\n","else:\n","    import nltk\n","    nltk.download('stopwords')\n","    nltk.download('averaged_perceptron_tagger')\n","    os.listdir(os.path.expanduser('~/nltk_data/corpora/stopwords/'))\n","\n","# テキスト前処理\n","# https://www.kaggle.com/alaasedeeq/commonlit-readability-eda\n","\n","#filtering the unwanted symbols, spaces, ....etc\n","to_replace_by_space = re.compile('[/(){}\\[\\]|@,;]')\n","punctuation = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n","bad_symbols = re.compile('[^0-9a-z #+_]')\n","stopwords = set(nltk.corpus.stopwords.words('english'))\n","\n","def text_prepare(text):\n","    '''\n","    text: a string\n","    returna modified version of the string\n","    '''\n","    text = text.lower() # lowercase text\n","    text = re.sub(punctuation, '',text)\n","    text = re.sub(to_replace_by_space, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n","    text = re.sub(bad_symbols, \"\", text)         # delete symbols which are in BAD_SYMBOLS_RE from text\n","    text = \" \".join([word for word in text.split(\" \") if word not in stopwords]) # delete stopwords from text\n","    text = re.sub(' +', ' ', text)\n","    return text\n","\n","def text_normalization(s:pd.Series):\n","    x = s.apply(text_prepare)\n","    return x\n","\n","# Counterオブジェクトを取得\n","def get_counter(text:str):\n","    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n","    counter = collections.Counter(text_list)\n","    return counter\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1627654567721,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"iAse0IDWDjho"},"outputs":[],"source":["# センテンス特徴付きのデータセット\n","class LitDataset(Dataset):\n","    def __init__(self, df, inference_only=False):\n","        super().__init__()\n","\n","        self.df = df        \n","        self.inference_only = inference_only # Testデータ用フラグ\n","        self.text = df.excerpt.tolist() # 分析対象カラムをlistにする。(分かち書きではなく、Seriesをlistへ変換するような処理)\n","        #self.text = [text.replace(\"\\n\", \" \") for text in self.text] # 単語単位で分かち書きする場合\n","        self.text_len = text_normalization(df.excerpt).map(lambda x: [0 if i \u003e= len(x.split(' ')) else len(x.split(' ')[i]) for i in range(132)])\n","        self.sentences = df['excerpt'].map(lambda x: x.split('.')).map(lambda x: [0 if i \u003e= len(x) else len(x[i]) for i in range(36)])\n","\n","        if not self.inference_only:\n","            self.target = torch.tensor(df.target.values, dtype=torch.float32) # trainのみ、targetをtensorに変換\n","            self.standard_error = torch.tensor(df.standard_error.values, dtype=torch.float32) \n","\n","        self.encoded = tokenizer.batch_encode_plus( # textをtokenize\n","            self.text,\n","            padding = 'max_length',            \n","            max_length = MAX_LEN,\n","            truncation = True, # 最大長を超える文字は切り捨て\n","            return_attention_mask=True\n","        )        \n"," \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, index): # 変換結果を返す\n","        input_ids = torch.tensor(self.encoded['input_ids'][index])\n","        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n","        input_len = torch.tensor(self.text_len.iloc[index], dtype=torch.float32)\n","        input_sentence = torch.tensor(self.sentences.iloc[index], dtype=torch.float32)\n","\n","        if self.inference_only:\n","            return (input_ids, attention_mask, input_len, input_sentence)            \n","        else:\n","            target = self.target[index]\n","            standard_error = self.standard_error[index]\n","            return (input_ids, attention_mask, input_len, input_sentence, target, standard_error)"]},{"cell_type":"markdown","metadata":{"id":"KKtdy32wXecG"},"source":["# Model\n","The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1627654567721,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"BpkxjXEUXecH"},"outputs":[],"source":["class LitModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(ROBERTA_PATH) # pretrainedからconfigを読み込み\n","        config.update({\"output_hidden_states\":True, # config更新: embedding層を抽出\n","                       \"hidden_dropout_prob\": 0.0, # config更新: dropoutしない\n","                       \"layer_norm_eps\": 1e-7}) # config更新: layer normalizationのepsilon                      \n","        \n","        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config) # cpuで処理する\n","            \n","        self.attention = nn.Sequential(# attentionレイヤー            \n","            nn.Linear(config.hidden_size, 512),      \n","            nn.Tanh(),                       \n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","\n","        self.mlm_layers = nn.Sequential(\n","            nn.Linear(132, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","        )\n","\n","        self.mlm_sentence = nn.Sequential(\n","            nn.Linear(36, 64),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(64, 64),\n","            nn.ReLU(),\n","\n","        )\n","\n","\n","        self.regressor = nn.Sequential( # 出力レイヤー                    \n","            nn.Linear(config.hidden_size + 64 + 64, 2)                        \n","        )\n","\n","    def forward(self, input_ids, attention_mask, input_len, input_sentence):\n","        roberta_output = self.roberta(input_ids=input_ids, # robertaに入力データを流し、出力としてrobertaモデル(layerの複合体)を得る\n","                                      attention_mask=attention_mask)     \n","        # attention_pooling\n","        last_hidden_state = roberta_output.hidden_states[-1] # robertaモデルの最後のlayerを得る\n","        weights = self.attention(last_hidden_state) # robertaの最後のlayerをattentionへ入力し、出力として重みを得る                \n","        context_vector = torch.sum(weights * last_hidden_state, dim=1) # 重み×最後の層を足し合わせて文書ベクトルとする。\n","\n","        # word_length_conv1d\n","        #input_chnl = input_len.unsqueeze(1)\n","        #conv1_layers = self.conv1_layers(input_chnl)\n","        #conv1_layers_v = conv1_layers.view(conv1_layers.size(0),-1)\n","\n","        # word_length_mlm\n","        mlm_layers = self.mlm_layers(input_len)\n","        mlm_sentence = self.mlm_sentence(input_sentence)\n","\n","\n","        # https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently\n","        # last_hidden_state = roberta_output[0]\n","        # input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        # sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        # sum_mask = input_mask_expanded.sum(1)\n","        # sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        # mean_embeddings = sum_embeddings / sum_mask\n","\n","        # concat_embeddings\n","        cat_embeddings = torch.cat([context_vector, mlm_layers, mlm_sentence], dim=1)        \n","        return self.regressor(cat_embeddings) # 文書ベクトルを線形層に入力し、targetを出力する\n","        \n","        # Now we reduce the context vector to the prediction score.\n","        #return self.regressor(mean_embeddings) # 文書ベクトルを線形層に入力し、targetを出力する"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1627654567722,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"bB4jvQTxXecH"},"outputs":[],"source":["def eval_mse(model, data_loader):\n","    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモードを選択。Batch Normとかdropoutをしなくなる           \n","    mse_mean_sum = 0\n","    mse_std_sum = 0\n","\n","    with torch.no_grad(): # 勾配の計算をしないBlock\n","        for batch_num, (input_ids, attention_mask, input_len, input_sentence, target, standard_error) in enumerate(data_loader): # data_loaderからinput, attentin_mask, targetをbatchごとに取り出す\n","            input_ids = input_ids.to(DEVICE)   \n","            attention_mask = attention_mask.to(DEVICE)  \n","            input_len = input_len.to(DEVICE) \n","            input_sentence = input_sentence.to(DEVICE)\n","            target = target.to(DEVICE)      \n","            standard_error = standard_error.to(DEVICE) \n","            \n","            output = model(input_ids, attention_mask, input_len, input_sentence) # 取得した値をモデルへ入力し、出力として予測値を得る。\n","\n","            mse_mean_sum += nn.MSELoss(reduction=\"sum\")(output[:,0].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","            mse_std_sum += nn.MSELoss(reduction=\"sum\")(output[:,1].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","\n","    del input_ids\n","    del attention_mask\n","    del target\n","\n","    mse_mean_result = mse_mean_sum / len(data_loader.dataset)\n","    mse_std_result = mse_std_sum / len(data_loader.dataset)\n","  \n","    return mse_mean_result, mse_std_result # 誤差の合計をdataset長で除し、mseを取得＆返す"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1627654567722,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"47bDno_LXecI"},"outputs":[],"source":["# 推論結果を返す\n","def predict(model, data_loader):\n","    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモード(dropout, batch_normしない)\n","\n","    result = np.zeros(len(data_loader.dataset)) # 結果をdataset長のzero配列として用意\n","    index = 0\n","    \n","    with torch.no_grad(): # 勾配の計算をしないblock(inputすると、現状の重みによる推論結果を返す)\n","        for batch_num, (input_ids, attention_mask, input_len, input_sentence) in enumerate(data_loader): # data_loaderからbatchごとにinputを得る\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)\n","            input_len = input_len.to(DEVICE)\n","            input_sentence = input_sentence.to(DEVICE)\n","                        \n","            output = model(input_ids, attention_mask, input_len, input_sentence) # modelにinputを入力し、予測結果を得る。\n","\n","            result[index : index + output[:,0].shape[0]] = output[:,0].flatten().to(\"cpu\") # result[index ~ predの長さ]へ、予測結果を格納\n","            index += output.shape[0] # indexを更新\n","\n","    return result # 全batchで推論が終わったら、結果を返す"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1627654567723,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"oInneuAmXecI"},"outputs":[],"source":["# 学習\n","def train(model, # モデル\n","          model_path, # モデルのアウトプット先\n","          train_loader, # train-setのdata_loader\n","          val_loader, # valid-setのdata_loader\n","          optimizer, # optimizer\n","          scheduler=None, # scheduler, デフォルトはNone\n","          num_epochs=NUM_EPOCHS # epoch数、notebook冒頭で指定した値\n","         ):    \n","    \n","    best_val_rmse = None\n","    best_epoch = 0\n","    step = 0\n","    last_eval_step = 0\n","    eval_period = EVAL_SCHEDULE[0][1] # eval期間(って何？) 冒頭で決めたEVAL_SCHEDULEの最初のtupleの[1]を取得\n","\n","    start = time.time() # 時間計測用\n","\n","    for epoch in range(num_epochs): # 指定したEpoch数だけ繰り返し\n","        val_rmse = None         \n","\n","        for batch_num, (input_ids, attention_mask, input_len, input_sentence, target, standard_error) in enumerate(train_loader): # train_loaderからinput, targetを取得\n","            input_ids = input_ids.to(DEVICE) # inputをDEVICEへ突っ込む\n","            attention_mask = attention_mask.to(DEVICE)   \n","            input_len = input_len.to(DEVICE)\n","            input_sentence = input_sentence.to(DEVICE)\n","            target = target.to(DEVICE)\n","            standard_error = standard_error.to(DEVICE)  \n","\n","            optimizer.zero_grad() # 勾配を初期化            \n","            model.train() # 学習モード開始\n","\n","            # https://www.kaggle.com/c/commonlitreadabilityprize/discussion/239421\n","            output = model(input_ids, attention_mask, input_len, input_sentence) # input,attention_maskを入力し、予測結果を得る\n","            p = torch.distributions.Normal(output[:,0], torch.sqrt(output[:,1]**2))\n","            q = torch.distributions.Normal(target, standard_error)\n","            kl_vector = torch.distributions.kl_divergence(p, q)\n","            loss = kl_vector.mean()\n","\n","            loss.backward() # 誤差逆伝播法により勾配を得る\n","            optimizer.step() # 重みを更新する\n","\n","            if scheduler:\n","                scheduler.step() # schedulerが与えられた場合は、schedulerの学習率更新\n","            \n","            if step \u003e= last_eval_step + eval_period: # batchを回すごとにstepを増やしていって、「前回evalしたstep + eval_period(16)」を超えたら実行。\n","                # Evaluate the model on val_loader.\n","                elapsed_seconds = time.time() - start # 経過時間\n","                num_steps = step - last_eval_step # 経過ステップ数\n","                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n","                last_eval_step = step # 前回stepの更新\n","                \n","                # valid-setによるrmse計算\n","                train_mean_mse = nn.MSELoss(reduction=\"mean\")(output[:,0].flatten(), target) \n","                train_std_mse = nn.MSELoss(reduction=\"mean\")(torch.sqrt(output[:,1]**2).flatten(), standard_error) \n","\n","                train_mean_rmse = math.sqrt(train_mean_mse)\n","                train_std_rmse = math.sqrt(train_std_mse)\n","\n","                val_mean_mse, val_std_mse = eval_mse(model, val_loader)\n","                val_mean_rmse = math.sqrt(val_mean_mse)                            \n","                val_std_rmse = math.sqrt(val_std_mse)                            \n","\n","                print(f\"Epoch: {epoch} batch_num: {batch_num}\")\n","                print(f\"train_rmse_target: {train_mean_rmse:0.4}\",\n","                      f\"train_rmse_stderror: {train_std_rmse:0.4}\",\n","                      f\"train_kl_div: {loss:0.4}\",\n","                      )\n","                print(f\"val_rmse_target: {val_mean_rmse:0.4}\",\n","                      f\"val_rmse_stderror: {val_std_rmse:0.4}\"\n","                      )\n","\n","                for rmse, period in EVAL_SCHEDULE: # eval_periodをvalid-rmseで切り替える処理\n","                    if val_mean_rmse \u003e= rmse: # valid rmseをEVAL_SCHEDULEと比較し、0項 \u003e valid rmseとなるまで回す : EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n","                        eval_period = period # eval_periodを更新\n","                        break                               \n","\n","                if not best_val_rmse or val_mean_rmse \u003c best_val_rmse: # 初回(best_val_rmse==None), またはbest_val_rmseを更新したらモデルを保存する\n","                    best_val_rmse = val_mean_rmse\n","                    best_epoch = epoch\n","                    torch.save(model.state_dict(), model_path) # 最高の自分を保存\n","                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n","                else:       \n","                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\", # 更新されない場合は、元のスコアを表示\n","                          f\"(from epoch {best_epoch})\")      \n","                                                  \n","                start = time.time()\n","            \n","            # batchごとにメモリ解放\n","            del input_ids\n","            del attention_mask\n","            del target\n","            torch.cuda.empty_cache()                                            \n","            step += 1\n","    \n","    return best_val_rmse"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1627654567723,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"rMY0fjXwXecJ"},"outputs":[],"source":["# optimizerの作成\n","def create_optimizer(model):\n","    parameters = []\n","\n","    named_parameters = list(model.named_parameters()) # モデルパラメータの取得\n","    roberta_parameters = list(model.roberta.named_parameters())[:-2] # パラメータをroberta用、attention用、regressor用に格納。(直接引っ張ってくる形式に変更)\n","\n","    attention_parameters = list(model.attention.named_parameters())\n","    attention_group = [{'params': params, 'lr': 2e-5} for (name, params) in attention_parameters] # attention用パラメータをリストとして取得\n","    parameters += attention_group\n","\n","    #norm_parameters = list(model.layer_norm.named_parameters())\n","    #norm_group = [{'params': params, 'lr': 2e-5} for (name, params) in norm_parameters]\n","    #parameters += norm_group\n","\n","    mlm_parameters = list(model.mlm_layers.named_parameters())\n","    mlm_group = [{'params': params, 'lr': 0.01} for (name, params) in mlm_parameters] # reg用パラメータをリストとして取得\n","    parameters += mlm_group\n","\n","    mlm_sent_parameters = list(model.mlm_sentence.named_parameters())\n","    mlm_sent_group = [{'params': params, 'lr': 0.01, 'weight_decay': 0.0 if \"bias\" in name else 0.01} for (name, params) in mlm_sent_parameters] # reg用パラメータをリストとして取得\n","    parameters += mlm_sent_group\n","\n","    regressor_parameters = list(model.regressor.named_parameters())\n","    regressor_group = [{'params': params, 'lr': 2e-5} for (name, params) in regressor_parameters] # reg用パラメータをリストとして取得\n","    parameters += regressor_group\n","\n","    for layer_num, (name, params) in enumerate(roberta_parameters): # レイヤーごとにname, paramsを取得していろんな処理\n","        weight_decay = 0.0 if \"bias\" in name else 0.01\n","\n","        lr = 8e-6\n","\n","        if layer_num \u003e= 69:        \n","            lr = 2e-5\n","\n","        if layer_num \u003e= 133:\n","            lr = 4e-5\n","\n","        parameters.append({\"params\": params,\n","                           \"weight_decay\": weight_decay,\n","                           \"lr\": lr})\n","\n","    return AdamW(parameters) # 最終的に、AdamWにパラメータを入力する。\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1627654567723,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"},"user_tz":-540},"id":"4PLKHwvKtNBn"},"outputs":[],"source":["def train_and_save_model(train_indices, val_indices, model_path):\n","    train_dataset = LitDataset(train_kf_df.loc[train_indices]) # train, validのDataset\n","    val_dataset = LitDataset(train_kf_df.loc[val_indices])\n","        \n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              drop_last=True, shuffle=True, num_workers=2) # train, validのDataLoader\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                            drop_last=False, shuffle=False, num_workers=2)    \n","\n","    model = LitModel().to(DEVICE) # modelをDEVICEへぶち込む\n","    optimizer = create_optimizer(model) # optimizerをモデルから作成\n","    scheduler = get_cosine_schedule_with_warmup( # schedulerを作成\n","        optimizer,\n","        num_training_steps=NUM_EPOCHS * len(train_loader),\n","        num_warmup_steps=50)    \n","    rmse = train(model, model_path, train_loader, val_loader, optimizer, scheduler=scheduler)\n","\n","    del train_dataset\n","    del val_dataset\n","    del train_loader\n","    del val_loader\n","    del model\n","    del optimizer\n","    del scheduler\n","    gc.collect() \n","    torch.cuda.empty_cache()\n","    return rmse"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"k2LGJD3XXecK"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Fold 1/5\n","{'total_MiB': 16280, 'used_MiB': 2}\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","64 steps took 81.9 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.6492 train_rmse_stderror: 0.1861 train_kl_div: 0.8432\n","val_rmse_target: 0.7242 val_rmse_stderror: 1.127\n","New best_val_rmse: 0.7242\n","\n","64 steps took 80.9 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.4913 train_rmse_stderror: 0.08779 train_kl_div: 0.5083\n","val_rmse_target: 0.6421 val_rmse_stderror: 1.165\n","New best_val_rmse: 0.6421\n","\n","64 steps took 81.1 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.6963 train_rmse_stderror: 0.0293 train_kl_div: 1.042\n","val_rmse_target: 0.5676 val_rmse_stderror: 1.158\n","New best_val_rmse: 0.5676\n","\n","64 steps took 81.0 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.6477 train_rmse_stderror: 0.04029 train_kl_div: 0.7967\n","val_rmse_target: 0.516 val_rmse_stderror: 1.156\n","New best_val_rmse: 0.516\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.5056 train_rmse_stderror: 0.02842 train_kl_div: 0.5204\n","val_rmse_target: 0.5639 val_rmse_stderror: 1.147\n","Still best_val_rmse: 0.516 (from epoch 1)\n","\n","64 steps took 80.9 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.2773 train_rmse_stderror: 0.02904 train_kl_div: 0.1761\n","val_rmse_target: 0.6303 val_rmse_stderror: 1.14\n","Still best_val_rmse: 0.516 (from epoch 1)\n","\n","64 steps took 81.2 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.244 train_rmse_stderror: 0.03815 train_kl_div: 0.1359\n","val_rmse_target: 0.5332 val_rmse_stderror: 1.146\n","Still best_val_rmse: 0.516 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.3744 train_rmse_stderror: 0.02806 train_kl_div: 0.2423\n","val_rmse_target: 0.5063 val_rmse_stderror: 1.143\n","New best_val_rmse: 0.5063\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.2792 train_rmse_stderror: 0.02515 train_kl_div: 0.1738\n","val_rmse_target: 0.5104 val_rmse_stderror: 1.162\n","Still best_val_rmse: 0.5063 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.2642 train_rmse_stderror: 0.03964 train_kl_div: 0.143\n","val_rmse_target: 0.5129 val_rmse_stderror: 1.14\n","Still best_val_rmse: 0.5063 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.2677 train_rmse_stderror: 0.03987 train_kl_div: 0.1312\n","val_rmse_target: 0.5134 val_rmse_stderror: 1.16\n","Still best_val_rmse: 0.5063 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.2002 train_rmse_stderror: 0.03847 train_kl_div: 0.08082\n","val_rmse_target: 0.5076 val_rmse_stderror: 1.146\n","Still best_val_rmse: 0.5063 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.1963 train_rmse_stderror: 0.02667 train_kl_div: 0.08323\n","val_rmse_target: 0.5067 val_rmse_stderror: 1.155\n","Still best_val_rmse: 0.5063 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.2166 train_rmse_stderror: 0.02748 train_kl_div: 0.0988\n","val_rmse_target: 0.5027 val_rmse_stderror: 1.157\n","New best_val_rmse: 0.5027\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1984 train_rmse_stderror: 0.02284 train_kl_div: 0.08694\n","val_rmse_target: 0.5029 val_rmse_stderror: 1.148\n","Still best_val_rmse: 0.5027 (from epoch 3)\n","\n","32 steps took 40.4 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1548 train_rmse_stderror: 0.02141 train_kl_div: 0.05097\n","val_rmse_target: 0.5088 val_rmse_stderror: 1.145\n","Still best_val_rmse: 0.5027 (from epoch 3)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.1285 train_rmse_stderror: 0.03186 train_kl_div: 0.0409\n","val_rmse_target: 0.5005 val_rmse_stderror: 1.139\n","New best_val_rmse: 0.5005\n","\n","32 steps took 40.7 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1517 train_rmse_stderror: 0.02546 train_kl_div: 0.04104\n","val_rmse_target: 0.5031 val_rmse_stderror: 1.148\n","Still best_val_rmse: 0.5005 (from epoch 3)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1154 train_rmse_stderror: 0.01757 train_kl_div: 0.0295\n","val_rmse_target: 0.4994 val_rmse_stderror: 1.15\n","New best_val_rmse: 0.4994\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.1205 train_rmse_stderror: 0.01662 train_kl_div: 0.03328\n","val_rmse_target: 0.5091 val_rmse_stderror: 1.158\n","Still best_val_rmse: 0.4994 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.0906 train_rmse_stderror: 0.02543 train_kl_div: 0.01972\n","val_rmse_target: 0.5019 val_rmse_stderror: 1.151\n","Still best_val_rmse: 0.4994 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.09261 train_rmse_stderror: 0.0197 train_kl_div: 0.01993\n","val_rmse_target: 0.5073 val_rmse_stderror: 1.153\n","Still best_val_rmse: 0.4994 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1255 train_rmse_stderror: 0.0366 train_kl_div: 0.04062\n","val_rmse_target: 0.5039 val_rmse_stderror: 1.149\n","Still best_val_rmse: 0.4994 (from epoch 4)\n","\n","32 steps took 40.7 seconds\n","Epoch: 5 batch_num: 20\n","train_rmse_target: 0.06798 train_rmse_stderror: 0.0156 train_kl_div: 0.01021\n","val_rmse_target: 0.5041 val_rmse_stderror: 1.147\n","Still best_val_rmse: 0.4994 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 52\n","train_rmse_target: 0.0921 train_rmse_stderror: 0.02516 train_kl_div: 0.01769\n","val_rmse_target: 0.4991 val_rmse_stderror: 1.155\n","New best_val_rmse: 0.4991\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 84\n","train_rmse_target: 0.07181 train_rmse_stderror: 0.01149 train_kl_div: 0.01108\n","val_rmse_target: 0.4978 val_rmse_stderror: 1.151\n","New best_val_rmse: 0.4978\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 116\n","train_rmse_target: 0.06163 train_rmse_stderror: 0.02041 train_kl_div: 0.01003\n","val_rmse_target: 0.4997 val_rmse_stderror: 1.148\n","Still best_val_rmse: 0.4978 (from epoch 5)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 148\n","train_rmse_target: 0.08415 train_rmse_stderror: 0.01469 train_kl_div: 0.01593\n","val_rmse_target: 0.503 val_rmse_stderror: 1.151\n","Still best_val_rmse: 0.4978 (from epoch 5)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 180\n","train_rmse_target: 0.05171 train_rmse_stderror: 0.01825 train_kl_div: 0.007194\n","val_rmse_target: 0.4997 val_rmse_stderror: 1.149\n","Still best_val_rmse: 0.4978 (from epoch 5)\n","\n","32 steps took 40.7 seconds\n","Epoch: 6 batch_num: 24\n","train_rmse_target: 0.06069 train_rmse_stderror: 0.01674 train_kl_div: 0.008857\n","val_rmse_target: 0.4975 val_rmse_stderror: 1.147\n","New best_val_rmse: 0.4975\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 56\n","train_rmse_target: 0.03973 train_rmse_stderror: 0.0196 train_kl_div: 0.004846\n","val_rmse_target: 0.5007 val_rmse_stderror: 1.151\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 88\n","train_rmse_target: 0.03225 train_rmse_stderror: 0.01403 train_kl_div: 0.002944\n","val_rmse_target: 0.5001 val_rmse_stderror: 1.152\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 120\n","train_rmse_target: 0.04009 train_rmse_stderror: 0.01789 train_kl_div: 0.004959\n","val_rmse_target: 0.499 val_rmse_stderror: 1.151\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 152\n","train_rmse_target: 0.03835 train_rmse_stderror: 0.01619 train_kl_div: 0.004119\n","val_rmse_target: 0.4993 val_rmse_stderror: 1.148\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 184\n","train_rmse_target: 0.05928 train_rmse_stderror: 0.02341 train_kl_div: 0.009817\n","val_rmse_target: 0.499 val_rmse_stderror: 1.151\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","32 steps took 40.7 seconds\n","Epoch: 7 batch_num: 28\n","train_rmse_target: 0.06204 train_rmse_stderror: 0.01633 train_kl_div: 0.009596\n","val_rmse_target: 0.4999 val_rmse_stderror: 1.149\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","32 steps took 40.5 seconds\n","Epoch: 7 batch_num: 60\n","train_rmse_target: 0.03214 train_rmse_stderror: 0.0215 train_kl_div: 0.00414\n","val_rmse_target: 0.4998 val_rmse_stderror: 1.15\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","32 steps took 40.5 seconds\n","Epoch: 7 batch_num: 92\n","train_rmse_target: 0.0527 train_rmse_stderror: 0.0178 train_kl_div: 0.005657\n","val_rmse_target: 0.4999 val_rmse_stderror: 1.15\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","32 steps took 40.5 seconds\n","Epoch: 7 batch_num: 124\n","train_rmse_target: 0.04414 train_rmse_stderror: 0.01602 train_kl_div: 0.005344\n","val_rmse_target: 0.4999 val_rmse_stderror: 1.15\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","32 steps took 40.5 seconds\n","Epoch: 7 batch_num: 156\n","train_rmse_target: 0.0451 train_rmse_stderror: 0.01906 train_kl_div: 0.005702\n","val_rmse_target: 0.4996 val_rmse_stderror: 1.15\n","Still best_val_rmse: 0.4975 (from epoch 6)\n","\n","Performance estimates:\n","[0.4974752328060981]\n","Mean: 0.4974752328060981\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 2/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","64 steps took 82.0 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.7715 train_rmse_stderror: 0.03818 train_kl_div: 1.171\n","val_rmse_target: 0.7458 val_rmse_stderror: 1.197\n","New best_val_rmse: 0.7458\n","\n","64 steps took 81.0 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.6881 train_rmse_stderror: 0.0408 train_kl_div: 1.026\n","val_rmse_target: 0.7542 val_rmse_stderror: 1.183\n","Still best_val_rmse: 0.7458 (from epoch 0)\n","\n","64 steps took 82.2 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.7589 train_rmse_stderror: 0.04289 train_kl_div: 1.082\n","val_rmse_target: 0.5334 val_rmse_stderror: 1.142\n","New best_val_rmse: 0.5334\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 36\n","train_rmse_target: 0.5415 train_rmse_stderror: 0.03603 train_kl_div: 0.6164\n","val_rmse_target: 0.5056 val_rmse_stderror: 1.151\n","New best_val_rmse: 0.5056\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.4375 train_rmse_stderror: 0.05581 train_kl_div: 0.3725\n","val_rmse_target: 0.5496 val_rmse_stderror: 1.142\n","Still best_val_rmse: 0.5056 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.3823 train_rmse_stderror: 0.03374 train_kl_div: 0.3379\n","val_rmse_target: 0.5124 val_rmse_stderror: 1.141\n","Still best_val_rmse: 0.5056 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.5526 train_rmse_stderror: 0.04189 train_kl_div: 0.6197\n","val_rmse_target: 0.5246 val_rmse_stderror: 1.14\n","Still best_val_rmse: 0.5056 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.4873 train_rmse_stderror: 0.03921 train_kl_div: 0.4456\n","val_rmse_target: 0.4925 val_rmse_stderror: 1.121\n","New best_val_rmse: 0.4925\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.1944 train_rmse_stderror: 0.01447 train_kl_div: 0.08462\n","val_rmse_target: 0.4933 val_rmse_stderror: 1.141\n","Still best_val_rmse: 0.4925 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.3788 train_rmse_stderror: 0.03925 train_kl_div: 0.2514\n","val_rmse_target: 0.488 val_rmse_stderror: 1.143\n","New best_val_rmse: 0.488\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.4591 train_rmse_stderror: 0.03307 train_kl_div: 0.401\n","val_rmse_target: 0.5224 val_rmse_stderror: 1.128\n","Still best_val_rmse: 0.488 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.2853 train_rmse_stderror: 0.0388 train_kl_div: 0.1805\n","val_rmse_target: 0.5051 val_rmse_stderror: 1.152\n","Still best_val_rmse: 0.488 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.2894 train_rmse_stderror: 0.02939 train_kl_div: 0.1874\n","val_rmse_target: 0.6119 val_rmse_stderror: 1.159\n","Still best_val_rmse: 0.488 (from epoch 2)\n","\n","64 steps took 81.2 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.2146 train_rmse_stderror: 0.0358 train_kl_div: 0.1007\n","val_rmse_target: 0.5134 val_rmse_stderror: 1.153\n","Still best_val_rmse: 0.488 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.2945 train_rmse_stderror: 0.02736 train_kl_div: 0.1976\n","val_rmse_target: 0.488 val_rmse_stderror: 1.155\n","Still best_val_rmse: 0.488 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.2858 train_rmse_stderror: 0.03883 train_kl_div: 0.1726\n","val_rmse_target: 0.4894 val_rmse_stderror: 1.131\n","Still best_val_rmse: 0.488 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.2008 train_rmse_stderror: 0.01949 train_kl_div: 0.09324\n","val_rmse_target: 0.4824 val_rmse_stderror: 1.151\n","New best_val_rmse: 0.4824\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.199 train_rmse_stderror: 0.02868 train_kl_div: 0.08448\n","val_rmse_target: 0.4656 val_rmse_stderror: 1.138\n","New best_val_rmse: 0.4656\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.2972 train_rmse_stderror: 0.02704 train_kl_div: 0.2104\n","val_rmse_target: 0.47 val_rmse_stderror: 1.138\n","Still best_val_rmse: 0.4656 (from epoch 3)\n","\n","32 steps took 40.6 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1411 train_rmse_stderror: 0.02245 train_kl_div: 0.04265\n","val_rmse_target: 0.4679 val_rmse_stderror: 1.136\n","Still best_val_rmse: 0.4656 (from epoch 3)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1862 train_rmse_stderror: 0.02608 train_kl_div: 0.06655\n","val_rmse_target: 0.4631 val_rmse_stderror: 1.141\n","New best_val_rmse: 0.4631\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.2329 train_rmse_stderror: 0.04009 train_kl_div: 0.09442\n","val_rmse_target: 0.475 val_rmse_stderror: 1.143\n","Still best_val_rmse: 0.4631 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.1655 train_rmse_stderror: 0.01792 train_kl_div: 0.06089\n","val_rmse_target: 0.467 val_rmse_stderror: 1.138\n","Still best_val_rmse: 0.4631 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.1728 train_rmse_stderror: 0.02857 train_kl_div: 0.06868\n","val_rmse_target: 0.4606 val_rmse_stderror: 1.146\n","New best_val_rmse: 0.4606\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1668 train_rmse_stderror: 0.04147 train_kl_div: 0.05894\n","val_rmse_target: 0.4726 val_rmse_stderror: 1.146\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.7 seconds\n","Epoch: 5 batch_num: 20\n","train_rmse_target: 0.06588 train_rmse_stderror: 0.02049 train_kl_div: 0.01098\n","val_rmse_target: 0.4681 val_rmse_stderror: 1.139\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 52\n","train_rmse_target: 0.1593 train_rmse_stderror: 0.02238 train_kl_div: 0.05515\n","val_rmse_target: 0.4684 val_rmse_stderror: 1.148\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 84\n","train_rmse_target: 0.07726 train_rmse_stderror: 0.02817 train_kl_div: 0.01622\n","val_rmse_target: 0.4651 val_rmse_stderror: 1.146\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 116\n","train_rmse_target: 0.1046 train_rmse_stderror: 0.02991 train_kl_div: 0.02299\n","val_rmse_target: 0.4639 val_rmse_stderror: 1.147\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 148\n","train_rmse_target: 0.1198 train_rmse_stderror: 0.0284 train_kl_div: 0.02725\n","val_rmse_target: 0.4668 val_rmse_stderror: 1.142\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 180\n","train_rmse_target: 0.03089 train_rmse_stderror: 0.02776 train_kl_div: 0.005077\n","val_rmse_target: 0.4641 val_rmse_stderror: 1.14\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.7 seconds\n","Epoch: 6 batch_num: 24\n","train_rmse_target: 0.08289 train_rmse_stderror: 0.02 train_kl_div: 0.01443\n","val_rmse_target: 0.4646 val_rmse_stderror: 1.144\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 56\n","train_rmse_target: 0.06885 train_rmse_stderror: 0.02554 train_kl_div: 0.01218\n","val_rmse_target: 0.4651 val_rmse_stderror: 1.145\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 88\n","train_rmse_target: 0.0637 train_rmse_stderror: 0.02417 train_kl_div: 0.01086\n","val_rmse_target: 0.4641 val_rmse_stderror: 1.143\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 120\n","train_rmse_target: 0.08833 train_rmse_stderror: 0.02017 train_kl_div: 0.01792\n","val_rmse_target: 0.4647 val_rmse_stderror: 1.143\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 152\n","train_rmse_target: 0.06927 train_rmse_stderror: 0.02458 train_kl_div: 0.01306\n","val_rmse_target: 0.4641 val_rmse_stderror: 1.143\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 184\n","train_rmse_target: 0.06326 train_rmse_stderror: 0.02304 train_kl_div: 0.01003\n","val_rmse_target: 0.464 val_rmse_stderror: 1.143\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.7 seconds\n","Epoch: 7 batch_num: 28\n","train_rmse_target: 0.04641 train_rmse_stderror: 0.01311 train_kl_div: 0.004941\n","val_rmse_target: 0.4649 val_rmse_stderror: 1.144\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 7 batch_num: 60\n","train_rmse_target: 0.04823 train_rmse_stderror: 0.02261 train_kl_div: 0.006537\n","val_rmse_target: 0.4645 val_rmse_stderror: 1.143\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 7 batch_num: 92\n","train_rmse_target: 0.05405 train_rmse_stderror: 0.01895 train_kl_div: 0.007385\n","val_rmse_target: 0.465 val_rmse_stderror: 1.144\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 7 batch_num: 124\n","train_rmse_target: 0.04075 train_rmse_stderror: 0.02199 train_kl_div: 0.005218\n","val_rmse_target: 0.4645 val_rmse_stderror: 1.144\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 7 batch_num: 156\n","train_rmse_target: 0.07618 train_rmse_stderror: 0.032 train_kl_div: 0.01409\n","val_rmse_target: 0.4643 val_rmse_stderror: 1.144\n","Still best_val_rmse: 0.4606 (from epoch 4)\n","\n","Performance estimates:\n","[0.4974752328060981, 0.4606025058223538]\n","Mean: 0.47903886931422596\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 3/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","64 steps took 81.9 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.7163 train_rmse_stderror: 0.06171 train_kl_div: 0.9801\n","val_rmse_target: 0.9134 val_rmse_stderror: 1.136\n","New best_val_rmse: 0.9134\n","\n","64 steps took 81.0 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.4724 train_rmse_stderror: 0.03788 train_kl_div: 0.4003\n","val_rmse_target: 0.5757 val_rmse_stderror: 1.111\n","New best_val_rmse: 0.5757\n","\n","64 steps took 81.3 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.4309 train_rmse_stderror: 0.03515 train_kl_div: 0.4233\n","val_rmse_target: 0.497 val_rmse_stderror: 1.119\n","New best_val_rmse: 0.497\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 36\n","train_rmse_target: 0.3134 train_rmse_stderror: 0.02445 train_kl_div: 0.2017\n","val_rmse_target: 0.5676 val_rmse_stderror: 1.125\n","Still best_val_rmse: 0.497 (from epoch 1)\n","\n","64 steps took 81.0 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.3365 train_rmse_stderror: 0.02827 train_kl_div: 0.2345\n","val_rmse_target: 0.5039 val_rmse_stderror: 1.12\n","Still best_val_rmse: 0.497 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.4108 train_rmse_stderror: 0.03643 train_kl_div: 0.3806\n","val_rmse_target: 0.5089 val_rmse_stderror: 1.12\n","Still best_val_rmse: 0.497 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.4505 train_rmse_stderror: 0.05048 train_kl_div: 0.4743\n","val_rmse_target: 0.4959 val_rmse_stderror: 1.111\n","New best_val_rmse: 0.4959\n","\n","32 steps took 40.7 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.2331 train_rmse_stderror: 0.03305 train_kl_div: 0.1125\n","val_rmse_target: 0.5115 val_rmse_stderror: 1.115\n","Still best_val_rmse: 0.4959 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.3354 train_rmse_stderror: 0.02638 train_kl_div: 0.2409\n","val_rmse_target: 0.5043 val_rmse_stderror: 1.123\n","Still best_val_rmse: 0.4959 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2297 train_rmse_stderror: 0.03023 train_kl_div: 0.09547\n","val_rmse_target: 0.4961 val_rmse_stderror: 1.109\n","Still best_val_rmse: 0.4959 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.2033 train_rmse_stderror: 0.02681 train_kl_div: 0.08141\n","val_rmse_target: 0.5061 val_rmse_stderror: 1.115\n","Still best_val_rmse: 0.4959 (from epoch 1)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.2658 train_rmse_stderror: 0.02457 train_kl_div: 0.1442\n","val_rmse_target: 0.503 val_rmse_stderror: 1.118\n","Still best_val_rmse: 0.4959 (from epoch 1)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.1556 train_rmse_stderror: 0.02773 train_kl_div: 0.05271\n","val_rmse_target: 0.5069 val_rmse_stderror: 1.117\n","Still best_val_rmse: 0.4959 (from epoch 1)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.1929 train_rmse_stderror: 0.02213 train_kl_div: 0.07374\n","val_rmse_target: 0.4896 val_rmse_stderror: 1.122\n","New best_val_rmse: 0.4896\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.2005 train_rmse_stderror: 0.01853 train_kl_div: 0.08659\n","val_rmse_target: 0.5036 val_rmse_stderror: 1.115\n","Still best_val_rmse: 0.4896 (from epoch 3)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1884 train_rmse_stderror: 0.02496 train_kl_div: 0.07512\n","val_rmse_target: 0.5027 val_rmse_stderror: 1.119\n","Still best_val_rmse: 0.4896 (from epoch 3)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1665 train_rmse_stderror: 0.02158 train_kl_div: 0.05678\n","val_rmse_target: 0.4883 val_rmse_stderror: 1.11\n","New best_val_rmse: 0.4883\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1824 train_rmse_stderror: 0.02599 train_kl_div: 0.0673\n","val_rmse_target: 0.4865 val_rmse_stderror: 1.112\n","New best_val_rmse: 0.4865\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.1357 train_rmse_stderror: 0.02221 train_kl_div: 0.04204\n","val_rmse_target: 0.4867 val_rmse_stderror: 1.111\n","Still best_val_rmse: 0.4865 (from epoch 3)\n","\n","32 steps took 40.7 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1024 train_rmse_stderror: 0.02255 train_kl_div: 0.02353\n","val_rmse_target: 0.4847 val_rmse_stderror: 1.115\n","New best_val_rmse: 0.4847\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1317 train_rmse_stderror: 0.02913 train_kl_div: 0.03975\n","val_rmse_target: 0.4885 val_rmse_stderror: 1.114\n","Still best_val_rmse: 0.4847 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.09429 train_rmse_stderror: 0.02143 train_kl_div: 0.02101\n","val_rmse_target: 0.4821 val_rmse_stderror: 1.112\n","New best_val_rmse: 0.4821\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.1013 train_rmse_stderror: 0.02741 train_kl_div: 0.02509\n","val_rmse_target: 0.4837 val_rmse_stderror: 1.122\n","Still best_val_rmse: 0.4821 (from epoch 4)\n","\n","32 steps took 40.3 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.08288 train_rmse_stderror: 0.02171 train_kl_div: 0.01751\n","val_rmse_target: 0.4823 val_rmse_stderror: 1.118\n","Still best_val_rmse: 0.4821 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1203 train_rmse_stderror: 0.01548 train_kl_div: 0.02983\n","val_rmse_target: 0.4821 val_rmse_stderror: 1.118\n","New best_val_rmse: 0.4821\n","\n","32 steps took 40.6 seconds\n","Epoch: 5 batch_num: 20\n","train_rmse_target: 0.08119 train_rmse_stderror: 0.01844 train_kl_div: 0.01631\n","val_rmse_target: 0.4833 val_rmse_stderror: 1.119\n","Still best_val_rmse: 0.4821 (from epoch 4)\n","\n","32 steps took 40.2 seconds\n","Epoch: 5 batch_num: 52\n","train_rmse_target: 0.05317 train_rmse_stderror: 0.01546 train_kl_div: 0.006921\n","val_rmse_target: 0.4803 val_rmse_stderror: 1.118\n","New best_val_rmse: 0.4803\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 84\n","train_rmse_target: 0.06116 train_rmse_stderror: 0.02809 train_kl_div: 0.01066\n","val_rmse_target: 0.4828 val_rmse_stderror: 1.116\n","Still best_val_rmse: 0.4803 (from epoch 5)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 116\n","train_rmse_target: 0.06016 train_rmse_stderror: 0.02766 train_kl_div: 0.008792\n","val_rmse_target: 0.4806 val_rmse_stderror: 1.121\n","Still best_val_rmse: 0.4803 (from epoch 5)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 148\n","train_rmse_target: 0.06244 train_rmse_stderror: 0.0234 train_kl_div: 0.008943\n","val_rmse_target: 0.4805 val_rmse_stderror: 1.118\n","Still best_val_rmse: 0.4803 (from epoch 5)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 180\n","train_rmse_target: 0.0774 train_rmse_stderror: 0.02188 train_kl_div: 0.01391\n","val_rmse_target: 0.4863 val_rmse_stderror: 1.118\n","Still best_val_rmse: 0.4803 (from epoch 5)\n","\n","32 steps took 40.6 seconds\n","Epoch: 6 batch_num: 24\n","train_rmse_target: 0.07519 train_rmse_stderror: 0.01966 train_kl_div: 0.01361\n","val_rmse_target: 0.4782 val_rmse_stderror: 1.117\n","New best_val_rmse: 0.4782\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 56\n","train_rmse_target: 0.05209 train_rmse_stderror: 0.01456 train_kl_div: 0.006397\n","val_rmse_target: 0.4817 val_rmse_stderror: 1.12\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 88\n","train_rmse_target: 0.04013 train_rmse_stderror: 0.01361 train_kl_div: 0.004327\n","val_rmse_target: 0.4798 val_rmse_stderror: 1.118\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 120\n","train_rmse_target: 0.02837 train_rmse_stderror: 0.01383 train_kl_div: 0.002436\n","val_rmse_target: 0.4802 val_rmse_stderror: 1.117\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 152\n","train_rmse_target: 0.05402 train_rmse_stderror: 0.01449 train_kl_div: 0.005575\n","val_rmse_target: 0.4812 val_rmse_stderror: 1.118\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 184\n","train_rmse_target: 0.04712 train_rmse_stderror: 0.01753 train_kl_div: 0.005689\n","val_rmse_target: 0.4801 val_rmse_stderror: 1.119\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","32 steps took 40.6 seconds\n","Epoch: 7 batch_num: 28\n","train_rmse_target: 0.03217 train_rmse_stderror: 0.01675 train_kl_div: 0.003457\n","val_rmse_target: 0.4803 val_rmse_stderror: 1.119\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 60\n","train_rmse_target: 0.04669 train_rmse_stderror: 0.009816 train_kl_div: 0.005064\n","val_rmse_target: 0.4803 val_rmse_stderror: 1.117\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 92\n","train_rmse_target: 0.0453 train_rmse_stderror: 0.01775 train_kl_div: 0.00582\n","val_rmse_target: 0.4794 val_rmse_stderror: 1.118\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 124\n","train_rmse_target: 0.02803 train_rmse_stderror: 0.01654 train_kl_div: 0.002845\n","val_rmse_target: 0.4796 val_rmse_stderror: 1.118\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","32 steps took 40.5 seconds\n","Epoch: 7 batch_num: 156\n","train_rmse_target: 0.03768 train_rmse_stderror: 0.02303 train_kl_div: 0.005111\n","val_rmse_target: 0.4799 val_rmse_stderror: 1.118\n","Still best_val_rmse: 0.4782 (from epoch 6)\n","\n","Performance estimates:\n","[0.4974752328060981, 0.4606025058223538, 0.47815605977590714]\n","Mean: 0.4787445994681197\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 4/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","64 steps took 81.8 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 1.013 train_rmse_stderror: 0.0744 train_kl_div: 1.89\n","val_rmse_target: 0.7712 val_rmse_stderror: 1.147\n","New best_val_rmse: 0.7712\n","\n","64 steps took 80.9 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.5008 train_rmse_stderror: 0.04519 train_kl_div: 0.5055\n","val_rmse_target: 0.5601 val_rmse_stderror: 1.137\n","New best_val_rmse: 0.5601\n","\n","64 steps took 81.3 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.4967 train_rmse_stderror: 0.0302 train_kl_div: 0.4867\n","val_rmse_target: 0.6373 val_rmse_stderror: 1.131\n","Still best_val_rmse: 0.5601 (from epoch 0)\n","\n","64 steps took 80.9 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.6629 train_rmse_stderror: 0.04556 train_kl_div: 0.8844\n","val_rmse_target: 0.5473 val_rmse_stderror: 1.109\n","New best_val_rmse: 0.5473\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.4036 train_rmse_stderror: 0.03414 train_kl_div: 0.3571\n","val_rmse_target: 0.5329 val_rmse_stderror: 1.113\n","New best_val_rmse: 0.5329\n","\n","32 steps took 40.5 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.4404 train_rmse_stderror: 0.03375 train_kl_div: 0.3943\n","val_rmse_target: 0.5562 val_rmse_stderror: 1.121\n","Still best_val_rmse: 0.5329 (from epoch 1)\n","\n","64 steps took 81.1 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.3162 train_rmse_stderror: 0.03446 train_kl_div: 0.2065\n","val_rmse_target: 0.4882 val_rmse_stderror: 1.121\n","New best_val_rmse: 0.4882\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2411 train_rmse_stderror: 0.03289 train_kl_div: 0.1281\n","val_rmse_target: 0.5037 val_rmse_stderror: 1.125\n","Still best_val_rmse: 0.4882 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2979 train_rmse_stderror: 0.02886 train_kl_div: 0.185\n","val_rmse_target: 0.4775 val_rmse_stderror: 1.119\n","New best_val_rmse: 0.4775\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.2905 train_rmse_stderror: 0.02845 train_kl_div: 0.1768\n","val_rmse_target: 0.5089 val_rmse_stderror: 1.124\n","Still best_val_rmse: 0.4775 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.1394 train_rmse_stderror: 0.03558 train_kl_div: 0.04858\n","val_rmse_target: 0.5075 val_rmse_stderror: 1.124\n","Still best_val_rmse: 0.4775 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.4178 train_rmse_stderror: 0.0294 train_kl_div: 0.4001\n","val_rmse_target: 0.5061 val_rmse_stderror: 1.129\n","Still best_val_rmse: 0.4775 (from epoch 2)\n","\n","32 steps took 40.7 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.1381 train_rmse_stderror: 0.0283 train_kl_div: 0.04436\n","val_rmse_target: 0.502 val_rmse_stderror: 1.127\n","Still best_val_rmse: 0.4775 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.191 train_rmse_stderror: 0.02339 train_kl_div: 0.0844\n","val_rmse_target: 0.4879 val_rmse_stderror: 1.119\n","Still best_val_rmse: 0.4775 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.2209 train_rmse_stderror: 0.02746 train_kl_div: 0.107\n","val_rmse_target: 0.4804 val_rmse_stderror: 1.125\n","Still best_val_rmse: 0.4775 (from epoch 2)\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1975 train_rmse_stderror: 0.02615 train_kl_div: 0.09522\n","val_rmse_target: 0.4725 val_rmse_stderror: 1.117\n","New best_val_rmse: 0.4725\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1988 train_rmse_stderror: 0.02294 train_kl_div: 0.08216\n","val_rmse_target: 0.4778 val_rmse_stderror: 1.125\n","Still best_val_rmse: 0.4725 (from epoch 3)\n","\n","32 steps took 40.4 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.3898 train_rmse_stderror: 0.06447 train_kl_div: 0.3189\n","val_rmse_target: 0.5635 val_rmse_stderror: 1.14\n","Still best_val_rmse: 0.4725 (from epoch 3)\n","\n","64 steps took 81.1 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.2515 train_rmse_stderror: 0.04121 train_kl_div: 0.117\n","val_rmse_target: 0.4811 val_rmse_stderror: 1.128\n","Still best_val_rmse: 0.4725 (from epoch 3)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.3262 train_rmse_stderror: 0.02495 train_kl_div: 0.2172\n","val_rmse_target: 0.5119 val_rmse_stderror: 1.129\n","Still best_val_rmse: 0.4725 (from epoch 3)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.191 train_rmse_stderror: 0.02603 train_kl_div: 0.07271\n","val_rmse_target: 0.4707 val_rmse_stderror: 1.121\n","New best_val_rmse: 0.4707\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.2095 train_rmse_stderror: 0.02426 train_kl_div: 0.08548\n","val_rmse_target: 0.4759 val_rmse_stderror: 1.124\n","Still best_val_rmse: 0.4707 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1349 train_rmse_stderror: 0.02947 train_kl_div: 0.0397\n","val_rmse_target: 0.4723 val_rmse_stderror: 1.121\n","Still best_val_rmse: 0.4707 (from epoch 4)\n","\n","32 steps took 40.7 seconds\n","Epoch: 5 batch_num: 20\n","train_rmse_target: 0.1586 train_rmse_stderror: 0.02957 train_kl_div: 0.05312\n","val_rmse_target: 0.4709 val_rmse_stderror: 1.127\n","Still best_val_rmse: 0.4707 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 52\n","train_rmse_target: 0.1239 train_rmse_stderror: 0.0146 train_kl_div: 0.03542\n","val_rmse_target: 0.4824 val_rmse_stderror: 1.127\n","Still best_val_rmse: 0.4707 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 84\n","train_rmse_target: 0.1202 train_rmse_stderror: 0.02201 train_kl_div: 0.03179\n","val_rmse_target: 0.4667 val_rmse_stderror: 1.122\n","New best_val_rmse: 0.4667\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 116\n","train_rmse_target: 0.06684 train_rmse_stderror: 0.01752 train_kl_div: 0.01055\n","val_rmse_target: 0.4755 val_rmse_stderror: 1.125\n","Still best_val_rmse: 0.4667 (from epoch 5)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 148\n","train_rmse_target: 0.1453 train_rmse_stderror: 0.02497 train_kl_div: 0.04642\n","val_rmse_target: 0.4753 val_rmse_stderror: 1.128\n","Still best_val_rmse: 0.4667 (from epoch 5)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 180\n","train_rmse_target: 0.08893 train_rmse_stderror: 0.02063 train_kl_div: 0.01747\n","val_rmse_target: 0.4677 val_rmse_stderror: 1.126\n","Still best_val_rmse: 0.4667 (from epoch 5)\n","\n","32 steps took 40.6 seconds\n","Epoch: 6 batch_num: 24\n","train_rmse_target: 0.1059 train_rmse_stderror: 0.02185 train_kl_div: 0.02544\n","val_rmse_target: 0.4665 val_rmse_stderror: 1.125\n","New best_val_rmse: 0.4665\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 56\n","train_rmse_target: 0.1149 train_rmse_stderror: 0.0264 train_kl_div: 0.03143\n","val_rmse_target: 0.4699 val_rmse_stderror: 1.125\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 88\n","train_rmse_target: 0.05543 train_rmse_stderror: 0.01578 train_kl_div: 0.007822\n","val_rmse_target: 0.4704 val_rmse_stderror: 1.122\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 120\n","train_rmse_target: 0.1007 train_rmse_stderror: 0.02196 train_kl_div: 0.0242\n","val_rmse_target: 0.4707 val_rmse_stderror: 1.124\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 152\n","train_rmse_target: 0.0789 train_rmse_stderror: 0.01712 train_kl_div: 0.0126\n","val_rmse_target: 0.4702 val_rmse_stderror: 1.125\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 184\n","train_rmse_target: 0.06163 train_rmse_stderror: 0.02797 train_kl_div: 0.0105\n","val_rmse_target: 0.4714 val_rmse_stderror: 1.126\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","32 steps took 40.6 seconds\n","Epoch: 7 batch_num: 28\n","train_rmse_target: 0.06936 train_rmse_stderror: 0.01551 train_kl_div: 0.01129\n","val_rmse_target: 0.47 val_rmse_stderror: 1.123\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 60\n","train_rmse_target: 0.06634 train_rmse_stderror: 0.01889 train_kl_div: 0.01054\n","val_rmse_target: 0.4693 val_rmse_stderror: 1.125\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 92\n","train_rmse_target: 0.08437 train_rmse_stderror: 0.02946 train_kl_div: 0.01901\n","val_rmse_target: 0.472 val_rmse_stderror: 1.124\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 124\n","train_rmse_target: 0.06138 train_rmse_stderror: 0.01306 train_kl_div: 0.008516\n","val_rmse_target: 0.4702 val_rmse_stderror: 1.124\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 156\n","train_rmse_target: 0.06935 train_rmse_stderror: 0.01279 train_kl_div: 0.01189\n","val_rmse_target: 0.4702 val_rmse_stderror: 1.124\n","Still best_val_rmse: 0.4665 (from epoch 6)\n","\n","Performance estimates:\n","[0.4974752328060981, 0.4606025058223538, 0.47815605977590714, 0.46651593134692726]\n","Mean: 0.47568743243782163\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 5/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["\n","64 steps took 81.9 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.6369 train_rmse_stderror: 0.1125 train_kl_div: 0.7691\n","val_rmse_target: 0.6416 val_rmse_stderror: 1.891\n","New best_val_rmse: 0.6416\n","\n","64 steps took 80.9 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.6519 train_rmse_stderror: 0.08903 train_kl_div: 0.7866\n","val_rmse_target: 0.6939 val_rmse_stderror: 1.741\n","Still best_val_rmse: 0.6416 (from epoch 0)\n","\n","64 steps took 81.1 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.5165 train_rmse_stderror: 0.0485 train_kl_div: 0.5049\n","val_rmse_target: 0.733 val_rmse_stderror: 1.783\n","Still best_val_rmse: 0.6416 (from epoch 0)\n","\n","64 steps took 81.1 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.4576 train_rmse_stderror: 0.05342 train_kl_div: 0.4514\n","val_rmse_target: 0.5776 val_rmse_stderror: 1.762\n","New best_val_rmse: 0.5776\n","\n","64 steps took 80.9 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.3795 train_rmse_stderror: 0.1144 train_kl_div: 0.3378\n","val_rmse_target: 0.6377 val_rmse_stderror: 1.726\n","Still best_val_rmse: 0.5776 (from epoch 1)\n","\n","64 steps took 81.1 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.297 train_rmse_stderror: 0.03963 train_kl_div: 0.2034\n","val_rmse_target: 0.5604 val_rmse_stderror: 1.77\n","New best_val_rmse: 0.5604\n","\n","64 steps took 80.9 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2767 train_rmse_stderror: 0.03886 train_kl_div: 0.1606\n","val_rmse_target: 0.5087 val_rmse_stderror: 1.805\n","New best_val_rmse: 0.5087\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.3179 train_rmse_stderror: 0.03748 train_kl_div: 0.2075\n","val_rmse_target: 0.5172 val_rmse_stderror: 1.806\n","Still best_val_rmse: 0.5087 (from epoch 2)\n","\n","32 steps took 40.4 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.2933 train_rmse_stderror: 0.05515 train_kl_div: 0.1778\n","val_rmse_target: 0.4938 val_rmse_stderror: 1.829\n","New best_val_rmse: 0.4938\n","\n","32 steps took 40.5 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.3531 train_rmse_stderror: 0.05235 train_kl_div: 0.2619\n","val_rmse_target: 0.5114 val_rmse_stderror: 1.832\n","Still best_val_rmse: 0.4938 (from epoch 2)\n","\n","32 steps took 40.6 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.2294 train_rmse_stderror: 0.04221 train_kl_div: 0.1177\n","val_rmse_target: 0.4892 val_rmse_stderror: 1.806\n","New best_val_rmse: 0.4892\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.2633 train_rmse_stderror: 0.0352 train_kl_div: 0.1465\n","val_rmse_target: 0.4848 val_rmse_stderror: 1.793\n","New best_val_rmse: 0.4848\n","\n","32 steps took 40.5 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.2581 train_rmse_stderror: 0.05463 train_kl_div: 0.1489\n","val_rmse_target: 0.5101 val_rmse_stderror: 1.811\n","Still best_val_rmse: 0.4848 (from epoch 3)\n","\n","32 steps took 40.4 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1971 train_rmse_stderror: 0.04857 train_kl_div: 0.09486\n","val_rmse_target: 0.4872 val_rmse_stderror: 1.812\n","Still best_val_rmse: 0.4848 (from epoch 3)\n","\n","32 steps took 40.3 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.2116 train_rmse_stderror: 0.03673 train_kl_div: 0.09227\n","val_rmse_target: 0.4838 val_rmse_stderror: 1.818\n","New best_val_rmse: 0.4838\n","\n","32 steps took 40.4 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.298 train_rmse_stderror: 0.03474 train_kl_div: 0.1809\n","val_rmse_target: 0.5283 val_rmse_stderror: 1.808\n","Still best_val_rmse: 0.4838 (from epoch 3)\n","\n","32 steps took 40.6 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1292 train_rmse_stderror: 0.03365 train_kl_div: 0.04139\n","val_rmse_target: 0.4951 val_rmse_stderror: 1.804\n","Still best_val_rmse: 0.4838 (from epoch 3)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1213 train_rmse_stderror: 0.03571 train_kl_div: 0.03527\n","val_rmse_target: 0.4819 val_rmse_stderror: 1.789\n","New best_val_rmse: 0.4819\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.1184 train_rmse_stderror: 0.03047 train_kl_div: 0.0315\n","val_rmse_target: 0.5162 val_rmse_stderror: 1.802\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.1289 train_rmse_stderror: 0.02878 train_kl_div: 0.04021\n","val_rmse_target: 0.4994 val_rmse_stderror: 1.793\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.1598 train_rmse_stderror: 0.02251 train_kl_div: 0.053\n","val_rmse_target: 0.4832 val_rmse_stderror: 1.798\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1428 train_rmse_stderror: 0.03416 train_kl_div: 0.04882\n","val_rmse_target: 0.4914 val_rmse_stderror: 1.795\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.6 seconds\n","Epoch: 5 batch_num: 20\n","train_rmse_target: 0.1017 train_rmse_stderror: 0.02117 train_kl_div: 0.02011\n","val_rmse_target: 0.4919 val_rmse_stderror: 1.796\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 52\n","train_rmse_target: 0.08665 train_rmse_stderror: 0.02496 train_kl_div: 0.01848\n","val_rmse_target: 0.4969 val_rmse_stderror: 1.786\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 5 batch_num: 84\n","train_rmse_target: 0.07801 train_rmse_stderror: 0.02421 train_kl_div: 0.01621\n","val_rmse_target: 0.4983 val_rmse_stderror: 1.809\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 116\n","train_rmse_target: 0.1124 train_rmse_stderror: 0.03255 train_kl_div: 0.02748\n","val_rmse_target: 0.4967 val_rmse_stderror: 1.789\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 148\n","train_rmse_target: 0.1019 train_rmse_stderror: 0.03746 train_kl_div: 0.0262\n","val_rmse_target: 0.4895 val_rmse_stderror: 1.796\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 5 batch_num: 180\n","train_rmse_target: 0.09463 train_rmse_stderror: 0.02689 train_kl_div: 0.01893\n","val_rmse_target: 0.4971 val_rmse_stderror: 1.792\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.6 seconds\n","Epoch: 6 batch_num: 24\n","train_rmse_target: 0.08471 train_rmse_stderror: 0.01996 train_kl_div: 0.01614\n","val_rmse_target: 0.4888 val_rmse_stderror: 1.797\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.5 seconds\n","Epoch: 6 batch_num: 56\n","train_rmse_target: 0.04554 train_rmse_stderror: 0.06867 train_kl_div: 0.02113\n","val_rmse_target: 0.4905 val_rmse_stderror: 1.799\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 88\n","train_rmse_target: 0.05049 train_rmse_stderror: 0.02244 train_kl_div: 0.007691\n","val_rmse_target: 0.4935 val_rmse_stderror: 1.797\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 120\n","train_rmse_target: 0.07319 train_rmse_stderror: 0.02297 train_kl_div: 0.01432\n","val_rmse_target: 0.4948 val_rmse_stderror: 1.793\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 152\n","train_rmse_target: 0.06994 train_rmse_stderror: 0.01601 train_kl_div: 0.01162\n","val_rmse_target: 0.4921 val_rmse_stderror: 1.797\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 6 batch_num: 184\n","train_rmse_target: 0.04305 train_rmse_stderror: 0.02569 train_kl_div: 0.006585\n","val_rmse_target: 0.4924 val_rmse_stderror: 1.797\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.6 seconds\n","Epoch: 7 batch_num: 28\n","train_rmse_target: 0.05767 train_rmse_stderror: 0.0194 train_kl_div: 0.007579\n","val_rmse_target: 0.49 val_rmse_stderror: 1.798\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 60\n","train_rmse_target: 0.05761 train_rmse_stderror: 0.02525 train_kl_div: 0.009429\n","val_rmse_target: 0.4905 val_rmse_stderror: 1.796\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 92\n","train_rmse_target: 0.04142 train_rmse_stderror: 0.02898 train_kl_div: 0.006919\n","val_rmse_target: 0.491 val_rmse_stderror: 1.797\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 124\n","train_rmse_target: 0.05791 train_rmse_stderror: 0.02836 train_kl_div: 0.009818\n","val_rmse_target: 0.4916 val_rmse_stderror: 1.796\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","32 steps took 40.4 seconds\n","Epoch: 7 batch_num: 156\n","train_rmse_target: 0.04204 train_rmse_stderror: 0.01847 train_kl_div: 0.005122\n","val_rmse_target: 0.4917 val_rmse_stderror: 1.796\n","Still best_val_rmse: 0.4819 (from epoch 4)\n","\n","Performance estimates:\n","[0.4974752328060981, 0.4606025058223538, 0.47815605977590714, 0.46651593134692726, 0.48191233648530774]\n","Mean: 0.4769324132473189\n","{'total_MiB': 16280, 'used_MiB': 927}\n"]}],"source":["# 実行処理。 KFold \u0026 学習\n","SEED = 1000\n","list_val_rmse = []\n","\n","for fold in sorted(train_kf_df['kfold'].unique()):\n","    if fold \u003e 10:\n","      pass\n","    else:\n","      print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n","      print(gpuinfo())\n","      model_path = f\"model_{fold + 1}.pth\" # model_fold数_.pth\n","      set_random_seed(SEED + fold) # SEEDはfold別に変わるようにする\n","\n","      train_indices = (train_kf_df['kfold'] != fold)\n","      val_indices = (train_kf_df['kfold'] == fold)\n","      list_val_rmse.append(train_and_save_model(train_indices, val_indices, model_path))\n","      print(\"\\nPerformance estimates:\")\n","      print(list_val_rmse)\n","      print(\"Mean:\", np.array(list_val_rmse).mean())\n","      print(gpuinfo())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"m4v-cGx-Mv7S"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.4974752328060981, 0.4606025058223538, 0.47815605977590714, 0.46651593134692726, 0.48191233648530774]\n"]}],"source":["print(list_val_rmse)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XU4gRXHCBEpC"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iAb99KSKBEmd"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jH0aFzWxBEkG"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"q2CdCMuIKDMP"},"outputs":[],"source":["#rep = MemReporter(model)\n","#rep.report()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eLl1yDOOKIe7"},"outputs":[],"source":["#rep = MemReporter(model.roberta)\n","#rep.report()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7qkqnknA_m9D"},"outputs":[],"source":["#gpuinfo()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PwrqSMdYA6Pu"},"outputs":[],"source":["#del model\n","#del optimizer \n","#del train_loader\n","#del val_loader\n","#del scheduler \n","#del list_val_rmse\n","#del train_indices\n","#del val_indices\n","#del tokenizer\n","#torch.cuda.empty_cache()\n","#gpuinfo()"]},{"cell_type":"markdown","metadata":{"id":"wXcHyUSJXecL"},"source":["# upload models"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YIV6UllSIGoa"},"outputs":[{"name":"stdout","output_type":"stream","text":["/root\n"]}],"source":["%cd\n","!mkdir .kaggle\n","!mkdir /content/model\n","!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\n","\n","!cp -r /content/model_1.pth /content/model/model_1.pth\n","!cp -r /content/model_2.pth /content/model/model_2.pth\n","!cp -r /content/model_3.pth /content/model/model_3.pth\n","!cp -r /content/model_4.pth /content/model/model_4.pth\n","!cp -r /content/model_5.pth /content/model/model_5.pth"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"14ddOZH4IMam"},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting upload for file model_4.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:49\u003c00:00, 29.0MB/s]\n","  0%|          | 0.00/1.33G [00:00\u003c?, ?B/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: model_4.pth (1GB)\n","Starting upload for file model_3.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:54\u003c00:00, 26.2MB/s]\n","  0%|          | 0.00/1.33G [00:00\u003c?, ?B/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: model_3.pth (1GB)\n","Starting upload for file model_5.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:50\u003c00:00, 28.5MB/s]\n","  0%|          | 0.00/1.33G [00:00\u003c?, ?B/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: model_5.pth (1GB)\n","Starting upload for file model_2.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:50\u003c00:00, 28.2MB/s]\n","  0%|          | 0.00/1.33G [00:00\u003c?, ?B/s]"]},{"name":"stdout","output_type":"stream","text":["Upload successful: model_2.pth (1GB)\n","Starting upload for file model_1.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:48\u003c00:00, 29.3MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: model_1.pth (1GB)\n"]}],"source":["def dataset_upload():\n","    import json\n","    from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","    id = f'{USERID}/{EX_NO}-02'\n","\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = id\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = f'{EX_NO}'\n","\n","    with open(UPLOAD_DIR / 'dataset-metadata.json', 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","\n","    api = KaggleApi()\n","    api.authenticate()\n","\n","    # データセットがない場合\n","    if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n","        api.dataset_create_new(folder=UPLOAD_DIR,\n","                               convert_to_csv=False,\n","                               dir_mode='skip')\n","    # データセットがある場合\n","    else:\n","        api.dataset_create_version(folder=UPLOAD_DIR,\n","                                   version_notes='update',\n","                                   convert_to_csv=False,\n","                                   delete_old_versions=True,\n","                                   dir_mode='skip')\n","dataset_upload()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ceDI72NumT5-"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvRi_JQgwcKI"},"outputs":[],"source":["# validation再実行_予測結果取得\n","\n","all_predictions = np.zeros(len(train_kf_df)) # 推論結果について、「fold　× 推論df」のzero行列で枠を作る\n","\n","for fold_ in sorted(train_kf_df['kfold'].unique()):\n","    model_path = UPLOAD_DIR/f\"model_{fold_ + 1}.pth\" # 対応するモデルを読む\n","    print(f\"\\nUsing {model_path}\")\n","\n","    val_idx = train_kf_df['kfold'] == fold_\n","    val_df = train_kf_df[val_idx]\n","    val_dataset = LitDataset(val_df, inference_only=True) # TestのDataset(何で、もう一回作るのだろう？)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                          drop_last=False, shuffle=False, num_workers=2) # TestのDataLoader\n","\n","    model = LitModel()\n","    model.load_state_dict(torch.load(model_path))    # 対応するモデルから、重みを読み込む\n","    model.to(DEVICE) # モデルをDEVICEへぶち込む\n","\n","    all_predictions[val_idx] = predict(model, val_loader) # 推論結果行列の対象列に、推論結果を入力(以後、繰り返し)\n","\n","    del model\n","    gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3oS-lD5t3FMc"},"outputs":[],"source":["train_kf_df['pred'] = all_predictions\n","train_kf_df['diff_sq'] = (train_kf_df['target'] - train_kf_df['pred'])**2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ab2NiFJ13FJ7"},"outputs":[],"source":["train_kf_df.plot(kind='scatter', x='target', y='diff_sq')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRHfZBVw3FHw"},"outputs":[],"source":["# 二乗誤差が2.0を超えるカラム\n","thr_ = 2.0 \n","train_kf_df[train_kf_df['diff_sq'] \u003e thr_]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hJjGdas3FFE"},"outputs":[],"source":["# 二乗誤差が2.0を超える文章\n","thr_ = 2.0 \n","tmp_df = train_kf_df[train_kf_df['diff_sq'] \u003e thr_].copy()\n","for i in tmp_df.index:\n","  print(tmp_df.loc[i].target)\n","  #print(tmp_df.loc[i].standard_error)\n","  print(tmp_df.loc[i].pred)\n","  print(tmp_df.loc[i].excerpt)\n","  print('--------------------------')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"056-train-02.ipynb","provenance":[{"file_id":"1HgqDqRZxMcO4R6hRzZPzzp110K4RRxYL","timestamp":1627579646583},{"file_id":"1xPfSbmykBGlw1mj57IfRZQMh5-IZiHrt","timestamp":1627571352916},{"file_id":"17BUK8yRF7SDX0khlOXoHcFRTEFffkvRb","timestamp":1627488927996},{"file_id":"1wNpTEKAuuKP7ivTcm1f9j0sdmYU1RyzA","timestamp":1627306793279},{"file_id":"1uE__yBR1oxeYaUIrUTMEOffmeyuJBRAU","timestamp":1627305921964},{"file_id":"1PbEPh6kL5p5cdH5HC8iHoMVCIzA0MqvB","timestamp":1627284576770},{"file_id":"1TlxQ4e-ZX1Zy51dKLuhNdrBWg1qhojqP","timestamp":1627273765934},{"file_id":"17a4F4aC9L0QBqU8BRTrdqPn0WwJ0b08b","timestamp":1626746992716},{"file_id":"1G_W9irFTrEmDeHR0S6_u0bjpk8nxipXW","timestamp":1626689695352},{"file_id":"1bhhkorT--y8XXaVLM8hibVgC-tLqZ16P","timestamp":1626358153868},{"file_id":"1WtT2hX6O9Qbt_hb9sF50nM2QmDXFi-XA","timestamp":1626338366006},{"file_id":"1k_p5wftcUeo711Xho1-T5an2Xkneau-J","timestamp":1626323813472},{"file_id":"1Vz2GB2BNTWuefEFkCSh3TBPEIel7KG1t","timestamp":1626317426487},{"file_id":"1djoMWojeaIPopG5tS1jNMohn8ineblRh","timestamp":1626306831897},{"file_id":"1-6tlDO8158Pi6TpptIF884oFaEiT4Uxb","timestamp":1626276420047},{"file_id":"1js8eA3mDNS8mwSpCiHuzPeARFlUPAVrg","timestamp":1626272452526},{"file_id":"1yhcPgulwJtjJKUK9IuRKmNMhJ-4YXGol","timestamp":1626267205517},{"file_id":"1mnnSv0Pofn1QxArywV81VYqnZPB8uUWN","timestamp":1626180468522},{"file_id":"1RRdjt_UAeHmr5QQBAMyC82Fq1s31OWdK","timestamp":1625833136005},{"file_id":"1JPgg44HFemzwk8VSCXih3PejL0idy-C4","timestamp":1625825483466},{"file_id":"1Ye6wqVX71xAAAhmjXkw9IpRvTqeUyJDA","timestamp":1625812137500}],"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}