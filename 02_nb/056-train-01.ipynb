{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"056-train-01.ipynb","provenance":[{"file_id":"1xPfSbmykBGlw1mj57IfRZQMh5-IZiHrt","timestamp":1627571352916},{"file_id":"17BUK8yRF7SDX0khlOXoHcFRTEFffkvRb","timestamp":1627488927996},{"file_id":"1wNpTEKAuuKP7ivTcm1f9j0sdmYU1RyzA","timestamp":1627306793279},{"file_id":"1uE__yBR1oxeYaUIrUTMEOffmeyuJBRAU","timestamp":1627305921964},{"file_id":"1PbEPh6kL5p5cdH5HC8iHoMVCIzA0MqvB","timestamp":1627284576770},{"file_id":"1TlxQ4e-ZX1Zy51dKLuhNdrBWg1qhojqP","timestamp":1627273765934},{"file_id":"17a4F4aC9L0QBqU8BRTrdqPn0WwJ0b08b","timestamp":1626746992716},{"file_id":"1G_W9irFTrEmDeHR0S6_u0bjpk8nxipXW","timestamp":1626689695352},{"file_id":"1bhhkorT--y8XXaVLM8hibVgC-tLqZ16P","timestamp":1626358153868},{"file_id":"1WtT2hX6O9Qbt_hb9sF50nM2QmDXFi-XA","timestamp":1626338366006},{"file_id":"1k_p5wftcUeo711Xho1-T5an2Xkneau-J","timestamp":1626323813472},{"file_id":"1Vz2GB2BNTWuefEFkCSh3TBPEIel7KG1t","timestamp":1626317426487},{"file_id":"1djoMWojeaIPopG5tS1jNMohn8ineblRh","timestamp":1626306831897},{"file_id":"1-6tlDO8158Pi6TpptIF884oFaEiT4Uxb","timestamp":1626276420047},{"file_id":"1js8eA3mDNS8mwSpCiHuzPeARFlUPAVrg","timestamp":1626272452526},{"file_id":"1yhcPgulwJtjJKUK9IuRKmNMhJ-4YXGol","timestamp":1626267205517},{"file_id":"1mnnSv0Pofn1QxArywV81VYqnZPB8uUWN","timestamp":1626180468522},{"file_id":"1RRdjt_UAeHmr5QQBAMyC82Fq1s31OWdK","timestamp":1625833136005},{"file_id":"1JPgg44HFemzwk8VSCXih3PejL0idy-C4","timestamp":1625825483466},{"file_id":"1Ye6wqVX71xAAAhmjXkw9IpRvTqeUyJDA","timestamp":1625812137500}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucCbvGD1XvG7","executionInfo":{"status":"ok","timestamp":1627576607324,"user_tz":-540,"elapsed":303,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"9cef8594-aee5-4a5a-be73-752b6e2c5ebe"},"source":["import sys\n","if 'google.colab' in sys.modules:  # colab特有の処理_2回目以降\n","  # Google Driveのマウント\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  # ライブラリのパス指定\n","  sys.path.append('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FACwJ6icpxrR"},"source":["# データセットをDriveから取得\n","!mkdir -p 'input'\n","!mkdir -p 'clrp-pre-trained'\n","\n","!cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input/commonlitreadabilityprize/' '/content/input'\n","!cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch5/pre-trained-roberta/clrp_roberta_large/' '/content/clrp-pre-trained'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RV9-VwbpZLZ9"},"source":["from pathlib import Path\n","\n","# input\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    DATA_DIR = Path('../input/commonlitreadabilityprize/')\n","\n","elif 'google.colab' in sys.modules: # Colab環境\n","    DATA_DIR = Path('/content/input/commonlitreadabilityprize')\n","\n","else:\n","    DATA_DIR = Path('../00_input/commonlitreadabilityprize/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5difyXe00UV"},"source":["from pathlib import Path\n","\n","# tokenizer\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    TOKENIZER_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    TOKENIZER_DIR = '/content/clrp-pre-trained/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    TOKENIZER_DIR = 'roberta-large'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKjsUxnOeDYl"},"source":["from pathlib import Path\n","\n","# pre-trained model\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    PRE_TRAINED_MODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    PRE_TRAINED_MODEL_DIR = '/content/clrp-pre-trained/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    PRE_TRAINED_MODEL_DIR = 'roberta-large'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLaT2V0ReoAZ"},"source":["UPLOAD_DIR = Path('/content/model')\n","EX_NO = '056-train-01'  # 実験番号などを入れる、folderのpathにする\n","USERID = 'calpis10000'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOGjAb4pAJ0F"},"source":["import subprocess\n","import shlex\n","\n","def gpuinfo():\n","    \"\"\"\n","    Returns size of total GPU RAM and used GPU RAM.\n","\n","    Parameters\n","    ----------\n","    None\n","\n","    Returns\n","    -------\n","    info : dict\n","        Total GPU RAM in integer for key 'total_MiB'.\n","        Used GPU RAM in integer for key 'used_MiB'.\n","    \"\"\"\n","\n","    command = 'nvidia-smi -q -d MEMORY | sed -n \"/FB Memory Usage/,/Free/p\" | sed -e \"1d\" -e \"4d\" -e \"s/ MiB//g\" | cut -d \":\" -f 2 | cut -c2-'\n","    commands = [shlex.split(part) for part in command.split(' | ')]\n","    for i, cmd in enumerate(commands):\n","        if i==0:\n","            res = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n","        else:\n","            res = subprocess.Popen(cmd, stdin=res.stdout, stdout=subprocess.PIPE)\n","    total, used = map(int, res.communicate()[0].decode('utf-8').strip().split('\\n'))\n","    info = {'total_MiB':total, 'used_MiB':used}\n","    return info\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g3-6m5MKXecB"},"source":["# Overview\n","This nb is based on copy from https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch .\n","\n","Acknowledgments(from base nb): \n","some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T06:26:32.834365Z","iopub.execute_input":"2021-07-04T06:26:32.834903Z","iopub.status.idle":"2021-07-04T06:26:40.143740Z","shell.execute_reply.started":"2021-07-04T06:26:32.834785Z","shell.execute_reply":"2021-07-04T06:26:40.142864Z"},"trusted":true,"id":"HRsRZ06WXecD"},"source":["import os\n","import math\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from transformers import AdamW # optimizer\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoConfig\n","from transformers import get_cosine_schedule_with_warmup # scheduler\n","from pytorch_memlab import profile\n","import pytorch_memlab\n","from pytorch_memlab import MemReporter\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gc\n","gc.enable()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.145217Z","iopub.execute_input":"2021-07-04T06:26:40.145539Z","iopub.status.idle":"2021-07-04T06:26:40.201326Z","shell.execute_reply.started":"2021-07-04T06:26:40.145504Z","shell.execute_reply":"2021-07-04T06:26:40.200136Z"},"trusted":true,"id":"omBfwshTXecE"},"source":["NUM_FOLDS = 5 # K Fold\n","NUM_EPOCHS = 5 # Epochs\n","BATCH_SIZE = 12 # Batch Size\n","MAX_LEN = 248 # ベクトル長\n","EVAL_SCHEDULE = [(0.55, 64), (-1., 32)] # schedulerの何らかの設定？\n","ROBERTA_PATH = PRE_TRAINED_MODEL_DIR # roberta pre-trainedモデル(モデルとして指定)\n","TOKENIZER_PATH = TOKENIZER_DIR # roberta pre-trainedモデル(Tokenizerとして指定)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # cudaがなければcpuを使えばいいじゃない"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.203398Z","iopub.execute_input":"2021-07-04T06:26:40.204055Z","iopub.status.idle":"2021-07-04T06:26:40.211572Z","shell.execute_reply.started":"2021-07-04T06:26:40.204015Z","shell.execute_reply":"2021-07-04T06:26:40.210762Z"},"trusted":true,"id":"4qcuXqwtXecF"},"source":["def set_random_seed(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n","\n","    torch.manual_seed(random_seed)\n","    torch.cuda.manual_seed(random_seed)\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","    torch.backends.cudnn.deterministic = True# cudnnによる最適化で結果が変わらないためのおまじない "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.214188Z","iopub.execute_input":"2021-07-04T06:26:40.214809Z","iopub.status.idle":"2021-07-04T06:26:40.309744Z","shell.execute_reply.started":"2021-07-04T06:26:40.214769Z","shell.execute_reply":"2021-07-04T06:26:40.308926Z"},"trusted":true,"id":"70PyLsJTXecF"},"source":["# read train_df(kfold)\n","train_kf_df = pd.read_csv(DATA_DIR/\"train_kfold.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.311021Z","iopub.execute_input":"2021-07-04T06:26:40.311347Z","iopub.status.idle":"2021-07-04T06:26:40.624393Z","shell.execute_reply.started":"2021-07-04T06:26:40.311314Z","shell.execute_reply":"2021-07-04T06:26:40.623347Z"},"trusted":true,"id":"xf0662k4XecF"},"source":["# tokenizerを指定\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6aaghNkXecG"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UU5uZKIcDjkV","executionInfo":{"status":"ok","timestamp":1627576616248,"user_tz":-540,"elapsed":542,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"14309fd2-df90-42bb-d52e-6946116c06c4"},"source":["# 前処理用\n","import string\n","import re\n","# ローカルの場合、stopwordsをダウンロード\n","import nltk\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    pass\n","else:\n","    import nltk\n","    nltk.download('stopwords')\n","    nltk.download('averaged_perceptron_tagger')\n","    os.listdir(os.path.expanduser('~/nltk_data/corpora/stopwords/'))\n","\n","# テキスト前処理\n","# https://www.kaggle.com/alaasedeeq/commonlit-readability-eda\n","\n","#filtering the unwanted symbols, spaces, ....etc\n","to_replace_by_space = re.compile('[/(){}\\[\\]|@,;]')\n","punctuation = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n","bad_symbols = re.compile('[^0-9a-z #+_]')\n","stopwords = set(nltk.corpus.stopwords.words('english'))\n","\n","def text_prepare(text):\n","    '''\n","    text: a string\n","    returna modified version of the string\n","    '''\n","    text = text.lower() # lowercase text\n","    text = re.sub(punctuation, '',text)\n","    text = re.sub(to_replace_by_space, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n","    text = re.sub(bad_symbols, \"\", text)         # delete symbols which are in BAD_SYMBOLS_RE from text\n","    text = \" \".join([word for word in text.split(\" \") if word not in stopwords]) # delete stopwords from text\n","    text = re.sub(' +', ' ', text)\n","    return text\n","\n","def text_normalization(s:pd.Series):\n","    x = s.apply(text_prepare)\n","    return x\n","\n","# Counterオブジェクトを取得\n","def get_counter(text:str):\n","    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n","    counter = collections.Counter(text_list)\n","    return counter\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iAse0IDWDjho"},"source":["# センテンス特徴付きのデータセット\n","class LitDataset(Dataset):\n","    def __init__(self, df, inference_only=False):\n","        super().__init__()\n","\n","        self.df = df        \n","        self.inference_only = inference_only # Testデータ用フラグ\n","        self.text = df.excerpt.tolist() # 分析対象カラムをlistにする。(分かち書きではなく、Seriesをlistへ変換するような処理)\n","        #self.text = [text.replace(\"\\n\", \" \") for text in self.text] # 単語単位で分かち書きする場合\n","        self.text_len = text_normalization(df.excerpt).map(lambda x: [0 if i >= len(x.split(' ')) else len(x.split(' ')[i]) for i in range(132)])\n","        self.sentences = df['excerpt'].map(lambda x: x.split('.')).map(lambda x: [0 if i >= len(x) else len(x[i]) for i in range(36)])\n","\n","        if not self.inference_only:\n","            self.target = torch.tensor(df.target.values, dtype=torch.float32) # trainのみ、targetをtensorに変換\n","            self.standard_error = torch.tensor(df.standard_error.values, dtype=torch.float32) \n","\n","        self.encoded = tokenizer.batch_encode_plus( # textをtokenize\n","            self.text,\n","            padding = 'max_length',            \n","            max_length = MAX_LEN,\n","            truncation = True, # 最大長を超える文字は切り捨て\n","            return_attention_mask=True\n","        )        \n"," \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, index): # 変換結果を返す\n","        input_ids = torch.tensor(self.encoded['input_ids'][index])\n","        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n","        input_len = torch.tensor(self.text_len.iloc[index], dtype=torch.float32)\n","        input_sentence = torch.tensor(self.sentences.iloc[index], dtype=torch.float32)\n","\n","        if self.inference_only:\n","            return (input_ids, attention_mask, input_len, input_sentence)            \n","        else:\n","            target = self.target[index]\n","            standard_error = self.standard_error[index]\n","            return (input_ids, attention_mask, input_len, input_sentence, target, standard_error)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKtdy32wXecG"},"source":["# Model\n","The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.649629Z","iopub.execute_input":"2021-07-04T06:26:40.650066Z","iopub.status.idle":"2021-07-04T06:26:40.666374Z","shell.execute_reply.started":"2021-07-04T06:26:40.650002Z","shell.execute_reply":"2021-07-04T06:26:40.665211Z"},"trusted":true,"id":"BpkxjXEUXecH"},"source":["class LitModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(ROBERTA_PATH) # pretrainedからconfigを読み込み\n","        config.update({\"output_hidden_states\":True, # config更新: embedding層を抽出\n","                       \"hidden_dropout_prob\": 0.0, # config更新: dropoutしない\n","                       \"layer_norm_eps\": 1e-7}) # config更新: layer normalizationのepsilon                      \n","        \n","        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config) # cpuで処理する\n","            \n","        self.attention = nn.Sequential(# attentionレイヤー            \n","            nn.Linear(config.hidden_size, 512),      \n","            nn.Tanh(),                       \n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","\n","        self.mlm_layers = nn.Sequential(\n","            nn.Linear(132, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","        )\n","\n","        self.mlm_sentence = nn.Sequential(\n","            nn.Linear(36, 18),\n","            nn.BatchNorm1d(18),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(18, 18),\n","        )\n","\n","\n","        self.regressor = nn.Sequential( # 出力レイヤー                    \n","            nn.Linear(config.hidden_size + 64 + 18, 2)                        \n","        )\n","\n","    def forward(self, input_ids, attention_mask, input_len, input_sentence):\n","        roberta_output = self.roberta(input_ids=input_ids, # robertaに入力データを流し、出力としてrobertaモデル(layerの複合体)を得る\n","                                      attention_mask=attention_mask)     \n","        # attention_pooling\n","        last_hidden_state = roberta_output.hidden_states[-1] # robertaモデルの最後のlayerを得る\n","        weights = self.attention(last_hidden_state) # robertaの最後のlayerをattentionへ入力し、出力として重みを得る                \n","        context_vector = torch.sum(weights * last_hidden_state, dim=1) # 重み×最後の層を足し合わせて文書ベクトルとする。\n","\n","        # word_length_conv1d\n","        #input_chnl = input_len.unsqueeze(1)\n","        #conv1_layers = self.conv1_layers(input_chnl)\n","        #conv1_layers_v = conv1_layers.view(conv1_layers.size(0),-1)\n","\n","        # word_length_mlm\n","        mlm_layers = self.mlm_layers(input_len)\n","        mlm_sentence = self.mlm_sentence(input_sentence)\n","\n","\n","        # https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently\n","        # last_hidden_state = roberta_output[0]\n","        # input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        # sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        # sum_mask = input_mask_expanded.sum(1)\n","        # sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        # mean_embeddings = sum_embeddings / sum_mask\n","\n","        # concat_embeddings\n","        cat_embeddings = torch.cat([context_vector, mlm_layers, mlm_sentence], dim=1)        \n","        return self.regressor(cat_embeddings) # 文書ベクトルを線形層に入力し、targetを出力する\n","        \n","        # Now we reduce the context vector to the prediction score.\n","        #return self.regressor(mean_embeddings) # 文書ベクトルを線形層に入力し、targetを出力する"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.672515Z","iopub.execute_input":"2021-07-04T06:26:40.672944Z","iopub.status.idle":"2021-07-04T06:26:40.684593Z","shell.execute_reply.started":"2021-07-04T06:26:40.672908Z","shell.execute_reply":"2021-07-04T06:26:40.683569Z"},"trusted":true,"id":"bB4jvQTxXecH"},"source":["def eval_mse(model, data_loader):\n","    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモードを選択。Batch Normとかdropoutをしなくなる           \n","    mse_mean_sum = 0\n","    mse_std_sum = 0\n","\n","    with torch.no_grad(): # 勾配の計算をしないBlock\n","        for batch_num, (input_ids, attention_mask, input_len, input_sentence, target, standard_error) in enumerate(data_loader): # data_loaderからinput, attentin_mask, targetをbatchごとに取り出す\n","            input_ids = input_ids.to(DEVICE)   \n","            attention_mask = attention_mask.to(DEVICE)  \n","            input_len = input_len.to(DEVICE) \n","            input_sentence = input_sentence.to(DEVICE)\n","            target = target.to(DEVICE)      \n","            standard_error = standard_error.to(DEVICE) \n","            \n","            output = model(input_ids, attention_mask, input_len, input_sentence) # 取得した値をモデルへ入力し、出力として予測値を得る。\n","\n","            mse_mean_sum += nn.MSELoss(reduction=\"sum\")(output[:,0].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","            mse_std_sum += nn.MSELoss(reduction=\"sum\")(output[:,1].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","\n","    del input_ids\n","    del attention_mask\n","    del target\n","\n","    mse_mean_result = mse_mean_sum / len(data_loader.dataset)\n","    mse_std_result = mse_std_sum / len(data_loader.dataset)\n","  \n","    return mse_mean_result, mse_std_result # 誤差の合計をdataset長で除し、mseを取得＆返す"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.690155Z","iopub.execute_input":"2021-07-04T06:26:40.692530Z","iopub.status.idle":"2021-07-04T06:26:40.703425Z","shell.execute_reply.started":"2021-07-04T06:26:40.692488Z","shell.execute_reply":"2021-07-04T06:26:40.702366Z"},"trusted":true,"id":"47bDno_LXecI"},"source":["# 推論結果を返す\n","def predict(model, data_loader):\n","    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモード(dropout, batch_normしない)\n","\n","    result = np.zeros(len(data_loader.dataset)) # 結果をdataset長のzero配列として用意\n","    index = 0\n","    \n","    with torch.no_grad(): # 勾配の計算をしないblock(inputすると、現状の重みによる推論結果を返す)\n","        for batch_num, (input_ids, attention_mask, input_len, input_sentence) in enumerate(data_loader): # data_loaderからbatchごとにinputを得る\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)\n","            input_len = input_len.to(DEVICE)\n","            input_sentence = input_sentence.to(DEVICE)\n","                        \n","            output = model(input_ids, attention_mask, input_len, input_sentence) # modelにinputを入力し、予測結果を得る。\n","\n","            result[index : index + output[:,0].shape[0]] = output[:,0].flatten().to(\"cpu\") # result[index ~ predの長さ]へ、予測結果を格納\n","            index += output.shape[0] # indexを更新\n","\n","    return result # 全batchで推論が終わったら、結果を返す"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.708605Z","iopub.execute_input":"2021-07-04T06:26:40.709024Z","iopub.status.idle":"2021-07-04T06:26:40.730675Z","shell.execute_reply.started":"2021-07-04T06:26:40.708983Z","shell.execute_reply":"2021-07-04T06:26:40.729705Z"},"trusted":true,"id":"oInneuAmXecI"},"source":["# 学習\n","def train(model, # モデル\n","          model_path, # モデルのアウトプット先\n","          train_loader, # train-setのdata_loader\n","          val_loader, # valid-setのdata_loader\n","          optimizer, # optimizer\n","          scheduler=None, # scheduler, デフォルトはNone\n","          num_epochs=NUM_EPOCHS # epoch数、notebook冒頭で指定した値\n","         ):    \n","    \n","    best_val_rmse = None\n","    best_epoch = 0\n","    step = 0\n","    last_eval_step = 0\n","    eval_period = EVAL_SCHEDULE[0][1] # eval期間(って何？) 冒頭で決めたEVAL_SCHEDULEの最初のtupleの[1]を取得\n","\n","    start = time.time() # 時間計測用\n","\n","    for epoch in range(num_epochs): # 指定したEpoch数だけ繰り返し\n","        val_rmse = None         \n","\n","        for batch_num, (input_ids, attention_mask, input_len, input_sentence, target, standard_error) in enumerate(train_loader): # train_loaderからinput, targetを取得\n","            input_ids = input_ids.to(DEVICE) # inputをDEVICEへ突っ込む\n","            attention_mask = attention_mask.to(DEVICE)   \n","            input_len = input_len.to(DEVICE)\n","            input_sentence = input_sentence.to(DEVICE)\n","            target = target.to(DEVICE)\n","            standard_error = standard_error.to(DEVICE)  \n","\n","            optimizer.zero_grad() # 勾配を初期化            \n","            model.train() # 学習モード開始\n","\n","            # https://www.kaggle.com/c/commonlitreadabilityprize/discussion/239421\n","            output = model(input_ids, attention_mask, input_len, input_sentence) # input,attention_maskを入力し、予測結果を得る\n","            p = torch.distributions.Normal(output[:,0], torch.sqrt(output[:,1]**2))\n","            q = torch.distributions.Normal(target, standard_error)\n","            kl_vector = torch.distributions.kl_divergence(p, q)\n","            loss = kl_vector.mean()\n","\n","            loss.backward() # 誤差逆伝播法により勾配を得る\n","            optimizer.step() # 重みを更新する\n","\n","            if scheduler:\n","                scheduler.step() # schedulerが与えられた場合は、schedulerの学習率更新\n","            \n","            if step >= last_eval_step + eval_period: # batchを回すごとにstepを増やしていって、「前回evalしたstep + eval_period(16)」を超えたら実行。\n","                # Evaluate the model on val_loader.\n","                elapsed_seconds = time.time() - start # 経過時間\n","                num_steps = step - last_eval_step # 経過ステップ数\n","                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n","                last_eval_step = step # 前回stepの更新\n","                \n","                # valid-setによるrmse計算\n","                train_mean_mse = nn.MSELoss(reduction=\"mean\")(output[:,0].flatten(), target) \n","                train_std_mse = nn.MSELoss(reduction=\"mean\")(torch.sqrt(output[:,1]**2).flatten(), standard_error) \n","\n","                train_mean_rmse = math.sqrt(train_mean_mse)\n","                train_std_rmse = math.sqrt(train_std_mse)\n","\n","                val_mean_mse, val_std_mse = eval_mse(model, val_loader)\n","                val_mean_rmse = math.sqrt(val_mean_mse)                            \n","                val_std_rmse = math.sqrt(val_std_mse)                            \n","\n","                print(f\"Epoch: {epoch} batch_num: {batch_num}\")\n","                print(f\"train_rmse_target: {train_mean_rmse:0.4}\",\n","                      f\"train_rmse_stderror: {train_std_rmse:0.4}\",\n","                      f\"train_kl_div: {loss:0.4}\",\n","                      )\n","                print(f\"val_rmse_target: {val_mean_rmse:0.4}\",\n","                      f\"val_rmse_stderror: {val_std_rmse:0.4}\"\n","                      )\n","\n","                for rmse, period in EVAL_SCHEDULE: # eval_periodをvalid-rmseで切り替える処理\n","                    if val_mean_rmse >= rmse: # valid rmseをEVAL_SCHEDULEと比較し、0項 > valid rmseとなるまで回す : EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n","                        eval_period = period # eval_periodを更新\n","                        break                               \n","\n","                if not best_val_rmse or val_mean_rmse < best_val_rmse: # 初回(best_val_rmse==None), またはbest_val_rmseを更新したらモデルを保存する\n","                    best_val_rmse = val_mean_rmse\n","                    best_epoch = epoch\n","                    torch.save(model.state_dict(), model_path) # 最高の自分を保存\n","                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n","                else:       \n","                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\", # 更新されない場合は、元のスコアを表示\n","                          f\"(from epoch {best_epoch})\")      \n","                                                  \n","                start = time.time()\n","            \n","            # batchごとにメモリ解放\n","            del input_ids\n","            del attention_mask\n","            del target\n","            torch.cuda.empty_cache()                                            \n","            step += 1\n","    \n","    return best_val_rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.735798Z","iopub.execute_input":"2021-07-04T06:26:40.738398Z","iopub.status.idle":"2021-07-04T06:26:40.750876Z","shell.execute_reply.started":"2021-07-04T06:26:40.738356Z","shell.execute_reply":"2021-07-04T06:26:40.749635Z"},"trusted":true,"id":"rMY0fjXwXecJ"},"source":["# optimizerの作成\n","def create_optimizer(model):\n","    parameters = []\n","\n","    named_parameters = list(model.named_parameters()) # モデルパラメータの取得\n","    roberta_parameters = list(model.roberta.named_parameters())[:-2] # パラメータをroberta用、attention用、regressor用に格納。(直接引っ張ってくる形式に変更)\n","\n","    attention_parameters = list(model.attention.named_parameters())\n","    attention_group = [{'params': params, 'lr': 2e-5} for (name, params) in attention_parameters] # attention用パラメータをリストとして取得\n","    parameters += attention_group\n","\n","    #norm_parameters = list(model.layer_norm.named_parameters())\n","    #norm_group = [{'params': params, 'lr': 2e-5} for (name, params) in norm_parameters]\n","    #parameters += norm_group\n","\n","    mlm_parameters = list(model.mlm_layers.named_parameters())\n","    mlm_group = [{'params': params, 'lr': 2e-5} for (name, params) in mlm_parameters] # reg用パラメータをリストとして取得\n","    parameters += mlm_group\n","\n","    mlm_sent_parameters = list(model.mlm_sentence.named_parameters())\n","    mlm_sent_group = [{'params': params, 'lr': 2e-6, 'weight_decay': 0.0 if \"bias\" in name else 0.01} for (name, params) in mlm_sent_parameters] # reg用パラメータをリストとして取得\n","    parameters += mlm_sent_group\n","\n","    regressor_parameters = list(model.regressor.named_parameters())\n","    regressor_group = [{'params': params, 'lr': 2e-5} for (name, params) in regressor_parameters] # reg用パラメータをリストとして取得\n","    parameters += regressor_group\n","\n","    for layer_num, (name, params) in enumerate(roberta_parameters): # レイヤーごとにname, paramsを取得していろんな処理\n","        weight_decay = 0.0 if \"bias\" in name else 0.01\n","\n","        lr = 8e-6\n","\n","        if layer_num >= 69:        \n","            lr = 2e-5\n","\n","        if layer_num >= 133:\n","            lr = 4e-5\n","\n","        parameters.append({\"params\": params,\n","                           \"weight_decay\": weight_decay,\n","                           \"lr\": lr})\n","\n","    return AdamW(parameters) # 最終的に、AdamWにパラメータを入力する。\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PLKHwvKtNBn"},"source":["def train_and_save_model(train_indices, val_indices, model_path):\n","    train_dataset = LitDataset(train_kf_df.loc[train_indices]) # train, validのDataset\n","    val_dataset = LitDataset(train_kf_df.loc[val_indices])\n","        \n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              drop_last=True, shuffle=True, num_workers=2) # train, validのDataLoader\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                            drop_last=False, shuffle=False, num_workers=2)    \n","\n","    model = LitModel().to(DEVICE) # modelをDEVICEへぶち込む\n","    optimizer = create_optimizer(model) # optimizerをモデルから作成\n","    scheduler = get_cosine_schedule_with_warmup( # schedulerを作成\n","        optimizer,\n","        num_training_steps=NUM_EPOCHS * len(train_loader),\n","        num_warmup_steps=50)    \n","    rmse = train(model, model_path, train_loader, val_loader, optimizer, scheduler=scheduler)\n","\n","    del train_dataset\n","    del val_dataset\n","    del train_loader\n","    del val_loader\n","    del model\n","    del optimizer\n","    del scheduler\n","    gc.collect() \n","    torch.cuda.empty_cache()\n","    return rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.755813Z","iopub.execute_input":"2021-07-04T06:26:40.758373Z","iopub.status.idle":"2021-07-04T06:27:12.493221Z","shell.execute_reply.started":"2021-07-04T06:26:40.758265Z","shell.execute_reply":"2021-07-04T06:27:12.490139Z"},"trusted":true,"id":"k2LGJD3XXecK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627581583946,"user_tz":-540,"elapsed":4967147,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"ca0c6c66-3d56-42f8-d3e8-bff06c9129a1"},"source":["# 実行処理。 KFold & 学習\n","SEED = 1000\n","list_val_rmse = []\n","\n","for fold in sorted(train_kf_df['kfold'].unique()):\n","    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n","    print(gpuinfo())\n","    model_path = f\"model_{fold + 1}.pth\" # model_fold数_.pth\n","    set_random_seed(SEED + fold) # SEEDはfold別に変わるようにする\n","\n","    train_indices = (train_kf_df['kfold'] != fold)\n","    val_indices = (train_kf_df['kfold'] == fold)\n","    list_val_rmse.append(train_and_save_model(train_indices, val_indices, model_path))\n","    print(\"\\nPerformance estimates:\")\n","    print(list_val_rmse)\n","    print(\"Mean:\", np.array(list_val_rmse).mean())\n","    print(gpuinfo())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Fold 1/5\n","{'total_MiB': 16160, 'used_MiB': 2}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 49.4 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.8074 train_rmse_stderror: 0.119 train_kl_div: 1.228\n","val_rmse_target: 0.6574 val_rmse_stderror: 1.19\n","New best_val_rmse: 0.6574\n","\n","64 steps took 48.9 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.5232 train_rmse_stderror: 0.1063 train_kl_div: 0.5665\n","val_rmse_target: 0.5584 val_rmse_stderror: 1.177\n","New best_val_rmse: 0.5584\n","\n","64 steps took 49.2 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.3609 train_rmse_stderror: 0.1205 train_kl_div: 0.3178\n","val_rmse_target: 0.5621 val_rmse_stderror: 1.126\n","Still best_val_rmse: 0.5584 (from epoch 0)\n","\n","64 steps took 49.0 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.7639 train_rmse_stderror: 0.06456 train_kl_div: 1.155\n","val_rmse_target: 0.5918 val_rmse_stderror: 1.15\n","Still best_val_rmse: 0.5584 (from epoch 0)\n","\n","64 steps took 49.0 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.5466 train_rmse_stderror: 0.09279 train_kl_div: 0.6734\n","val_rmse_target: 0.5267 val_rmse_stderror: 1.146\n","New best_val_rmse: 0.5267\n","\n","32 steps took 24.5 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.3615 train_rmse_stderror: 0.05737 train_kl_div: 0.3044\n","val_rmse_target: 0.5598 val_rmse_stderror: 1.158\n","Still best_val_rmse: 0.5267 (from epoch 1)\n","\n","64 steps took 49.3 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.3047 train_rmse_stderror: 0.05245 train_kl_div: 0.2009\n","val_rmse_target: 0.5247 val_rmse_stderror: 1.15\n","New best_val_rmse: 0.5247\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2693 train_rmse_stderror: 0.0738 train_kl_div: 0.1886\n","val_rmse_target: 0.5329 val_rmse_stderror: 1.169\n","Still best_val_rmse: 0.5247 (from epoch 2)\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.2536 train_rmse_stderror: 0.0655 train_kl_div: 0.1461\n","val_rmse_target: 0.5094 val_rmse_stderror: 1.162\n","New best_val_rmse: 0.5094\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.3235 train_rmse_stderror: 0.0845 train_kl_div: 0.2491\n","val_rmse_target: 0.505 val_rmse_stderror: 1.121\n","New best_val_rmse: 0.505\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.2009 train_rmse_stderror: 0.05289 train_kl_div: 0.08879\n","val_rmse_target: 0.509 val_rmse_stderror: 1.141\n","Still best_val_rmse: 0.505 (from epoch 2)\n","\n","32 steps took 24.8 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.1302 train_rmse_stderror: 0.06827 train_kl_div: 0.05611\n","val_rmse_target: 0.5066 val_rmse_stderror: 1.17\n","Still best_val_rmse: 0.505 (from epoch 2)\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.135 train_rmse_stderror: 0.05583 train_kl_div: 0.05272\n","val_rmse_target: 0.5059 val_rmse_stderror: 1.126\n","Still best_val_rmse: 0.505 (from epoch 2)\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1232 train_rmse_stderror: 0.03846 train_kl_div: 0.03725\n","val_rmse_target: 0.5036 val_rmse_stderror: 1.151\n","New best_val_rmse: 0.5036\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1515 train_rmse_stderror: 0.04168 train_kl_div: 0.05381\n","val_rmse_target: 0.4984 val_rmse_stderror: 1.148\n","New best_val_rmse: 0.4984\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1826 train_rmse_stderror: 0.06977 train_kl_div: 0.09498\n","val_rmse_target: 0.5017 val_rmse_stderror: 1.147\n","Still best_val_rmse: 0.4984 (from epoch 3)\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.1822 train_rmse_stderror: 0.04307 train_kl_div: 0.07806\n","val_rmse_target: 0.5005 val_rmse_stderror: 1.157\n","Still best_val_rmse: 0.4984 (from epoch 3)\n","\n","32 steps took 24.8 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1187 train_rmse_stderror: 0.07773 train_kl_div: 0.04744\n","val_rmse_target: 0.5009 val_rmse_stderror: 1.151\n","Still best_val_rmse: 0.4984 (from epoch 3)\n","\n","32 steps took 24.5 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.05999 train_rmse_stderror: 0.04537 train_kl_div: 0.01565\n","val_rmse_target: 0.4989 val_rmse_stderror: 1.147\n","Still best_val_rmse: 0.4984 (from epoch 3)\n","\n","32 steps took 24.5 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.08433 train_rmse_stderror: 0.0553 train_kl_div: 0.02942\n","val_rmse_target: 0.4974 val_rmse_stderror: 1.155\n","New best_val_rmse: 0.4974\n","\n","32 steps took 24.4 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.08162 train_rmse_stderror: 0.05479 train_kl_div: 0.02589\n","val_rmse_target: 0.498 val_rmse_stderror: 1.154\n","Still best_val_rmse: 0.4974 (from epoch 4)\n","\n","32 steps took 24.5 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.1083 train_rmse_stderror: 0.06896 train_kl_div: 0.03542\n","val_rmse_target: 0.4983 val_rmse_stderror: 1.153\n","Still best_val_rmse: 0.4974 (from epoch 4)\n","\n","32 steps took 24.5 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.113 train_rmse_stderror: 0.0467 train_kl_div: 0.03337\n","val_rmse_target: 0.4986 val_rmse_stderror: 1.152\n","Still best_val_rmse: 0.4974 (from epoch 4)\n","\n","Performance estimates:\n","[0.4973942334383366]\n","Mean: 0.4973942334383366\n","{'total_MiB': 16160, 'used_MiB': 1359}\n","\n","Fold 2/5\n","{'total_MiB': 16160, 'used_MiB': 1359}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 49.5 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.5625 train_rmse_stderror: 0.08782 train_kl_div: 0.6014\n","val_rmse_target: 0.5882 val_rmse_stderror: 1.128\n","New best_val_rmse: 0.5882\n","\n","64 steps took 49.0 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.5019 train_rmse_stderror: 0.0372 train_kl_div: 0.4936\n","val_rmse_target: 0.5716 val_rmse_stderror: 1.124\n","New best_val_rmse: 0.5716\n","\n","64 steps took 49.3 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.4507 train_rmse_stderror: 0.07281 train_kl_div: 0.4024\n","val_rmse_target: 0.5255 val_rmse_stderror: 1.132\n","New best_val_rmse: 0.5255\n","\n","32 steps took 24.5 seconds\n","Epoch: 1 batch_num: 36\n","train_rmse_target: 0.4636 train_rmse_stderror: 0.05795 train_kl_div: 0.4124\n","val_rmse_target: 0.5703 val_rmse_stderror: 1.13\n","Still best_val_rmse: 0.5255 (from epoch 1)\n","\n","64 steps took 49.1 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.4297 train_rmse_stderror: 0.0797 train_kl_div: 0.3945\n","val_rmse_target: 0.5094 val_rmse_stderror: 1.146\n","New best_val_rmse: 0.5094\n","\n","32 steps took 24.5 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.4538 train_rmse_stderror: 0.09472 train_kl_div: 0.4294\n","val_rmse_target: 0.4889 val_rmse_stderror: 1.139\n","New best_val_rmse: 0.4889\n","\n","32 steps took 24.5 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.8476 train_rmse_stderror: 0.0559 train_kl_div: 1.368\n","val_rmse_target: 0.5893 val_rmse_stderror: 1.135\n","Still best_val_rmse: 0.4889 (from epoch 1)\n","\n","64 steps took 49.4 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2688 train_rmse_stderror: 0.07151 train_kl_div: 0.1659\n","val_rmse_target: 0.4856 val_rmse_stderror: 1.145\n","New best_val_rmse: 0.4856\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.3017 train_rmse_stderror: 0.0422 train_kl_div: 0.1851\n","val_rmse_target: 0.5013 val_rmse_stderror: 1.147\n","Still best_val_rmse: 0.4856 (from epoch 2)\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.2046 train_rmse_stderror: 0.04823 train_kl_div: 0.09947\n","val_rmse_target: 0.4926 val_rmse_stderror: 1.148\n","Still best_val_rmse: 0.4856 (from epoch 2)\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.3934 train_rmse_stderror: 0.07314 train_kl_div: 0.3448\n","val_rmse_target: 0.4895 val_rmse_stderror: 1.14\n","Still best_val_rmse: 0.4856 (from epoch 2)\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.2792 train_rmse_stderror: 0.05536 train_kl_div: 0.1708\n","val_rmse_target: 0.5121 val_rmse_stderror: 1.169\n","Still best_val_rmse: 0.4856 (from epoch 2)\n","\n","32 steps took 24.8 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.1856 train_rmse_stderror: 0.0504 train_kl_div: 0.06136\n","val_rmse_target: 0.4761 val_rmse_stderror: 1.139\n","New best_val_rmse: 0.4761\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.1556 train_rmse_stderror: 0.07853 train_kl_div: 0.06967\n","val_rmse_target: 0.4761 val_rmse_stderror: 1.149\n","Still best_val_rmse: 0.4761 (from epoch 3)\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1427 train_rmse_stderror: 0.05151 train_kl_div: 0.04716\n","val_rmse_target: 0.4773 val_rmse_stderror: 1.14\n","Still best_val_rmse: 0.4761 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1051 train_rmse_stderror: 0.0515 train_kl_div: 0.03298\n","val_rmse_target: 0.4771 val_rmse_stderror: 1.153\n","Still best_val_rmse: 0.4761 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1443 train_rmse_stderror: 0.05075 train_kl_div: 0.05122\n","val_rmse_target: 0.4791 val_rmse_stderror: 1.149\n","Still best_val_rmse: 0.4761 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.1436 train_rmse_stderror: 0.04033 train_kl_div: 0.04848\n","val_rmse_target: 0.4753 val_rmse_stderror: 1.15\n","New best_val_rmse: 0.4753\n","\n","32 steps took 24.8 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1025 train_rmse_stderror: 0.05595 train_kl_div: 0.03202\n","val_rmse_target: 0.4748 val_rmse_stderror: 1.144\n","New best_val_rmse: 0.4748\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1211 train_rmse_stderror: 0.04743 train_kl_div: 0.03922\n","val_rmse_target: 0.4745 val_rmse_stderror: 1.146\n","New best_val_rmse: 0.4745\n","\n","32 steps took 24.5 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.0904 train_rmse_stderror: 0.06984 train_kl_div: 0.03692\n","val_rmse_target: 0.4748 val_rmse_stderror: 1.143\n","Still best_val_rmse: 0.4745 (from epoch 4)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.1095 train_rmse_stderror: 0.04711 train_kl_div: 0.03557\n","val_rmse_target: 0.4741 val_rmse_stderror: 1.147\n","New best_val_rmse: 0.4741\n","\n","32 steps took 24.5 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.06633 train_rmse_stderror: 0.04659 train_kl_div: 0.0185\n","val_rmse_target: 0.4739 val_rmse_stderror: 1.145\n","New best_val_rmse: 0.4739\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1544 train_rmse_stderror: 0.03788 train_kl_div: 0.05049\n","val_rmse_target: 0.474 val_rmse_stderror: 1.144\n","Still best_val_rmse: 0.4739 (from epoch 4)\n","\n","Performance estimates:\n","[0.4973942334383366, 0.4739272001314702]\n","Mean: 0.4856607167849034\n","{'total_MiB': 16160, 'used_MiB': 1359}\n","\n","Fold 3/5\n","{'total_MiB': 16160, 'used_MiB': 1359}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 49.7 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.6728 train_rmse_stderror: 0.05428 train_kl_div: 0.9045\n","val_rmse_target: 0.7901 val_rmse_stderror: 1.071\n","New best_val_rmse: 0.7901\n","\n","64 steps took 49.1 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.6774 train_rmse_stderror: 0.08874 train_kl_div: 1.073\n","val_rmse_target: 0.5678 val_rmse_stderror: 1.1\n","New best_val_rmse: 0.5678\n","\n","64 steps took 49.3 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.4496 train_rmse_stderror: 0.1939 train_kl_div: 0.5824\n","val_rmse_target: 0.5681 val_rmse_stderror: 1.054\n","Still best_val_rmse: 0.5678 (from epoch 0)\n","\n","64 steps took 49.2 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.516 train_rmse_stderror: 0.08223 train_kl_div: 0.5063\n","val_rmse_target: 0.5408 val_rmse_stderror: 1.104\n","New best_val_rmse: 0.5408\n","\n","32 steps took 24.5 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.4067 train_rmse_stderror: 0.06189 train_kl_div: 0.334\n","val_rmse_target: 0.5239 val_rmse_stderror: 1.102\n","New best_val_rmse: 0.5239\n","\n","32 steps took 24.5 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.4168 train_rmse_stderror: 0.07823 train_kl_div: 0.3799\n","val_rmse_target: 0.5404 val_rmse_stderror: 1.124\n","Still best_val_rmse: 0.5239 (from epoch 1)\n","\n","32 steps took 24.6 seconds\n","Epoch: 1 batch_num: 164\n","train_rmse_target: 0.4385 train_rmse_stderror: 0.08566 train_kl_div: 0.4594\n","val_rmse_target: 0.5137 val_rmse_stderror: 1.099\n","New best_val_rmse: 0.5137\n","\n","32 steps took 24.8 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.2654 train_rmse_stderror: 0.07365 train_kl_div: 0.1727\n","val_rmse_target: 0.518 val_rmse_stderror: 1.125\n","Still best_val_rmse: 0.5137 (from epoch 1)\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.3483 train_rmse_stderror: 0.05869 train_kl_div: 0.2568\n","val_rmse_target: 0.4969 val_rmse_stderror: 1.103\n","New best_val_rmse: 0.4969\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.4518 train_rmse_stderror: 0.1032 train_kl_div: 0.4117\n","val_rmse_target: 0.6158 val_rmse_stderror: 1.137\n","Still best_val_rmse: 0.4969 (from epoch 2)\n","\n","64 steps took 49.1 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.3157 train_rmse_stderror: 0.07093 train_kl_div: 0.2046\n","val_rmse_target: 0.4976 val_rmse_stderror: 1.109\n","Still best_val_rmse: 0.4969 (from epoch 2)\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.3252 train_rmse_stderror: 0.04498 train_kl_div: 0.207\n","val_rmse_target: 0.4932 val_rmse_stderror: 1.117\n","New best_val_rmse: 0.4932\n","\n","32 steps took 24.8 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.2816 train_rmse_stderror: 0.04543 train_kl_div: 0.1612\n","val_rmse_target: 0.5182 val_rmse_stderror: 1.11\n","Still best_val_rmse: 0.4932 (from epoch 2)\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.2384 train_rmse_stderror: 0.05701 train_kl_div: 0.1329\n","val_rmse_target: 0.4944 val_rmse_stderror: 1.122\n","Still best_val_rmse: 0.4932 (from epoch 2)\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.4168 train_rmse_stderror: 0.07706 train_kl_div: 0.3089\n","val_rmse_target: 0.4928 val_rmse_stderror: 1.116\n","New best_val_rmse: 0.4928\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1625 train_rmse_stderror: 0.06368 train_kl_div: 0.07776\n","val_rmse_target: 0.4876 val_rmse_stderror: 1.121\n","New best_val_rmse: 0.4876\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.2262 train_rmse_stderror: 0.07391 train_kl_div: 0.1256\n","val_rmse_target: 0.4849 val_rmse_stderror: 1.11\n","New best_val_rmse: 0.4849\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.1664 train_rmse_stderror: 0.03755 train_kl_div: 0.06547\n","val_rmse_target: 0.4815 val_rmse_stderror: 1.116\n","New best_val_rmse: 0.4815\n","\n","32 steps took 24.8 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1838 train_rmse_stderror: 0.05317 train_kl_div: 0.07505\n","val_rmse_target: 0.481 val_rmse_stderror: 1.116\n","New best_val_rmse: 0.481\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.1397 train_rmse_stderror: 0.06673 train_kl_div: 0.05521\n","val_rmse_target: 0.4819 val_rmse_stderror: 1.115\n","Still best_val_rmse: 0.481 (from epoch 4)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.2036 train_rmse_stderror: 0.07329 train_kl_div: 0.1041\n","val_rmse_target: 0.4828 val_rmse_stderror: 1.116\n","Still best_val_rmse: 0.481 (from epoch 4)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.09435 train_rmse_stderror: 0.05649 train_kl_div: 0.03198\n","val_rmse_target: 0.4845 val_rmse_stderror: 1.117\n","Still best_val_rmse: 0.481 (from epoch 4)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.1609 train_rmse_stderror: 0.07212 train_kl_div: 0.07179\n","val_rmse_target: 0.484 val_rmse_stderror: 1.116\n","Still best_val_rmse: 0.481 (from epoch 4)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1681 train_rmse_stderror: 0.06265 train_kl_div: 0.06685\n","val_rmse_target: 0.484 val_rmse_stderror: 1.116\n","Still best_val_rmse: 0.481 (from epoch 4)\n","\n","Performance estimates:\n","[0.4973942334383366, 0.4739272001314702, 0.48102082719633504]\n","Mean: 0.48411408692204727\n","{'total_MiB': 16160, 'used_MiB': 1359}\n","\n","Fold 4/5\n","{'total_MiB': 16160, 'used_MiB': 1359}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 49.6 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.6832 train_rmse_stderror: 0.08917 train_kl_div: 0.9521\n","val_rmse_target: 0.6785 val_rmse_stderror: 1.734\n","New best_val_rmse: 0.6785\n","\n","64 steps took 49.0 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.5105 train_rmse_stderror: 0.08067 train_kl_div: 0.4939\n","val_rmse_target: 0.5925 val_rmse_stderror: 1.766\n","New best_val_rmse: 0.5925\n","\n","64 steps took 49.3 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.6274 train_rmse_stderror: 0.07935 train_kl_div: 0.7891\n","val_rmse_target: 0.5213 val_rmse_stderror: 1.774\n","New best_val_rmse: 0.5213\n","\n","32 steps took 24.5 seconds\n","Epoch: 1 batch_num: 36\n","train_rmse_target: 0.6032 train_rmse_stderror: 0.08097 train_kl_div: 0.6897\n","val_rmse_target: 0.5036 val_rmse_stderror: 1.743\n","New best_val_rmse: 0.5036\n","\n","32 steps took 24.5 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.3139 train_rmse_stderror: 0.07147 train_kl_div: 0.2293\n","val_rmse_target: 0.5127 val_rmse_stderror: 1.796\n","Still best_val_rmse: 0.5036 (from epoch 1)\n","\n","32 steps took 24.5 seconds\n","Epoch: 1 batch_num: 100\n","train_rmse_target: 0.4987 train_rmse_stderror: 0.05478 train_kl_div: 0.5474\n","val_rmse_target: 0.5247 val_rmse_stderror: 1.763\n","Still best_val_rmse: 0.5036 (from epoch 1)\n","\n","32 steps took 24.6 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.5729 train_rmse_stderror: 0.04412 train_kl_div: 0.6382\n","val_rmse_target: 0.5726 val_rmse_stderror: 1.726\n","Still best_val_rmse: 0.5036 (from epoch 1)\n","\n","64 steps took 49.4 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.3657 train_rmse_stderror: 0.03386 train_kl_div: 0.2597\n","val_rmse_target: 0.4871 val_rmse_stderror: 1.775\n","New best_val_rmse: 0.4871\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.2336 train_rmse_stderror: 0.06533 train_kl_div: 0.09912\n","val_rmse_target: 0.47 val_rmse_stderror: 1.792\n","New best_val_rmse: 0.47\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.2744 train_rmse_stderror: 0.0654 train_kl_div: 0.1816\n","val_rmse_target: 0.4812 val_rmse_stderror: 1.768\n","Still best_val_rmse: 0.47 (from epoch 2)\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.313 train_rmse_stderror: 0.0322 train_kl_div: 0.1815\n","val_rmse_target: 0.4816 val_rmse_stderror: 1.764\n","Still best_val_rmse: 0.47 (from epoch 2)\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.2872 train_rmse_stderror: 0.07296 train_kl_div: 0.1447\n","val_rmse_target: 0.4902 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.47 (from epoch 2)\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.3997 train_rmse_stderror: 0.06112 train_kl_div: 0.3016\n","val_rmse_target: 0.475 val_rmse_stderror: 1.77\n","Still best_val_rmse: 0.47 (from epoch 2)\n","\n","32 steps took 24.8 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.08151 train_rmse_stderror: 0.04784 train_kl_div: 0.02457\n","val_rmse_target: 0.4693 val_rmse_stderror: 1.746\n","New best_val_rmse: 0.4693\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.2204 train_rmse_stderror: 0.04193 train_kl_div: 0.1081\n","val_rmse_target: 0.4677 val_rmse_stderror: 1.772\n","New best_val_rmse: 0.4677\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 76\n","train_rmse_target: 0.1691 train_rmse_stderror: 0.05267 train_kl_div: 0.06359\n","val_rmse_target: 0.4733 val_rmse_stderror: 1.754\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.1292 train_rmse_stderror: 0.03183 train_kl_div: 0.04107\n","val_rmse_target: 0.4727 val_rmse_stderror: 1.752\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1705 train_rmse_stderror: 0.04064 train_kl_div: 0.06928\n","val_rmse_target: 0.4774 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.08063 train_rmse_stderror: 0.0459 train_kl_div: 0.02237\n","val_rmse_target: 0.4744 val_rmse_stderror: 1.756\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","32 steps took 24.8 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.1543 train_rmse_stderror: 0.03637 train_kl_div: 0.05421\n","val_rmse_target: 0.4708 val_rmse_stderror: 1.755\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","32 steps took 24.5 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.08417 train_rmse_stderror: 0.02982 train_kl_div: 0.0152\n","val_rmse_target: 0.4719 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.0924 train_rmse_stderror: 0.04436 train_kl_div: 0.02449\n","val_rmse_target: 0.4749 val_rmse_stderror: 1.756\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.1372 train_rmse_stderror: 0.04852 train_kl_div: 0.05071\n","val_rmse_target: 0.4729 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","32 steps took 24.5 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.1063 train_rmse_stderror: 0.02608 train_kl_div: 0.02676\n","val_rmse_target: 0.4718 val_rmse_stderror: 1.76\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.1022 train_rmse_stderror: 0.03447 train_kl_div: 0.02695\n","val_rmse_target: 0.4724 val_rmse_stderror: 1.759\n","Still best_val_rmse: 0.4677 (from epoch 3)\n","\n","Performance estimates:\n","[0.4973942334383366, 0.4739272001314702, 0.48102082719633504, 0.467656362969419]\n","Mean: 0.4799996559338902\n","{'total_MiB': 16160, 'used_MiB': 1359}\n","\n","Fold 5/5\n","{'total_MiB': 16160, 'used_MiB': 1359}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 49.7 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.8358 train_rmse_stderror: 0.1072 train_kl_div: 1.243\n","val_rmse_target: 0.6587 val_rmse_stderror: 1.134\n","New best_val_rmse: 0.6587\n","\n","64 steps took 49.0 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.4902 train_rmse_stderror: 0.07766 train_kl_div: 0.5309\n","val_rmse_target: 0.5932 val_rmse_stderror: 1.135\n","New best_val_rmse: 0.5932\n","\n","64 steps took 49.4 seconds\n","Epoch: 1 batch_num: 4\n","train_rmse_target: 0.3861 train_rmse_stderror: 0.06258 train_kl_div: 0.3115\n","val_rmse_target: 0.5508 val_rmse_stderror: 1.12\n","New best_val_rmse: 0.5508\n","\n","64 steps took 49.1 seconds\n","Epoch: 1 batch_num: 68\n","train_rmse_target: 0.7425 train_rmse_stderror: 0.07797 train_kl_div: 1.121\n","val_rmse_target: 0.619 val_rmse_stderror: 1.109\n","Still best_val_rmse: 0.5508 (from epoch 1)\n","\n","64 steps took 49.1 seconds\n","Epoch: 1 batch_num: 132\n","train_rmse_target: 0.4311 train_rmse_stderror: 0.07707 train_kl_div: 0.4191\n","val_rmse_target: 0.5799 val_rmse_stderror: 1.16\n","Still best_val_rmse: 0.5508 (from epoch 1)\n","\n","64 steps took 49.4 seconds\n","Epoch: 2 batch_num: 8\n","train_rmse_target: 0.5394 train_rmse_stderror: 0.05634 train_kl_div: 0.6666\n","val_rmse_target: 0.5368 val_rmse_stderror: 1.148\n","New best_val_rmse: 0.5368\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 40\n","train_rmse_target: 0.214 train_rmse_stderror: 0.05603 train_kl_div: 0.1102\n","val_rmse_target: 0.5062 val_rmse_stderror: 1.145\n","New best_val_rmse: 0.5062\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 72\n","train_rmse_target: 0.313 train_rmse_stderror: 0.07114 train_kl_div: 0.2281\n","val_rmse_target: 0.5467 val_rmse_stderror: 1.129\n","Still best_val_rmse: 0.5062 (from epoch 2)\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 104\n","train_rmse_target: 0.5014 train_rmse_stderror: 0.07903 train_kl_div: 0.458\n","val_rmse_target: 0.5048 val_rmse_stderror: 1.134\n","New best_val_rmse: 0.5048\n","\n","32 steps took 24.5 seconds\n","Epoch: 2 batch_num: 136\n","train_rmse_target: 0.4029 train_rmse_stderror: 0.05868 train_kl_div: 0.2864\n","val_rmse_target: 0.4934 val_rmse_stderror: 1.154\n","New best_val_rmse: 0.4934\n","\n","32 steps took 24.6 seconds\n","Epoch: 2 batch_num: 168\n","train_rmse_target: 0.3372 train_rmse_stderror: 0.02089 train_kl_div: 0.2561\n","val_rmse_target: 0.5227 val_rmse_stderror: 1.139\n","Still best_val_rmse: 0.4934 (from epoch 2)\n","\n","32 steps took 24.9 seconds\n","Epoch: 3 batch_num: 12\n","train_rmse_target: 0.2899 train_rmse_stderror: 0.04364 train_kl_div: 0.1782\n","val_rmse_target: 0.5218 val_rmse_stderror: 1.138\n","Still best_val_rmse: 0.4934 (from epoch 2)\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 44\n","train_rmse_target: 0.3259 train_rmse_stderror: 0.05774 train_kl_div: 0.21\n","val_rmse_target: 0.5541 val_rmse_stderror: 1.138\n","Still best_val_rmse: 0.4934 (from epoch 2)\n","\n","64 steps took 49.1 seconds\n","Epoch: 3 batch_num: 108\n","train_rmse_target: 0.3979 train_rmse_stderror: 0.06573 train_kl_div: 0.3376\n","val_rmse_target: 0.4923 val_rmse_stderror: 1.14\n","New best_val_rmse: 0.4923\n","\n","32 steps took 24.5 seconds\n","Epoch: 3 batch_num: 140\n","train_rmse_target: 0.1789 train_rmse_stderror: 0.06095 train_kl_div: 0.0799\n","val_rmse_target: 0.4972 val_rmse_stderror: 1.139\n","Still best_val_rmse: 0.4923 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 3 batch_num: 172\n","train_rmse_target: 0.2359 train_rmse_stderror: 0.05714 train_kl_div: 0.1387\n","val_rmse_target: 0.4972 val_rmse_stderror: 1.136\n","Still best_val_rmse: 0.4923 (from epoch 3)\n","\n","32 steps took 24.8 seconds\n","Epoch: 4 batch_num: 16\n","train_rmse_target: 0.2248 train_rmse_stderror: 0.06932 train_kl_div: 0.1302\n","val_rmse_target: 0.5201 val_rmse_stderror: 1.139\n","Still best_val_rmse: 0.4923 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 48\n","train_rmse_target: 0.2161 train_rmse_stderror: 0.04633 train_kl_div: 0.1135\n","val_rmse_target: 0.5012 val_rmse_stderror: 1.142\n","Still best_val_rmse: 0.4923 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 80\n","train_rmse_target: 0.2703 train_rmse_stderror: 0.04354 train_kl_div: 0.1537\n","val_rmse_target: 0.5067 val_rmse_stderror: 1.145\n","Still best_val_rmse: 0.4923 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 112\n","train_rmse_target: 0.1722 train_rmse_stderror: 0.06162 train_kl_div: 0.07799\n","val_rmse_target: 0.4986 val_rmse_stderror: 1.141\n","Still best_val_rmse: 0.4923 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 144\n","train_rmse_target: 0.3226 train_rmse_stderror: 0.0529 train_kl_div: 0.2177\n","val_rmse_target: 0.4982 val_rmse_stderror: 1.147\n","Still best_val_rmse: 0.4923 (from epoch 3)\n","\n","32 steps took 24.6 seconds\n","Epoch: 4 batch_num: 176\n","train_rmse_target: 0.09739 train_rmse_stderror: 0.06139 train_kl_div: 0.03843\n","val_rmse_target: 0.4983 val_rmse_stderror: 1.145\n","Still best_val_rmse: 0.4923 (from epoch 3)\n","\n","Performance estimates:\n","[0.4973942334383366, 0.4739272001314702, 0.48102082719633504, 0.467656362969419, 0.49234386491219495]\n","Mean: 0.48246849772955114\n","{'total_MiB': 16160, 'used_MiB': 1359}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m4v-cGx-Mv7S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627581583948,"user_tz":-540,"elapsed":34,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"83d174dc-fd94-46ce-e606-66bc4950714a"},"source":["print(list_val_rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.4973942334383366, 0.4739272001314702, 0.48102082719633504, 0.467656362969419, 0.49234386491219495]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XU4gRXHCBEpC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAb99KSKBEmd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jH0aFzWxBEkG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2CdCMuIKDMP"},"source":["#rep = MemReporter(model)\n","#rep.report()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLl1yDOOKIe7"},"source":["#rep = MemReporter(model.roberta)\n","#rep.report()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qkqnknA_m9D"},"source":["#gpuinfo()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwrqSMdYA6Pu"},"source":["#del model\n","#del optimizer \n","#del train_loader\n","#del val_loader\n","#del scheduler \n","#del list_val_rmse\n","#del train_indices\n","#del val_indices\n","#del tokenizer\n","#torch.cuda.empty_cache()\n","#gpuinfo()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wXcHyUSJXecL"},"source":["# upload models"]},{"cell_type":"code","metadata":{"id":"YIV6UllSIGoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627581697930,"user_tz":-540,"elapsed":114004,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"1f9c498e-e5fb-4c81-bfbe-2f8ac79e2835"},"source":["%cd\n","!mkdir .kaggle\n","!mkdir /content/model\n","!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\n","\n","!cp -r /content/model_1.pth /content/model/model_1.pth\n","!cp -r /content/model_2.pth /content/model/model_2.pth\n","!cp -r /content/model_3.pth /content/model/model_3.pth\n","!cp -r /content/model_4.pth /content/model/model_4.pth\n","!cp -r /content/model_5.pth /content/model/model_5.pth"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"14ddOZH4IMam","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627581953500,"user_tz":-540,"elapsed":255586,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"5af53dca-6655-4378-b022-8a3fe7f2d4d1"},"source":["def dataset_upload():\n","    import json\n","    from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","    id = f'{USERID}/{EX_NO}-02'\n","\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = id\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = f'{EX_NO}'\n","\n","    with open(UPLOAD_DIR / 'dataset-metadata.json', 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","\n","    api = KaggleApi()\n","    api.authenticate()\n","\n","    # データセットがない場合\n","    if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n","        api.dataset_create_new(folder=UPLOAD_DIR,\n","                               convert_to_csv=False,\n","                               dir_mode='skip')\n","    # データセットがある場合\n","    else:\n","        api.dataset_create_version(folder=UPLOAD_DIR,\n","                                   version_notes='update',\n","                                   convert_to_csv=False,\n","                                   delete_old_versions=True,\n","                                   dir_mode='skip')\n","dataset_upload()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Starting upload for file model_4.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:49<00:00, 28.6MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_4.pth (1GB)\n","Starting upload for file model_3.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:50<00:00, 28.3MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_3.pth (1GB)\n","Starting upload for file model_5.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:48<00:00, 29.2MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_5.pth (1GB)\n","Starting upload for file model_2.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:51<00:00, 27.8MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_2.pth (1GB)\n","Starting upload for file model_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:50<00:00, 28.0MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_1.pth (1GB)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ceDI72NumT5-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvRi_JQgwcKI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627582059531,"user_tz":-540,"elapsed":106046,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"1ce7e28d-04cb-4330-fd32-8258d17b4c87"},"source":["# validation再実行_予測結果取得\n","\n","all_predictions = np.zeros(len(train_kf_df)) # 推論結果について、「fold　× 推論df」のzero行列で枠を作る\n","\n","for fold_ in sorted(train_kf_df['kfold'].unique()):\n","    model_path = UPLOAD_DIR/f\"model_{fold_ + 1}.pth\" # 対応するモデルを読む\n","    print(f\"\\nUsing {model_path}\")\n","\n","    val_idx = train_kf_df['kfold'] == fold_\n","    val_df = train_kf_df[val_idx]\n","    val_dataset = LitDataset(val_df, inference_only=True) # TestのDataset(何で、もう一回作るのだろう？)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                          drop_last=False, shuffle=False, num_workers=2) # TestのDataLoader\n","\n","    model = LitModel()\n","    model.load_state_dict(torch.load(model_path))    # 対応するモデルから、重みを読み込む\n","    model.to(DEVICE) # モデルをDEVICEへぶち込む\n","\n","    all_predictions[val_idx] = predict(model, val_loader) # 推論結果行列の対象列に、推論結果を入力(以後、繰り返し)\n","\n","    del model\n","    gc.collect()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Using /content/model/model_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/model/model_2.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/model/model_3.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/model/model_4.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/model/model_5.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3oS-lD5t3FMc"},"source":["train_kf_df['pred'] = all_predictions\n","train_kf_df['diff_sq'] = (train_kf_df['target'] - train_kf_df['pred'])**2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"Ab2NiFJ13FJ7","executionInfo":{"status":"ok","timestamp":1627582059532,"user_tz":-540,"elapsed":30,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"6c6ecd93-e960-4885-c96d-f04df705542f"},"source":["train_kf_df.plot(kind='scatter', x='target', y='diff_sq')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fd5f0338250>"]},"metadata":{"tags":[]},"execution_count":31},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYMAAAEGCAYAAACHGfl5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZwV1Zn3f0/VXXqDBhuUpVlURAeIYGxFgzpRsrggzoxIHEx8E6NO5g3RxIjGSVzQN5lxTaKQ+CHGmZgQDWISFtFEA45CFG20ITQidlygwQVaaGjovkvVef+oW7frVp2qW3epvvd2P9/Px4/0vVV1T1Wdc55znpWEEGAYhmEGNkqpG8AwDMOUHhYGDMMwDAsDhmEYhoUBwzAMAxYGDMMwDIBQqRuQD8OGDRPjx48vdTMYhmEqik2bNu0TQgyXfVeRwmD8+PFobm4udTMYhmEqCiJ63+07VhMxDMMwwQoDIqoioleJaDMRtRLRQskxXyWivUTUkvrv6iDbxDAMwzgJWk0UA3CeEKKLiMIA1hPRM0KIV2zH/U4IMT/gtjAMwzAuBCoMhJHroiv1Zzj1H+e/YBiGKTMCtxkQkUpELQA+BvCcEGKj5LBLiWgLES0nojEu17mWiJqJqHnv3r2BtplhGGagEbgwEEJoQohpABoBnE5EU2yHrAIwXghxMoDnAPzK5TpLhBBNQoim4cOlnlEloaMrhs27DqCjK1bqpjAMw+RNn7mWCiEOENE6AOcD2Gr5vMNy2CMA7umrNhXKipbduPmpLQgrChK6jnsuPRmzp40udbMYpuLo6IqhfX83GodWo6EuWurmDEgCFQZENBxAIiUIqgF8HsDdtmNGCiE+SP05G8CbQbapWHR0xXDzU1vQk9DRAx0AcNNTWzBjwjDuzAyTA7yoKg+CVhONBLCOiLYAeA2GzWA1Ed1JRLNTx1yXcjvdDOA6AF8NuE1FoX1/N8JK5uMLKwra93eXqEUMU3lYF1WHYkn0JHTc9NQWVruWgKC9ibYAOEXy+W2Wf98C4JYg2xEEjUOrkdD1jM8Suo7GodUlahHDVB7mosrcXQO9iyreYfctHIGcJw11Udxz6cmoCisYFA2hKqzgnktP5g7MMDnAi6ryoSJzE5ULs6eNxowJw9jwxTB5Yi6qbrLZDHgs9T0sDAqkoS7KHZdhCoAXVeUBCwOGYUoOL6pKD9sMGIZhGBYGDMMwDAsDhmEYBiwMGIZhGLAwYBiGYcDCgGEYhgELA4ZhGAYsDBiGYRiwMGAYhmHAwoBhmAEMVyrshdNRMAwzIOGiOpnwzoBhmAEHF9VxwsKAYZgBB1cqdMLCgGGYAQcX1XHCwoBhmAEHVyp0wgZkhmEGJFxUJ5NAhQERVQF4EUA09VvLhRC3246JAngMwKkAOgB8SQjxXpDtYhiGAbiojpWg1UQxAOcJIaYCmAbgfCI6w3bM1wHsF0JMAPBjAHcH3CaGYRjGRqDCQBh0pf4Mp/4TtsMuAfCr1L+XA5hJRBRkuxiGYZhMAjcgE5FKRC0APgbwnBBio+2Q0QB2AYAQIgmgE0CD5DrXElEzETXv3bs36GYzDMMMKAIXBkIITQgxDUAjgNOJaEqe11kihGgSQjQNHz68uI1kGIYZ4PSZa6kQ4gCAdQDOt321G8AYACCiEIB6GIZkhmEYpo8IVBgQ0XAiGpL6dzWAzwPYbjtsJYD/k/r3HABrhRB2u0LFwwmxGIYpZ4KOMxgJ4FdEpMIQPMuEEKuJ6E4AzUKIlQB+CeDXRNQG4BMAlwfcpj6HE2IxDFPuUCUuwpuamkRzc3Opm+GLjq4YZty9Fj2J3tD3qrCCDTefx/7NDMP0KUS0SQjRJPuO01EEDCfEYhimEmBhEDCcEIthmEqAhUHAcEIshmEqAU5U1wdwQiyGYcodFgZ9BCfEYhimnGE1EcMwDMPCgGEYhmFhwDAMw4CFAcMwDAMWBoHAeYgYhqk02JuoyHAeIoZhKhHeGRSRjq4Ybn5qC3oSOg7FkuhJ6LjpqS28Q2AYpuxhYVBEOA8RwzCVCguDIsJ5iBiGqVRYGBQRzkPEMEylwgbkIsN5iBiGqURYGAQA5yFiGKbSYDURwzAMw8KAYRiGYWHAMAzDIGBhQERjiGgdEW0jolYiul5yzGeJqJOIWlL/3RZkmxiGYRgnQRuQkwC+K4R4nYgGAdhERM8JIbbZjntJCDEr4LYwDMOUNR1dsZJ5IgYqDIQQHwD4IPXvQ0T0JoDRAOzCgGEYZkBT6rxmfWYzIKLxAE4BsFHy9ZlEtJmIniGiyS7nX0tEzUTUvHfv3gBbyjAM07eUQ16zPhEGRFQH4CkA3xZCHLR9/TqAcUKIqQAeAvBH2TWEEEuEEE1CiKbhw4cH22CGYZg+pBzymgUuDIgoDEMQLBVC/N7+vRDioBCiK/XvNQDCRDQs6HYxDMOUC+WQ1yxobyIC8EsAbwohHnA5ZkTqOBDR6ak2dQTZLoZhmHKiHPKaBe1NNAPAVwD8jYhaUp/9B4CxACCEeBjAHAD/TkRJAN0ALhdCiIDbxTAMU1aUOq9Z0N5E6wFQlmMWAVgUZDsYhmEqgVLmNeMIZIZhGIaFAcMwDMPCgGEYhgELA4ZhGAYsDBiGYRiwMGAYhmHAwoBhGIYBCwOGYRgGLAwYhmEYsDBgGIbxpKMrhs27DvRpOulSEHRuIoZhmIql1AVn+hLeGTAMw0goh4IzfQkLA4ZhGAnlUHCmL2FhwDBMxdEXevxyKDjTl7DNgGGYiqKv9PhmwZmbbL9VqhTTQcPCoELp6IqVrAgGw5QKqx6/B8aq/aantmDGhGEFjwPZmCp1wZm+hIVBBTKQPBwYxoqpxzcFAdCrxy9kovYaU6UsONOXsM2gwhhoHg4MYyUIPT6PKQMWBhXGQPNwYBgrQRSO5zFlwGqiItCX+vuB5uHQ32HbT+4UW4/PY8og0J0BEY0honVEtI2IWonoeskxREQPElEbEW0hok8H2aZis6JlN2bcvRZffmQjZty9Fitbdgf6e0GsjPwyUMLy+4q+7jv9iYa6KKaOGVKUfl/KMVVOkBAiuIsTjQQwUgjxOhENArAJwD8JIbZZjrkQwLcAXAhgOoCfCiGme123qalJNDc3B9Zuv3R0xTDj7rXoSfSuKqrCCjbcfF7gHamvV5RstC4upew7jJyBsEsjok1CiCbZd4HuDIQQHwghXk/9+xCANwHYZ5BLADwmDF4BMCQlRAKhmKvbUuoai7kyygYb2IoP66nLj74cU+VIn9kMiGg8gFMAbLR9NRrALsvf7anPPrCdfy2AawFg7NixebWh2KvbgaJrDMqdbyAzUPoOUzn0iTcREdUBeArAt4UQB/O5hhBiiRCiSQjRNHz48JzPD2J1O1B0jTxxFZ+B0neYysH3ziCbYddUB0nOC8MQBEuFEL+XHLIbwBjL342pz4pKUKvbgRChONDC8vuKgdB3mMohFzXRzwB8GsAWAATgUzAMwj0ABIDz7CcQEQH4JYA3hRAPuFx3JYD5RPQEDANypxDiA5dj86YYq1s3A9NAiFDkiSsYBkLfCZKBYPTtK3IRBnsAXCOE+BsAENEUAHcIIeZ4nDMDwFcA/I2IWlKf/QeAsQAghHgYwBoYnkRtAI4A+FpOd+CTQle37E3jb+Liwcn0FTwmi4tv11IiahVCTM72WV9QiGtpPpMVuwH6gwcn01fwmMyPYrmWbiGiR4jos6n/fgFDZVRR5OM+xm6A2WH30+LBwX3Z4TFZfHJRE30NwL8DMKOIXwTw86K3qAwJypumP6lU2P20OPDuyh/s4VZ8fO8MhBA9QogfCyH+GcDXAfxFCNETXNPKhyDcAPtbKgIenIXDuyv/sGtu8cnFtfQFALNT52wC8DER/VUI8Z2A2lZWFNObJsgCHaWC3U8Lh3dXucEebsUlFzVRvRDiIBFdDSN9xO1EVHE2g0Iolhtgfx30hQ7O/qQ2ywfeXeUOu+YWj1yEQSiVM2gugO8H1J4BQX8e9PkOzlx15f1RcPDuiikluQiDOwH8CcB6IcRrRHQcgLeDaVb/hgd9JrmqzfqzkZVVH0yp8C0MhBBPAnjS8vc7AC41/yaiW4QQ/1nc5vVfeND3kovarD/aW+yw6qN86I87UDeKmbX0MgAsDHKgGIO+0M5aDp09F7VZf7W3MOVHf96ByiimMKAiXovxQaGdtVw6ey5qs0LtLeUg/JjyZyDsQO0UUxgEVzKNcVBoZy23zu5XbVaIvaVchB9T/gzEHSjvDCqUQjtrOXZ2v2qzfOwt5Sb8mPJmIGYdyBqBTER3p/5/WZZDn8zyPVNECu2sxUrpXaocOrnmmOJcNkwuDMSsA352BhcS0fcA3AKPCV8I8aOitYrJSqHuqQMtpXc5xXaU8+qQ6WWgZR3wIwyeBbAfQB0RWUtWEgAhhBgcSMsCpj8MyEI7a77nV0LHtlMusR2VJkSLSRBjLuhxPJCyDvgRBj8QQiwgohVCiEsCb1Ef0J8GZKGdNZ/z/XTsQgZpUAO81LEdlShEi0UQY66SxnE57Uzd8CMMXoZR7jKvQvblhmxA3vjkZkwaORgTjhlU4tZVBtk6diGDNOgBXsqArkpYHQZBEEKw0gRruexMvfAjDCJENA/AZ4joX+xfuhS5L1tkAzKuCVz40HrcN6d8VxblhFfHLmSQVtoAz5VKWB0GQRBCsBIFa6l3ptnwIwy+AeAKAEMAXGz7TgCoKGEgG5AAEE/qWLB8C4bURDB51OCye1HlhlvHLmSQVuIAz4VKWB0GQRBCsC8EaxDqynJONZJVGAgh1gNYT0TNQohf5nJxInoUwCwAHwshpki+/yyAFQDeTX30eyHEnbn8Rq6YA/LGJzcjrmXGycWSOr7x603QIcpa/1guyDp2bURFTMtvkA6ElXO5rw6DIAghGLRgrSR7RLEgIbwDh4noPCHEWpmKCPBWExHROQC6YNQ/cBMGNwohZuXS6KamJtHc3JzLKQ7aPjqECx9aj3jSuUsAuLh2PpgDSOgCMU2gKmz49ecykFa27HYM8P4+CAcKleJN1NEVw4y716In0Ts3+JkPKsFDkYg2CSGaZN/5UROdA2AtDBWRQMql1PJ/V2EghHiRiMbn2N4+YcIxg3DfHGNloRDhSFzL+L4Y6olK6BzFwqrvN9F1gTXXnZ2TYX4grpwHCkGoSIK4ZuueTiiUmVAh23zQH3YSfoTBISK6AcBW9AoBoHi5iM4kos0A9sDYJbQW6bpZMSee1j2duOaxZsSSvbdUqHqiP3SOXJDp+6MhFYdtQtYP5axX7c8MpMWLGytaduOm5Zsz5gLAez7oL44PfoRBXer/JwI4DYaOn2DsFF4t8PdfBzBOCNFFRBcC+COAE2QHEtG1AK4FgLFjxxb4s7001EVxzsSjce+cqUXTP7Z9dAgLUjaJSu4cuTAQ9P39mXwrzdVGDIGfa9BiOQodc1K3C4JoyDsVRX9xfPBjQF4IAET0IoBPCyEOpf6+A8DThfy4EOKg5d9riOhnRDRMCLFPcuwSAEsAw2ZQyO/KKJZ6YkXLbixYvsVhnK7EzmHiZ/AOVE+Z/kC+lebysQ1ZhU5c0zD/3BMwb/rYsugnskm9Jqzi4a+cinMmDnc9r78shHLJWnoMgLjl73jqs7whohEAPhJCCCI6HUbivI5CrlkIhaonzEElM0pXYucAclsxygRqua4CmV7yrTRnYv472+63oyuWVsGYv3X/czuwaF0b7i2DGB/ZpK5DYPIo74w7/WUhlIsweAzAq0T0h9Tf/wTgf7xOIKLHAXwWwDAiagdwO4AwAAghHgYwB8C/E1ESQDeAy0U296YyRjaoACCiUkV2jnx0oVaBOtDsJpVKoZXmTLLtfpdu3OlQwQCGS3c5qFELmdT7g+NDLjWQf0hEzwA4O/XR14QQb2Q551+zfL8IwCK/bSh3ZIMqElKw5ltnuXrUBLlyLvTahehC+4NRrVjvptx3R7lMgm5Bm0B2I+vidW2ubSgXNWohk3qlOz7kVNxGCPE6DKMv48I3PzsBi9a1IaL2Dio3QRDkyrkY1y5EF1ouRrV8J+JivZtK2R3lU2lOZjPwMrJGVAUxl7ieclKj5jupl7vQz0YxK50NaKyDHhC49pzjPA1jQa6ci3XtQrbN5WBUy3ciLtbzq7Tdkd9J0Co4/HoTue0oaiMqNCEqUo1qpVKEvhcsDIqAbNAvfqEN86a7u8AGuXIu5rXz3TaX2qhWyERcrOdXLrujIMh19SzrD7fOmoQpo+ordiVtUmlC3w0WBkUgn0Ef5Mq52NfOd9tcSqNaIRNxsZ5fOeyOyon+YGSV0V+EftYayIwca/3ffAa9uVIqZo3Vvri2DK9ayA11udUqzvX6bhQyERfr+fX1e6gEitEfyo3+IvSzJqorR4qRqE6GXwOQTD8IIK8Ea+XsTeSHoHWlhVy/0KR3felNVOnGx/5EPu/Cb18r9Xv2SlTHwiCF30nHK6MhgAE1oPPN7tiX1y/14PNDfzA+lpJivuNC3kW2dhTjPRd6r4VmLe335GIA8tIP9rftbzaC1pW6Xb91z0HUV4d9DYhy9/0uN+NjJQhPK8UUpOXu4Rf0ooGFAXKb1PqLfrAYBP0sZNfvTiRxzWPNGXEclbyK7gvjYyHqz3J+tsWevAt5F9menezaqkK+33NfLBrYgIzcJrVKNgrmY4j1IuhnYb9+NEQgIsSSOg7FkuhJGGkMinU/paBxaDXieVaG88OKlt2YcfdafPmRjZhx91qsbNktPc462VTKszUnWCvm5J0P+S5u/Dw72bUPxzRs3d3pq23FvlcZvDNA7j7xbi5y5bzFDmrV5+UuWIznYb1+Z3cc31z6BhJaMv19JbrwWVnftg+aZZIIF5DHyv68ZYnh8lF/lqtjQ66Td7bfzTc2xs+za6iL4tZZk/D9P2zNOPeup7fh/Ckj+szd2QsWBily9YG266LLeYsd9BZTppc3ioRsgaoQNF0UlJXSvH5HV6xfqejM92LN0KAQMGPCsJyvJet/73UccSSGK7X6M59x4jaJZ5u8reetb9vn63fziYXw++ymjKpPR2yb+BW4fRHEycLAgpex0WtVUW5GQDt9HRTT0RXDjU9uRsJS0+G7T24u+HnYB0Rc0/DNz04oRpNLguy9RFQ15/ci638Llm9Gb1HCXuKa5qn+DHKyyWecZBMebpN3Zt0EHZquI6nD1+8WI7pa9uwah1ZDE/4rqNkJOmiPhYEP8jEO5TvZFlvV1NEVQ2d3IlC9tJ3WPZ0ZggAAEppA655OnDPx6IKubQ6IpRt3YvG6Nix58R0sfqGtbHZiuby/Yq3GpcZJUmSyAFfNODZn9WexyHWc+BUe9slbdp6dYi+G/Dy7YgjcIL3jWBhkwU+HLNagLraqyXo9TdcRVglVITUjUG7zrgOunTd/wSSZhTw/z52fvdCGWFJPZ8Esh51Yru+vWKtxWf/ThA4I5/N+dMN7+IeRg13b5Weyybdf5DpO8l1kedVc8PO7+WK2qXVPJwDC5FGDHe0s55QcLAyy4Nc4VOigLraqSXa9aAhYfMUpmDyqHuvb9mHG3WtdJ65CBNPkUYMRUpChCw8pyFoxyi/lmAsm3/dXjMnB2v9UIiQ0HbfNmoxBVSEssBV3L7SQTCH9Itdxku8iS3ZeWCUoZKjhgkqauKJld4Z6NKQAD8yd5ng+5Rr7wsIgC347pCytb0dXzPdLL/YE56aPrq+OAIDnxJXvxGZdMT4wdxoWLN8MlRRoQse9c6YWbQD0ZayH31VwIe+vGJPD7GmjcagniTtWtSKkEO5c3Yp750zFL65swjd+8zqO5GG0tFOMBUsuwi/fRZbbeUGuyA3PrS0Z6tGkbthuSr1j9QsLAxv2wZ9Lh2yoi/r2WrBT7AnO63rZJq58JjbZivGv35uZ8+DzM/n2hbETyG0VXOpgxI6uGG5fuRVJHekJ6YZlLXj2+nOgF2C0tNK6pxMKZaqe8hEsuQi/fHdObucFNSm37++GqjjVcipVjuszCwMLboPfb4csZOVU7Aku2/W8Jq58/Ldl973h5vMwdcwQ3212e/4yARG07jXXd9lXAkrWzvb93dj1yRHYi4gldWBPZ09R2mW4Cm92uKr2hcDLd+fUl+qYxqHV0HRnnjdNyJ9POcYksTBIkW3w++lYhap6cpng/HQmr9WR1wSR68RWDBWX2/M/1JPEXU9vk67Ogxzs+dxTXwgo67WtwjOW1FzOEgW3y3w3dkEQDVVO9H3QNNRFce+ck/Fdm81Aph4t15ikQIUBET0KYBaAj4UQUyTfE4CfArgQwBEAX03VWe5z8nF7sw+uYqgK/ExwuXQmt+tlmyBy2Q25ua7WRlRPbyUrsuevEKVVH0HFb7gJ1XzfZVACyv7Ob71oEu56epu3+6RKmDyqvuB2yd5NTVjFw185FedMHJ7XNfsj5pjx8iYq55ikoHcG/wNgEYDHXL6/AMAJqf+mA/h56v99Ti6D39wyW42js6eNDkRVIEsxUKzOlG2CyPa9l+vq3FMbMWvRet+rH9nztxo9TXLZcWTbPS195X0sXL0NEZWQ1IVj19EXah+/tQ7s73zhqlZEQpm5aqIh8z2o0IUR9V2MvlcbUR3vRocomndYf6KhLuoZS1OOnnAmgQoDIcSLRDTe45BLADwmjKIKrxDRECIaKYT4IIj2eA08v4O/oyuG7y5rSelnjcnqhmUt6cm4mKoC2Q5gXENtWXQmL9fVUfXVmLVofU4CK8M9UiEcjsnVHn53Wtl2T0tfeR/f/6ORJyaeSnW0YPkWDKmJpFd0Qat9/O7wpBOIqiBuC+yLJXXUhBUkdR23Xzy5aDEqCV3H3KZGLGtu71N7SBD41dUHpdMvtaOBF6W2GYwGsMvyd3vqM4cwIKJrAVwLAGPHjs35h/wMPD+Dv3XPQamhrnXPwfSWuRiqArcdwOr5Z5VFZ/JyXT0c1/ISWObzX7f9Y9y+sjUjh4txfX9J3LLtnjq6Yli4qtVxXiyp4xu/3gQdvbuEfN6l20Ri/Rzwdu+1Ig8qE7j94km4a/W2DOF5JFUIyE8CNK922tu2rLkdq+efhcNxrSgTpHXXkc8186ke51f4FjOvlp1SORr4odTCwDdCiCUAlgBGpbNczs1FtZJ98Lv9dObnha4s3LaTh+NaWXQmtxVObUTFns6evNNfNNRFce5JR+MHKzKzO0ZCCtZ86yxMOGaQ5/kdXTGs2/4xQoq7C2T7/u7Uytq5+ziSMD7LR/XW0RVLpcl4OyO4afa00Y6J6JufneBbYDbURXHrRZOwcFUrwqoCTfQKq/Mnj5AKz3xcgc0Jz6vv5eId5ob52wDQk9ARVQmkkG9Dqh+vs2e3fpjxvGQ2Ftk7DiqvlpVyjUIutTDYDWCM5e/G1GdFpZh6usmj6hFWKaOzhFXCqPrqtLE031gDE8MoG3dMVrGkhtqI2uedSSbYGuqimNvUiMde3pk+7rRxQ9N2Aln6C3NVnm8cQTZBYE4SKpFjV2F3nbUnDLOTa/8wV5NmeoxY0tA93fTUFkwaOdixGFm0rg32BYSXjequp7chEjJUQ7dfPCnDtiETnvm4ApsTXpCqDOtvm8Q0AWjCd2BjNq+znqSWHp/mGFq4qhVEzjgA+zsOMq+Wlb50e/VLqYXBSgDziegJGIbjziDsBcXs3A11Udx/2VQssGwjv3Rar7E0rmnQhdGB8jHwWlc9ujDc01QixDQBRSHMWrQ+Q4WRK9km446uWIY3hJtg6+iKYVlze8a5L7V1AIDDhjB5VH1O23Qg99WTbJIBgNqoCi1lHJa5zqpEiGsahMhMn5FL/+h1vXR69YQVBS27DkhUagquPec4LH6hLauNyn5fd63ehvMnj5Dej19XYGHziRe6SE+MDXVRzD21EY+90ivo5zY1FmXy8soblG/eIVUhLFzVirhlzNlRCei2vZ+ehLGTzST4vFrlStCupY8D+CyAYUTUDuB2AGEAEEI8DGANDLfSNhiupV8Loh3F1tPZU0/YjaV2/K4yZauesEowx605IXgJF5n3kd+c7vbcKioBikIOwTZp5GC07DoAVbLSsmLaEPL1gspl9SSbJGojKhZePBnnnnS0QxUwrqE2rQOvjah4ZuuHWGRT7xTy2yYJXce0MUOki5F508di3vSxngLP7642F+FZG1GN1biFmCbSE2NHVwzLNmUK+mXN7bh+5sSCBYJsYWaSb96hhCZc1X691xaIqpRx32HFCMqz7jiDzqtVzgTtTfSvWb4XAL4ZZBtMiq1aMSeqzbsOOPTTdvyuMmUD375lBdyFi5cHSLYdiyy3iiYATXOuIC98aD0iqlMV43XfQbvUuRlZ7YLA8YxObcSyTcYzAgjXnnMc5k0f61ul5fbbgCHIb501CROOGZQ1yC+Xa7v1J7/C83BcQ1VYydhtVIWV9PsM8l1ZF2aA02aQ7fqyhd2tswxDuhc3ffEkPPD8DqNTp0jowDWPNWcYiBvqooHm1SpnSq0m6lOC0NNt3d2JLpsbZEgBVEXJKNru53e9Vk1WZJOBbOVt6vP97FjccqvYMVZWIu2OCRgrTU0IT/fDoF3qsu3+TPWXvQykqQox/178QhvmTR+bVaVlFxTW3+5JatB1gYiq4K7V2zAoGvIsleoVpJSLy7Ps2jKPHbdnnm86klyQ7cpy9SaSPcu3PjiYodYiADVRFYmUjeWK6eMwor7KVxbXcjXwBs2AEgbFpqMrhruedq5IFl4yBedPHuE6ON06mDnwb1y+BXGJ/rkmYgQTySYDL1WFG9YBXhtRkdCc56oEhFRDsMWSGhSFMlaUtdFMVcz1MydK79FtUgO8ayr4xT7JWK9nTuwKkSOlgp2woqB1T6enSstNUJjRp9c81oyEQHqlbU9rYiJTy337cxMxb7rhOm0+xxkThmHJV06Fm8CQtUcAnh47xUxH4peg0jDI1FqRkIKfX3Fq+nmZ/eP+y6bipqf+ljWLazkaeIOGhYGFXN1B3fTUU1IGUy/1hFcN1kkjB+PCB1/KCCqKhggPf/nTaWOsncah1Z46UxOZh0/aC0fJ9JIyJ6cLpoxIr+JmLVqfcT1Nz1TFeA0i+4orW00Fv8ierekC6WZYdsNYEZOrmgTwjg+or44goqppb3gMtaYAACAASURBVCLruXa7hUwtd/9zO/CT53dAUYz31J1Igijzndl3KM6Sl1sAiMwVcMpjxwysmzFhGDbcfF7B6Ui8yCeuws8Y9OOmG1EV1FeHHcLbLIFppVyCvkxKlcSOhUGKfFYtbnpqPyqcG5cbhliZu+SEYwbhvsumOlZmXq5tDXVRzD/3BNz/3A7PNiuU6eEjmywjKmHe6WPx+Gu7HGUli1W2r/ndjvSquJC0GtkM0147ppACzJs+Fsua29OC8NZZkzB51OC80397qVisg9xLLWfaahKaKVB6/21/Rm7eNUaVM+fiQBZY50Y+q2PzHrfu7sxIMOgnrsLPGJS970Xr3obd28f6zO3HhxRjcRVkoZt8KWUSOxYGcJ9QJo0c7KnP9Ludlg3YeFLHhQ++hPsum+q6Q8h1ZXbBlBF4aG2bI+DLitXDx61tYVXBb1/bhbikrGQxVoy3/fFvGfrd9O9aJge/qyO3HPutezpRXx2R5tUxISJcP3MiTjxmcDpAydTx55v+280t0+7JdetFk6Qpj7Nhn0ClCxLdsOu4UUhgnReyOA+/cRV+vc3a93cjMyOT0afd3HQ3S9x6kzpwzdnH4vwpI8vKJlDqJHYDXhiYEat2N0nTayaqZlfrZJsc3QzD8SyBNrmszMyBSKkBVxVWIASg6bqn/7ysbXFNRzSkZBiJFaJ04I29XV7urPb2N7/bIRUE5u92diew9JX3pWmrZekFZDn2uxMarv5VM6Jm0rymRjzx6i5HLp+EJvDn1g9x19PbENdEWs1m1mKQqVH8GKrt+uvfvdae/n1zkN/19DbcdvEk3LGyVeox5ob9/XnZYqweO/ZASaC43lzZ1HERVcGXzxiLX65/NyOK2nNRImnf0o3vp1NumHi56bqpTx/d8B6uPvu4dH/1MuLL7jUINU6pk9gNaGFgTqAhxekm2es1k11CZ5u0vQzDxXjZsoGo6wJrrjsb2z446LlzMdtmzcOu68IRQHUkrqXc8KY6YhPcXDXtQnRFy25898nN0nswAvh0/N+lm9LeWV51DW74/ETc96e3HBM8ACR1M/rUkGbLmtuxcPZk3PKHrY5jf7Biq2NnARgDc+qYIenJwjRyA3A1VJvn2Qe0EHC0M6womDKqHq/cMhOtezrx8t878OiG9xBS3ZP0AcCtF00CkGl0d1uQWGNh9nR245rHmjMEZzF15dkcGLoTSTy64T1pFHVHVwy7PjmCnqR71DgAtH10yBHoCAA3fG5ihrC24qY+jajGuFvfts9X3WKTINU4pU5iN2CFgWvEasqrxu41k8+kbV1BzJ42Gh929uBHz2zPOKYYL1s2EKMhw2XPz85l0sjBxoyVQhMAUq6RVpVTLNm7kwHgy1VzwfLe429+aguSLqtgBQIJHQ43XcCIwF64ehviyd7t84/WbHcc50ZYUTB6aLUjmAgANB3QbOoLa2SqdfDLjLlWQ7VbumeZ2i6u6en3cc7Eo3HOxKNx9dnHuSbpAwzPrY7DcXzmv/4iTZ8umwjNzyYcMwj3zsm0Q9160aS0URxAQatdt91vbVRFUtOhC8NeEUvtNs0oavtkDBi7WgCOhUvLrgPS3z6qNuLZtnnTx2LRuraMBY6ZR2uB7bftdYvzTS6YjbaPDqFl1wFMGzMkbTcMyovLLwNWGEg9gVJuktPGDHF4zeQ6aTuKkcyaZAS92Lj1okkFv+xsKwqvncuKlt1YsHwL7Lt7TQAkdIQVZHyngPCLl97B//z1PV+umrGkjgf+vANfnDLCdeV40ZQRePHtfUjEkpIrAAlNN1aU8q+z0pPUMHlUPR6YOw3fXdbiuFc7UZWwp7Mbezq7HcJOZsy12gPimo7PnXQ0nt/+seGOq+kgIRwRv/PPnSCdvGV5hkySmsBPntuRaokzfXo2rAsDq4E3m8eSG3b1iiMY7KJJmDK6Hp3dcXxz6RsWg3ivXcfuUQUAmqbjmevPcThXTHNJkuf2uXUiv3eOc5Jds/VD6c7SrFtst/PkklzQC7vN7Mozx+LOSz4FoLQxDgNWGLgZ3kw3yUIktLwYyTaEbd4jtVEVU0bXF3wvsvZaV31u7TbbKYtpAJyraMAwPj78v+/k1L6lr+7E8tfbHS59IZXwxNXTcezwOnzmv/7iOM8MZjMzTnoRVgHAqRcHAJHa9cyYMAw//tIp+M6yFk89vQ4jMlVVFE9hZ0xoBx3v+umtHyKiAteeMwEXTBlhLCxsbsLzpo91TQBovkuhG0KkKqxA0wUSSd0hSu3p0/3oszu747hzdaurkLPXdZBhj5Ew1SsyO0tHV0y6WAFI6lEVVtX0zsh6PxOOGYQrzxybkRzxyjPHSj3yZOoca9uM+1wrvTdNGLuGQpILutH20SGHzeyxl3fiyjPGZ+wQSmHUHrDCIFsQVDY/bC/kHjqERNIpfIqlD3Rb9Xmt9PwEqhkTkY5ElhCGmrAKHQJnHT8Mz2//2PF9LGkYMaMhZLj0NR3bgBUtu2F1rAkpRuDelFH16Wc/qCqUTiznqHMQUvDbr5+Ol9/5BA+ufdsx0VeHQ1i6cSd+lvI2MX+jOhySpO0w/NANbVX2dBuAkD7DuNYbzSzrZ155ouy5r7Z/eAjfWdbi8ZaM+7V68yQ0o8DNFWeMSx/lN/gum/upLEbCVK/89XszHWmu3cba5FGDPYvIyyb0Oy/5FK48Y7xDxeJHnbPh5vPSbdu860AqkDLzqaoE3DZrMlokaWb8Jhf0wk3V1bLrQNasvEEzYIUBEFwQlNuu4/aLJzsm6WKuAMxrfWnJy3kXTZHx47nTHFGbViKqgoe/cipG1VdhfdteqTAAgKqQisVXnIL66kiG59HNT2VOLKqiZGTlBGzCbk8n7lrd+xznNjXiy4++mh68CiFDuMQ1DYtTOuPejKoKFl/x6fTq14yc7uxO4JtLX8chm8qqJqwirmkOdcrkUfWuz9BUIdj7GQDMuHut5zsyV4crWnY79Np2RtVXS21g3//jVoCAK6aPyzn4zsv91C1GwlSvyPq0EUXdBEBkBE66FZEH3PXzE44ZlDFx5lMrQtb3I6qCG78wEXc9vc01Dbqf5IJW7Du1XFVdfcmAFgZA76Ardm1h2Upo9rTROH+KM01FMcnmnuaVU8e+Sjbbfebxw6B75P+PazoWr3sbm9s7PZP2JXTdEUEta29IJazb/rEjyZz5rqaOGZJO92HNGmslpACRkJG+ev65E7DkxXcyVoHWCFXz2gCkdSSiIUPYmZkr7e/vnktPxoLlzhTWcU1DZ3ccHV2xjK2/LLmhfbKy5lKS6bXTbUslDDwc75ZmkV24alv6WdnTVgOZQi6sKg6BL9OJNw6t9lzRm7gFoMl2QXbXTll8gFskdy5BaCYNdVHc8LmJuPfPbyGsKtCFSCe885sGPRtunkd+VV19zYAXBibF9vF1MwQFrQ/0MiZ75dSxttOaXwgwno05UExVin3i2/jufkdbTO8dN+8Qt/Yejmm4fWUrfrBiq2N3ZvXCmDpmiGvW2KQOhIXhHnxUbQTdicyVfncimREVbJ2wzDoSphrJiP4enj7Xfg/mM/ztxp3pNNjdiSR0AcNwapsAN77T4fCask5WueRSIoXS58lyS4UUYN32jzG+ocZhxAaAe+acjDOPbwCAdF6lbO6nDXVR1xW9+WysbtsyV2H7LsgeXe/XzVI2br2C0EysRtyEpmFu02hMGVXv6Eu1ERULvjARxw0flFMaa6/FpZuqq9SwMEjhN41ALhN5KQxBXrYQr52PbAXu8IhKeYe8sXM/7ljlbdCtDiu4+fyTcNaEYb6juK21fGVJ3mReGNfPnIi4y4TZnVrh3bnarHLVexwR4dmtRsCZbMIKK8Dlp4/BFycdg6ZjGzzv1byPb808AfOmj0XrnoOpSVV3eB49u/VDh3sxANw6a5Jjh+qFaVy3TnK3XzzZUA1ZOBLXcceqVsSTOlTKsGMjrBLGHFWTPn/yqHrMP/cE17oOdldp2YrePM7rHlSirIssqVPELKdThGzcxpIaLpgywlWdIzPiLmvejZGDqx1COq7p+M9ntjvKmGYj2+LSruoqB1gYpHCbRGWGvr5w/SokylG22ve77bb+vl143PX0Nmy4+TxJdSgn3QkdP3x6G+Y2jcXXZoz3vAezvTIfe1WhVE1juHphfP2s8fi5h4eTSgpUxVgBmkRUJR27ICOhA7946V384qV3M1z/stFQF0V9ddhhnDRdKe9Y1eo4J6oSxgytAeDPqG/PFGty/pQRaD9wBI+89C5CCtCdMGZ+WewGYERgy+Ip7HUd7N/HNR3zz52AedPHSvNlZbuHw3ENW/d0Zq2nbO3HG9/pwMKVzhrQMu8re0VAIHM8uRlxF//v3x2f6bpAQmSWMfWjOi51AFk+sDCw4MfQ990nN0MhuK4UihGqXkiUo/X3rYMt187ptbKZOmYI5jaNxrJm73LVCd1wK1366s6sE6ppXEvYdNGHYxpuXr7F1a9nfds+XH32cfjFS+9IXWEBQ5dtJG7rJabpiKoK4p53YGB3/QO837Pbs3757x1SQ3BME+kiKzMmDMtq1E8kdRw9OPM3MydzoCeLesnkqTfacfVZxzkEv+kJZdguDqbrO5vf3//cDixa97YjIt3t/u3YS3e60VAXzdhNWVOGmJNyOtPvQ+sBCEdFQPuC7obPT5T+VlSljKDI6rACiMxymX5Vx7m6p5cqU6kVe86nAY9poGyoi0qToCU0IzXwoVgSPQmjMEZHVwyAMSBn3L0WX35kI2bcvRYrW7wnSxltHx3CguXGwJT9hhdev292zqqwgkHREKrCimfndNt+myvJe+ZMw39ccBLCKqEmoqIqrODf//E41ITlu4bHXt6Jto8OebZ91qL16UjoaKi3a3o5eP5ozZvY0LYPC2dPgd10EFUJVWEF986ZinvnGPdu2i8UIaRRvhGXEWFdTVqf82f+6y946C9vo+2jQ9i860DaWDy3qTHj/NlTR+LRDe+53odZZAUwVEZemLYI8x1bd3GHYkkkNAEPe38Gj7z0Llr3dKaFiElYUbB0407MuHstvvHrTdL6zmZEur1v2vtaJKQgqsoN5tno6IrhNkkQnqlqMjkc1xBVnfdgjQMxx9MDz+1wvJ9Lpo50COp4UnfUTXZbQJkpS6zPYva00dhw83n4zdXTseHm8zxTXBQ6bxQD3hm44JYEzY7fPPd+f3PBk07vEZmniSywJ9vv5xLdaA7oBcs3Q9eNGrL27fe1/3g8Lj21MWMn9d9/fc/1muvb9kn1pDIdcyzpjH6WEdcEvvvkZhAE7A4uOoAfz5mKM49vQENdFKPqq3D5IxuN8yzXNVImCJw9oQEv7Ngr/Z1pkvoI1lXy/c/tSBeQufWiSY4cOr9/fTeiIRVeIt18z1NG1acrgMlI6iLt+nrTU1uw5CtNWVVLbkRCCg52JxGzGZ9ND7Fc+r/ZD/YfjiOe1PGbq05HOKT21sHQvA3TMlr3HITMmaonoWWoK912Y7I4kLCi4Irp43Dt2cdhfds+vLfvMB5/bRcUxTCqRFUCUu7J9rKvsowBXjt5qz0u33HbV7AwkGC+IPtAiIYU1yyg7fudrn3myqS+OpzhmSPL7gkYwkTmRijzNLF3PGlee4uhzvpbZnsBbxc5AWOhbqpu7Ntvs6Nbr3HPpSe7pnwYVifPISN7dsZ9uzYt8zgX18tEKiusnirJ+cSruxy5kSIq4ZKpo7F80048v10uCKyuf176cLOAzMJVrYiEMlepSR1I5lAzWnNZ2tuNwAbCV7yIjFhCM4SpZUcmAFw1Yzx+88rOjCI9bm3eursTX1ryMsKKgiMJLcPt1FQPZsv06r5AcXkOCuGih9anbRfugW3OOBDzOa9v24f/fOZNxzgXRHjgspNxy++3ZqTQkGUMaPvoUHoB5zWZ5zJu+zJTqZXAhQERnQ/gpwBUAI8IIf7L9v1XAdwLwNwbLRJCPBJ0u7yQvaCasIqHv3IqDhyJSzv1s1s/dKzkuhNJXPNYMyJqZv6XuKZj5klH4y+p/DVugTKAsXIzf8NrFSF10UwZ6rbu7sTC1dsQUQmxpA4hRIbbpGz7mk5VIZlovTqrqb/9/I9fzBjGBOCkEYOlA79xaDXiSedEGQ0pEEIYag/Ht/4w/eatft1W4prA8k27IJunoyrh/rlTMWtq7/Pxow+X+etbsU/oKgHh1HsGUq68F03Cnau3OdQz9tfRk9Axqr7aUYPZb1psIsr4jVhSR01YwaMb3nXstKIhwlUzjsWjG95FSFGQ0HTc8DkjSMvaJ62Y9ha3XWk2+9jkUfXS9NvxVFbh+5/bgYfW7sB9l01z/Q0v7zrZzieqKhhcHZYGjlp3M0tfeR93rHKmIJft5HMZt6UyNAcqDIhIBbAYwOcBtAN4jYhWCiHsfom/E0LMD7ItuSB7QTpE2n3O3uHcaiEDZqZGZ/6XNVs/TH8PyANlAODGz09MD45sRt1bL5rkcC28fUVrb0pnyyLPqmaQbUm9VsBmxk0r9hwyP718Gm58cjM0XUATQDSs4IIHX0oLIqtHyvq2fYDN9RMwPlr69emY98tXHV4/KgE1EeM6SU2XqhL8EFEJIUWR5ryPaQKdPZkrY3MF+p3ftbj+ZiKVpVNGdZjSXj4mmgD+36xJONSTzIiAv2rGePzqr++no4FlmEFn1omwszuBf/t1c9q11utcVVUyvKwApOsFGBXBlPSCxZyoG4fWGIuLkIL7/vyWoV7xwEy1YN9F+lGRNNRFccfFk3HHqlYQnKnAASP1hzVhn1scSDbvOhMzONJrN7P0lfcdY816vnV8ZBu3pcxUaiXoncHpANqEEO8AABE9AeASAN5O6iUmmyeAvcPJXnZ1WAGBkNSz1yUGAAjgKomL5APP78Clpzaioc4oqWjP+d6T1NIdb8roetRF1QxXwqTbrJTCzee7NqIiJlmtA0BS0/Hs1g/TeW/cVnemh4eW1DPsAaYguv+5HXjwLzsg4EyKFw0ZxdvDIdXw+rF6dKiEm794Ik47tgGNQ6vx7NYPXQdmdsj1PgFg4cpWTB9/VIatY8aEYQipCjQX96XTxh2FDe90SL9zm9fvWNmaDgoz+5FhcM4i5YjQ2Z3IiHLu6Ip5RoybCMCz2lp1OITFV3w6reY0r71wVatRDMiUk1kk8fiGGunnbgZka39c0bIbdz29LVVsSUAlIf05e8I+O/Yx67bDi1p24m47DfMZyIio5JjMZWPJKjBKmanUStDeRKMB7LL83Z76zM6lRLSFiJYT0RjZhYjoWiJqJqLmvXvlut1i4tcToKMrhs7uOI7Y8ivHk7rh0uiTmGbkbKmLZnrjqAph1eY9eHHHx9h/OA7dNnh1XWD/YcNBsnFoddbJ346pSrJievaYK76IzRNEE0bem7ufeRNtHx1yeGuYHiYyDw87Cd0pCCKqgl9c2YQZE4ahszvhqAeQ0AT+35rtuD3lZXL+lBFQvRenrtz4hYmeK9u4ZlS8s3p4tO/vTmdCldG80xmNbWJ/fyay6OCIqmDmSe51rwFjZ3nNr17L8Cxq39+N2y6ejFCW0T3n1EbcdvEkVIUV1EadXmDGCnlw2rsOAH7wx62O1blKhjpzUDTkyFkUUoAvP/pqum1Wj5vaiOoITLPWkrDuHLpiGuKpOiPu9yWkXj0y7B5P0RDhu5+fiL9+L3OsW70LTdr3dyMs6ddhlbDmurMdxZ+sY8n0cJMVmbL/Tl9TDgbkVQAeF0LEiOjfAPwKwHn2g4QQSwAsAYCmpqZ8VcgA/Pv0ZosgtmaJtI9lRSHcNmuyI2e8my43oioAhGMyPxzT0tG+9gRsgDExX/jQetw3J7NgPYHQnS3VaIo7Vramfb6lVdOEQHWI0G3Tr/78f9/BI+vfgSpxSzTzBtm9VPwQ13Rsem8/rv5VMwAh1ecDQEt7J8740fO4+uzjclYTqQTceckUTBldbySe09wNpfGkjhuXGzWxJxwzCImk5pkvKKIqmPkPw7Hmbx85vsvlacQ1HX/e5ryGHdNwfcOyFihkqL00oWPh7CkYc1QNnt6yB7+TVAhb0bIbv39jN269aBLGHFWDtW9+hKUb3zdyOqUCuwDgxR17AQjUhFU8k1JvWtEE8LuvG55DW3d3YuGqbWkBntSBpK6nPb5CipouyjOuodZRcCik9Eafy3bcVSEV//kvn8K3f9eSMVbCKmHXJ9249tebUoFxGuafe0JG4Jwd2YrcWtXOy+1aZuC/Y/ZkRyyKfSwJIjw9/6yyiz4GghcGuwFYV/qN6DUUAwCEENb99CMA7gmyQcUqW5ct5L4qZHgeWNNg7z8cx39veA/Lmnc6PGXimo7vLNuMfz5lNFZu3pORmsHEbdEfT/moWwvWr9q8G3esetNxrEygJDSRrm8sG4BJ3V2dkNCcOXESuo61b36ExS+0pa0g0ZAzXbAbKoCfrm3zdWxCBx55yT362I2QSjh/yggAcKjeZMSTOi588CVcftoYPPHaLs9je5Ia7rrkU7jhcyfi3j+9hT/5mNBlnHNCA5570/8u2Hi8vbWcb1uxFT+9/BT8/nWnIACAwyn/WtOP35RvekLDnf80BQLA9B89n56s3XZfIYUQDqloHFqNLy15WVrZzVwEmW27YVkLnrjmDMeuMKkjvTNwM66eeXwDHpg7FQuWb0mVSxW47eLeJHOZgXFtuHeO+xi3Lvisi7u4pmPBF07Etf94vPScdAqVVLrwG794IsYMrcGLOz5OJ2OUjaWoqri6DJeaoIXBawBOIKJjYQiBywHMsx5ARCOFEB+k/pwNwDmDFYli+vRmC7k3dYKyPD+KouCiSUfjuTc/zhg4CU1gWXM7/uOCk3BUbQS3/nGrI+jFDaGLtK61oS6Ki6eOxg/XbHfsRNy1SMZIbxwqLyCuKuSpggophOqwEZU9rbHeMZnLkqi5ketQiYRUaAnN496cKCC07jmIyaMGu6pu7MQ14UiJIcO83tDaCF7Y4Uzn7TSVy8lFEMjQhBExn81F177J0YRhKzF2qu7HmSR1ga27O+VfupDUga17DiKqUoaKzDSIA962O9MmZSZ7OxzXpOMxZlkoeY1x2eLuR89sR9veQ7hnzjTH8fb6Ibev3Jp+VmGVcP9lU6XR5OWckiJQm4EQIglgPoA/wZjklwkhWonoTiKanTrsOiJqJaLNAK4D8NWg2mNO4Fb8RkLa8ar5Gg0RvvnZCenP7BGisaSOP2370FXvfN+f38K0MUOg5+BQGbPkmQGMgfSvp2WaX9x0rSohnZGxoS6Kmf/g1FNHwwouPWUUwi7LQ4WAxVecgt9cdTpekWQw9TtRK4BnGmwZsaQmza9vEpXceHdSx9f++1X8duNO6feFoAngtxt3on1/NyKqUxevKIRoSIGqEL49cwKe/845+MxxRxW1DSZ+d2MySDI9uD2pu57ehkRSk+6U3R7vex2HQbb3Zs3CCmTa7n5z1emIJ3W0fXQorYtfuGobZi1aj617OrPWlfDCLdZlWfNu18h506njztWtmbFHmsCNT24GgJyi/ktN4DYDIcQaAGtsn91m+fctAG4Juh1AcZNHSbMqXjQJHYfjWLyuDUtefAeLX2jDPZeejHENtVLVi9v6MJzaSt47ZypuWNbSu01XCASBSEh1+LFHQwpadh3A0NpIWve5bFOmesBtXiACNrTtS2ehfFai6z4c07D6bx+42jyiIRX11RG84WE89YMOdyOrjLACwMMWAxiBcyHJzkYTwI/TNYWLy6J1bbhgygjpBKXpIq12e3BtGxava/MdYBdSCEIXjt1TSIGnTSpXNF1AIee1dBiqDrstKKwo2LqnUxpD8ZMvnYJv/+4NR/97/NVduG3WJF8Fnx78yw78xRIUaO6u0kkUV29LxWa0OmIH7NmHzehuR6yLy+7VqwqZIUQU2PezcU3gtxt34lszTygLTyE/lIMBuc/I5jKaS7Kojq4YxjXUYvX8s9IdCzAS21ljC256agtWzz8rpwhRTYh0ojljgj4Is0IUYOSdv/pXr2UYVmNJPaMGgEwA2Y11Jknd0OGqimLYFFza5ZWawBxwr74rd6kMiiumj8NTr+/2NgB7qKgKFQReKp/Dcc0WDJZ0uJbqwv+uCXB3FY6GVHzjH4/HonVtUAhZYwyyoQlIYz8ASJ0CepIafvj0m1KV00kjBuH6mRNx/3M7Mq+T1PHJ4bhnedkVLbvxnSecJT/trQorCqaMrsdfvzczVVeiLR0fceusSVi6cScWr2sDkeGxZKYOsWY/XfCFE6Xpxb2qkBnGZPmzXrSuLW3ALmchYDKghAHg7tObi2FZdqxZaEUWXGKfFOKaod+2r+Jqwgp0wBHTYPed3n8kIfVXt9YAWD3/LIfu30tjYHp95EpIAUKqkk73fe+f3vJ97vcvPAn3/Omtglazj7+2E7Jgvb7CreWxpOEiae1vf3i9Hf/z8vuBtEMTAhdMGYFjh9Vg3Vt78cc3dvv2sHITaF4xCCZGlLjRd2SHmzaAedPHYlGq9KiVB9e+jQumjJCmszZqLW/2JbCtNjqzroSpz79zlXO3YHpgWe0J1/7j8WjbeygjG6+ZisRtodhQF8Vtsybj1j9udbQzopYmrUS+DDhhADhdRv0Ylq1bTLdjZa6UZie1r/J3fdKdsT2+ddakjALwbnR0xbDgyc2e1oSwomDN1g8zBqfMi6gYJHXgpi9OxIwJwzDj7rWuLpf2CefKM8fiXz7diP+UrMRyIaQoOOO4ozJUCOXCa+99kjZwNg6txuNZvJDyRVWA2VNH4YIHXyqamsgv2WwSpg2goS6K+edOcOwOEprABT99EdfNnIh508cC6M3f5aaCsRNWjeSA1nxb5hj60pKXPXe01uyn7fu7cfP5/4Brzz4+owqZbPFnNR7f9fQ2VEeUtHdW+t7K2FgsY0AKAzvZkkVZO0NM09NJvazHLt24Ez97oc2R8MusYAXAkVfdjwCwr0ja93c7qnbZ9r+EaQAAEXRJREFUORJPGnrojCLzBD2gieK+P7+Fk0YO8vSuMn85rBg6/BOPGYzWPZ0FC6jDcQ1ry1AQAMAtf+iNij55VH3egXHZ0HQ4sqTmQlDiI6wSbvj8xPRkO2/6WDy0docjbiShG26gP3l+BxTFyN9ljg8/Efy6LnDn6m2OtBluRmErh+MaHl3/Dv607aOMyX5Ok+F8IVsomirVsErSwkGymsmVAAsDZC95ae8MdmTpfs2EX3euasUnXXFcMGWEs3LYaqNymFuHcVuReEW/AgDIGIgxixo9ElKg65pv1YGbfUGGEY1Jnrp5E1OV/f0/bsUpY+qlx5w2fig2vbfftz6/b9fC+bFlT26ul5WOqhAUIvxozfa0fn7uqY0QHlYWTRgpo037z8JV2zDv9HFZVWuaADSLnW7B8s0YUhPG9g8O+fLpX7HZ8GyXaQXc4m6Sup4xvkxqI/IqdJUAF7eBd+EXmTtqVVhBRKX0sfPPnSB1ITyS0BFLGpkVL3zwJQjduaNwc3lzK3Lz7NYPs+adUW2ZKAFjO37Rp0b4eRwAkBq0/kjqRhK/CcNrfZ8DAG/skk+Qr3kIgrBKga2wmeKh6SLdB2OaUX3ssVd25qTGiid1/PqV93OepGJJgX/79SapMdgP1nHplaNLhiZERQoCgHcGadwMy27xBGuuOzvDi2jxC94Rs9nqFJh0dMWwdONOPLT2bcfAURXCwlWZPs0KDJ2x1XnE/C1VIYRVQk/CUG2t3OJMJeCGm/FQtmP451NG4uW/d6D1A/dKZsUgohJ+fsWn8fXHNgX6O34w5VEl7EoqGT+yQ1bjoRBvqrhmjGujwNUWmBavqEoQLulRaiIKdIGKUw1Z4Z2BBVmyKLddw4RjBqWPtR4jS/hlYt1R2APTAEMt9Jn/+gseeG6HvFZuQnes13Ug7XJqR9MFEpbVWTGQqY6eeG035j/+RlGu7wYB+NZ5J+C19wuLYygWBKAmohr+/aVujE8IqcVDpTTYJzdfcBKqwoprydVc0QXwbOuHuPHJzYgl9bT6UxPAwtlTpOfc9MUTPRNaVgKUVf9chjQ1NYnm5uY+/U0/MQjmMVt3d0qDX6rCClbPPwtrtn6IxRY/aNMWMOPuta65jhgjrXW2MoxM33HmsUNx+rENeHBtm+cOSUHh8RxumJXU2j46lKpctj3nqGsFRlS4NYYjopJ0N7/gCxNx7593OD5/7KrTXdNnlxNEtEkI0ST7jtVEPvETOGIeM3XMEJw/ZYQj+OWeS0/G0NoIfvZCmyMwrZA6tv0BPxMGC4Ly4uV39+NlSeoRKwqldk4BvLqISrh+5sR0vXKVFEcRJD/Iot5VRQEkObrGNdRIM62a6VxyCVwtN1gYBIQ9+MVaYcmee8cwUOdfx7Y/MHDvvHwgMtx+i0kxYlvc/I/CIQUv/30fbkhXnSteNlBdCIctIqQAZx4/DA/MnYYFKeFjpuO2u6AXkhG5VLAw8KAYUt6+o9i6u9Phmywrs3cknoRAfoPp3InD8Nd3PikoSRlTORCMibzQibccNcZf+8w4LN24U6qyORzTcN3jzlQVxeBLpzWiadxR0knfrQ5CsTIilwoWBi64SXmvZFfZhIdbrWQzMM3sZGaUck1Yldb/9SKiAl8761h8bvII3LV6m2f6C6Z/IFCeE3m+mCpDVTGS2XkVEgpqubOsuR1XnjEev7jyNFjzglkL32QrfWsNXK0EWBhIcJPyh3qSuOvpbRC6QEwTqAobzlj3XHoyBJB1iyjrMLURFVNG1WcYn61pKi4/rRHLmtuhEDkylQLA3KZGrNy8J11NTYDwzaVvIK5puGrGsZg8ajAGV0ewesuegqJUGcaOzKWzGJijQ9MBrUSq00RSx4UPrUc0Ze+be2ojlm1qdx3fMhf0mKZnpJUvd9ibSMLmXQfw5Uc2pgu3A0aIeSKpS1cpRj58kWHgrAorjujijq6Yw2OoKqzg1ouMNL4qkSNiMhJS8FtLSUHzuISm48YvnIjpxzWgNqJiT2cPrnmsWaoacvOMYJh8KZZqqlKRje+VLbtx01NbAMgzo5YD7E2UI9L0FJpAWFVcq4BBEKwGLNkWUVoDYVZvuT4Z8aSOeY9sxI1fOBFH1Ubwm6t6BcOdq7ely/4ZUdDy0pIsCJhiU0mqqdqoikkjB+G19w6kPyvUWC4b32b1tQsffAmAPDNqOcPCQILXpC3DiNaVF9SwYzc+ZSufCRiTuTW0fm5TI/7wxu4MG4CZ5IthmEziSR2bLIIAKFyQuY3vw3EN0ZCKuKW+RqXYDlgYuCDzGBgUDeGmp7akbQYmSU3HFWeMxbLmTJ2i28u3G59kuw0vZLp/TQCSwlQMM2Axq9u5OU5EVMNLKIfy3AAMtbDb+C5mNcW+ZkDaDPJ1Ge3oiqF1z0Ec7E7gusffyFjLhxTg2evPweG45vA0sv+e3SPp2a0f4Of/+07e98MwTO6EFIIQIicjeE1YxcNfOdUz2ti0HZRjvAHbDCzkGxhiPa8nqTmUOkkd2NPZg/1H4hnXt3shzG1qTK/sexI6woq7ES7IMH6GGcgQ3EuIeqFDpKON3XBLelnuBJ6ojojOJ6K3iKiNiL4n+T5KRL9Lfb+RiMYH1Rary6g1LXRHVyyn89y2nQe7447rP/bKzsy/Xzb+Ng3GCV3unvel0xrx4L9OQ7i/ZRVjmDIgVzFQE1EzUttnQ5b0stwJdGdARCqAxQA+D6AdwGtEtFIIYbXEfh3AfiHEBCK6HMDdAL4URHvyDQzxY+QNq4TB1eGi5Rf63WvteGpTu+8CMwzDBENYAR7+8qcxeVR9RU3uuRL0zuB0AG1CiHeEEHEATwC4xHbMJQB+lfr3cgAzibLUqsuTfI07svNCimFIqomoiIYU3H/ZVEweVe+r2pdfWBAwTOm5buZEnDOxMgvW5ELQwmA0AGsV8PbUZ9JjhBBJAJ0AGuwXIqJriaiZiJr37s2v5q1XRbNcz3tg7jT89Xvn4fFrzsBfv2fkMTeLfsswt5lXnjkW0RCrfhimEoiGCPOmjy11M/qEijEgCyGWAFgCGN5E+V4nX+OO23n28+dNH4tFtnrI0RBlbDOvnzkxnd7aDBr77ucn4t4/v+WsbhZQyD/D9BdCBBQ7u3ltRIUmKq+ofSEELQx2Axhj+bsx9ZnsmHYiCgGoB9ARZKP81CbI97yGuijunTPV4Vp2zsSjM46RpbceUV+FG5a1pNVDYZVw/2VTcagniYWrWgEhEPdQHTUOiaL9QK8x/DPHH4U7Z0/BkpfeKZu8RGZcXERV0MN6MMYnBODcE4dh7Vv7Mj6fPXUkvn7WcVi68T0sa+6dWlSzowmBmkjIyNslCJrIXKQRUdrDzxpgOmVUfUV5AhWDQOMMUpP7DgAzYUz6rwGYJ4RotRzzTQCfEkJ8I2VA/hchxFyv65ai0lmuFBrLYGZKtGdENfIQdQMgTB41GPsPx9Gy6wCmjRmCCccMQttHhzL+NjEqQe3FsLoqVIcVbG7vxDknDAMAvPj2PkxtrEc4pOJgdxxdPUns2n8YNZEwThoxCPuPJDC0Joxd+7vReSSOrriGySMHIakD4xtq8MauA2jdcxBNY4egsyeJw/EkJo2sx0kjBmH7hwfxfscRHFUbweih1ensj+azeXdvF/7YshuaLtCT0NCd0HH+5GNwVF0VNry9F5t27kdNSEVNVQjnTz4G73UcwcZ3OzC8NgpFJTSNHYqkABKajgNH4uhO5XZqqIti9JAqNL9/AIdjCXTFNCSSGiIhFQ11EVSlSiTWRFSEVQV//7gLPUkdE4+pxbknHoMPD8awoW0v3tl7GNVhBccOr0NEVfDRoRiG14bx6vv7cSSmYVBVCMfUV0HTdOw/EscnhxNQCAiphMFVEZw0YhDCIRXjj6rG9o+6sOdAN/6+tws9CQ3/MHIQThheh+e3f4TumAZBCjRdQ100jOOG16EnnsR7nxxBdzyJobURTBheh/HD6rDjw0PYfySBM447CuOH1WJL+wG0f3IEIOBQt4Zjh9fg2GG12PlJNwZVhTB6SDV2H+jG3kM9GFQVwu793dj+4SEcM7gasz41AvuOJHD0oCj2HYrhlXc+QU1ExZCaMNoPHEE8oeHT447CpZ9uxJGEjh0fHsT2Dw+lrxtLajiqNoquWBIb3t6Hdzu6oEDBsLoIxg2rwVkThuFAdwLD6qowYnAUW/ccRDSkYO+hHvx972GcNm4oqiIhjG+oQTikIpHU0scAwCeH4xjXUIszj29Ix+i8/Pd92NcVx1kThjn6eMuuA+lrmbZAs68BQOueTgCEUfVVnrFA/RWvOIPAg86I6EIAPwGgAnhUCPFDIroTQLMQYiURVQH4NYBTAHwC4HIhhGcEViUIA4ZhmHKjpEFnQog1ANbYPrvN8u8eAJcF3Q6GYRjGncCDzhiGYZjyh4UBwzAMw8KAYRiGYWHAMAzDoEJTWBPRXgDvl7odATAMwL6sR1Uu/f3+gP5/j3x/lc04IYQ0/3ZFCoP+ChE1u7l99Qf6+/0B/f8e+f76L6wmYhiGYVgYMAzDMCwMyo0lpW5AwPT3+wP6/z3y/fVT2GbAMAzD8M6AYRiGYWHAMAzDgIVBWUFEdxHRFiJqIaI/E9GoUrep2BDRvUS0PXWffyCiIaVuUzEhosuIqJWIdCLqVy6KRHQ+Eb1FRG1E9L1St6eYENGjRPQxEW0tdVtKBQuD8uJeIcTJQohpAFYDuC3bCRXIcwCmCCFOhlHr4pYSt6fYbAXwLwBeLHVDigkRqQAWA7gAwCQA/0pEk0rbqqLyPwDOL3UjSgkLgzJCCHHQ8mctgH5n3RdC/DlV6xoAXoFR/a7fIIR4UwjxVqnbEQCnA2gTQrwjhIgDeALAJSVuU9EQQrwIo57KgKViaiAPFIjohwCuBNAJ4NwSNydorgLwu1I3gvHFaAC7LH+3A5heorYwAcDCoI8houcBjJB89X0hxAohxPcBfJ+IbgEwH8DtfdrAIpDtHlPHfB9AEsDSvmxbMfBzfwxTabAw6GOEEJ/zeehSGBXiKk4YZLtHIvoqgFkAZooKDHTJ4R32J3YDGGP5uzH1GdNPYJtBGUFEJ1j+vATA9lK1JSiI6HwANwGYLYQ4Uur2ML55DcAJRHQsEUUAXA5gZYnbxBQRjkAuI4joKQAnAtBhpOj+hhCiX62+iKgNQBRAR+qjV4QQ3yhhk4oKEf0zgIcADAdwAECLEOKLpW1VcSCiCwH8BIAK4FEhxA9L3KSiQUSPA/gsjBTWHwG4XQjxy5I2qo9hYcAwDMOwmohhGIZhYcAwDMOAhQHDMAwDFgYMwzAMWBgwDMMwYGHAMFKIaAgR/d8++J1/6mcJ35gKhYUBw8gZAsC3MCCDfMbTP8HIAsowJYXjDBhGAhGZWTnfArAOwMkAhgIIA/iBEGIFEY0H8CcAGwGcCuBCGEkGvwxgL4zEbpuEEPcR0fEwUkAPB3AEwDUAjoKRqrwz9d+lQoi/99EtMkwGnJuIYeR8D0bdhWlEFAJQI4Q4SETDALxCRGYqhhMA/B8hxCtEdBqASwFMhSE0XgewKXXcEhgR5W8T0XQAPxNCnJe6zmohxPK+vDmGscPCgGGyQwB+RETnwEgVMhrAManv3hdCvJL69wwAK4QQPQB6iGgVABBRHYDPAHiSiMxrRvuq8QzjBxYGDJOdK2Cod04VQiSI6D0AVanvDvs4XwFwIFXBjmHKEjYgM4ycQwAGpf5dD+DjlCA4F8A4l3M2ALiYiKpSu4FZQLqC3btEdBmQNjZPlfwOw5QMFgYMI0EI0QFgQ6pA+jQATUT0NxgGYmlqcSHEazDSOm8B8AyAv8EwDAPG7uLrRLQZQCt6S0Y+AWABEb2RMjIzTElgbyKGKSJEVCeE6CKiGgAvArhWCPF6qdvFMNlgmwHDFJclqSCyKgC/YkHAVAq8M2AYhmHYZsAwDMOwMGAYhmHAwoBhGIYBCwOGYRgGLAwYhmEYAP8f2C2yR0IKUkIAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DRHfZBVw3FHw","executionInfo":{"status":"ok","timestamp":1627582059533,"user_tz":-540,"elapsed":14,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"972f881f-0248-4433-c624-1d6775f163f7"},"source":["# 二乗誤差が2.0を超えるカラム\n","thr_ = 2.0 \n","train_kf_df[train_kf_df['diff_sq'] > thr_]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>url_legal</th>\n","      <th>license</th>\n","      <th>excerpt</th>\n","      <th>target</th>\n","      <th>standard_error</th>\n","      <th>kfold</th>\n","      <th>bins_tg</th>\n","      <th>bins_std</th>\n","      <th>bins</th>\n","      <th>pred</th>\n","      <th>diff_sq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>59</th>\n","      <td>9ea0d2788</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>The most important bee in the hive is naturall...</td>\n","      <td>0.987862</td>\n","      <td>0.594408</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>9</td>\n","      <td>109</td>\n","      <td>-0.453831</td>\n","      <td>2.078479</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>bcd734621</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Midas was enjoying himself in his treasure-roo...</td>\n","      <td>0.943021</td>\n","      <td>0.537713</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>5</td>\n","      <td>105</td>\n","      <td>-0.822759</td>\n","      <td>3.117980</td>\n","    </tr>\n","    <tr>\n","      <th>304</th>\n","      <td>f04e03fd8</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Jupiter, two hours high, was the herald of the...</td>\n","      <td>-3.229761</td>\n","      <td>0.551435</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>-1.588812</td>\n","      <td>2.692714</td>\n","    </tr>\n","    <tr>\n","      <th>695</th>\n","      <td>e26914a57</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Seedlings from the same fruit, and the young o...</td>\n","      <td>-0.769207</td>\n","      <td>0.469529</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>62</td>\n","      <td>-2.189955</td>\n","      <td>2.018523</td>\n","    </tr>\n","    <tr>\n","      <th>741</th>\n","      <td>24e00a515</td>\n","      <td>https://simple.wikipedia.org/wiki/Inkpad</td>\n","      <td>CC BY-SA 3.0 and GFDL</td>\n","      <td>An inkpad is a small box which contains a pad ...</td>\n","      <td>0.627619</td>\n","      <td>0.520607</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>95</td>\n","      <td>-0.825186</td>\n","      <td>2.110640</td>\n","    </tr>\n","    <tr>\n","      <th>990</th>\n","      <td>afeb324bd</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>On the morning of the 20th of March, a long se...</td>\n","      <td>0.401053</td>\n","      <td>0.481889</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>92</td>\n","      <td>-1.448431</td>\n","      <td>3.420589</td>\n","    </tr>\n","    <tr>\n","      <th>1152</th>\n","      <td>03b761fd9</td>\n","      <td>https://simple.wikipedia.org/wiki/Larva</td>\n","      <td>CC BY-SA 3.0 and GFDL</td>\n","      <td>Probably the most widely accepted theory expla...</td>\n","      <td>-2.778515</td>\n","      <td>0.533111</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>15</td>\n","      <td>-1.333335</td>\n","      <td>2.088545</td>\n","    </tr>\n","    <tr>\n","      <th>1412</th>\n","      <td>8f35441e3</td>\n","      <td>https://www.africanstorybook.org/#</td>\n","      <td>CC BY 4.0</td>\n","      <td>Every day, Emeka's father took him to school i...</td>\n","      <td>1.583847</td>\n","      <td>0.624776</td>\n","      <td>1</td>\n","      <td>11</td>\n","      <td>10</td>\n","      <td>1110</td>\n","      <td>0.006977</td>\n","      <td>2.486518</td>\n","    </tr>\n","    <tr>\n","      <th>1944</th>\n","      <td>04ade0eb2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>While I was hailing the brig, I spied a tract ...</td>\n","      <td>-3.315282</td>\n","      <td>0.544735</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>-1.462916</td>\n","      <td>3.431261</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>bf3ea5462</td>\n","      <td>https://en.wikipedia.org/wiki/Machine_learning</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>Machine learning is a subfield of computer sci...</td>\n","      <td>-3.295576</td>\n","      <td>0.614204</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>-1.877392</td>\n","      <td>2.011246</td>\n","    </tr>\n","    <tr>\n","      <th>2124</th>\n","      <td>76f92b721</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>The biggest desert in the world is in Africa, ...</td>\n","      <td>1.103341</td>\n","      <td>0.553751</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>6</td>\n","      <td>106</td>\n","      <td>-0.375955</td>\n","      <td>2.188318</td>\n","    </tr>\n","    <tr>\n","      <th>2277</th>\n","      <td>7c732b8bb</td>\n","      <td>https://en.wikipedia.org/wiki/Environmental_sc...</td>\n","      <td>CC BY-SA 3.0</td>\n","      <td>Environmental science is an interdisciplinary ...</td>\n","      <td>-3.137143</td>\n","      <td>0.555843</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>16</td>\n","      <td>-1.432657</td>\n","      <td>2.905273</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id  ...   diff_sq\n","59    9ea0d2788  ...  2.078479\n","141   bcd734621  ...  3.117980\n","304   f04e03fd8  ...  2.692714\n","695   e26914a57  ...  2.018523\n","741   24e00a515  ...  2.110640\n","990   afeb324bd  ...  3.420589\n","1152  03b761fd9  ...  2.088545\n","1412  8f35441e3  ...  2.486518\n","1944  04ade0eb2  ...  3.431261\n","1997  bf3ea5462  ...  2.011246\n","2124  76f92b721  ...  2.188318\n","2277  7c732b8bb  ...  2.905273\n","\n","[12 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8hJjGdas3FFE","executionInfo":{"status":"ok","timestamp":1627582059534,"user_tz":-540,"elapsed":14,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"168e601b-0cd4-43a2-8068-85c717e0c14a"},"source":["# 二乗誤差が2.0を超える文章\n","thr_ = 2.0 \n","tmp_df = train_kf_df[train_kf_df['diff_sq'] > thr_].copy()\n","for i in tmp_df.index:\n","  print(tmp_df.loc[i].target)\n","  #print(tmp_df.loc[i].standard_error)\n","  print(tmp_df.loc[i].pred)\n","  print(tmp_df.loc[i].excerpt)\n","  print('--------------------------')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.987861812\n","-0.4538312554359436\n","The most important bee in the hive is naturally the queen. She is longer and sleeker than the others, and has a crooked sting, of which, however, she seldom makes use. Similar in form, but smaller, are the working-bees, whose sting is straight. The male bee, or drone, is thicker than the others, and stingless.\n","\"What has the queen to do in the hive?\" I asked. The old gentleman replied, \"She is the mother-bee, lays all the eggs, and is so diligent that she often lays twelve hundred in a day, having a separate cell for each egg. That is her only work; for she leaves the whole care of her children to the industrious working-bees, who have various labors to perform. Some of them build cells of wax; others bring in honey on the dust of flowers, called pollen; yet others feed and take care of the young; and a small number act as body-guard to the queen.\"\n","--------------------------\n","0.943020903\n","-0.8227593302726746\n","Midas was enjoying himself in his treasure-room, one day, as usual, when he perceived a shadow fall over the heaps of gold; and, looking suddenly up, what should he behold but the figure of a stranger, standing in the bright and narrow sunbeam! It was a young man, with a cheerful and ruddy face. Whether it was that the imagination of King Midas threw a yellow tinge over everything, or whatever the cause might be, he could not help fancying that the smile with which the stranger regarded him had a kind of golden radiance in it. Certainly, although his figure intercepted the sunshine, there was now a brighter gleam upon all the piled-up treasures than before. Even the remotest corners had their share of it, and were lighted up, when the stranger smiled, as with tips of flame and sparkles of fire.\n","--------------------------\n","-3.229761439\n","-1.5888123512268066\n","Jupiter, two hours high, was the herald of the day; the Pleiades, just above the horizon, shed their sweet influence in the east; Lyra sparkled near the zenith; Andromeda veiled her newly discovered glories from the naked eye in the south; the steady Pointers, far beneath the pole, looked meekly up from the depths of the north to their sovereign.\n","Such was the glorious spectacle as I entered the train. As we proceeded, the timid approach of twilight became more perceptible; the intense blue of the sky began to soften; the smaller stars, like little children, went first to rest; the sister-beams of the Pleiades soon melted together; but the bright constellations of the west and north remained unchanged. Steadily the wondrous transfiguration went on. Hands of angels, hidden from mortal eyes, shifted the scenery of the heavens; the glories of night dissolved into the glories of the dawn.\n","--------------------------\n","-0.7692072529999999\n","-2.1899547576904297\n","Seedlings from the same fruit, and the young of the same litter, sometimes differ considerably from each other, though both the young and the parents, as Muller has remarked, have apparently been exposed to exactly the same conditions of life; and this shows how unimportant the direct effects of the conditions of life are in comparison with the laws of reproduction, and of growth, and of inheritance; for had the action of the conditions been direct, if any of the young had varied, all would probably have varied in the same manner. To judge how much, in the case of any variation, we should attribute to the direct action of heat, moisture, light, food, etc., is most difficult: my impression is, that with animals such agencies have produced very little direct effect, though apparently more in the case of plants.\n","--------------------------\n","0.627618676\n","-0.8251855969429016\n","An inkpad is a small box which contains a pad of cloth or other material. It is impregnated with ink (the pad is inky). A marker is pressed onto the pad, then onto paper. Any raised marks on the pad leave an impression in ink on the paper.\n","Ink pads are used with rubber stamps. On the stamp is the symbol of an organization, for example. After a form has been passed by an official, it is stamped to show it is authentic. Another variation is a date stamp, placed on all letters which arrive in the building. Another version is a stamp with a facsimile (copy) of an official's signature. Using this, staff can send out letters when the official is not present. Rubber stamps and ink pads have been used for at least a hundred years by civil servants and businesses. They are still in use in many countries but are gradually being replaced with other systems.\n","--------------------------\n","0.401052549\n","-1.4484308958053589\n","On the morning of the 20th of March, a long series of earthquakes spread alarm throughout all the cities and numerous villages that are scattered over the sides of Mt. Etna. The shocks followed each other at intervals of a few minutes; dull subterranean rumblings were heard; and a catastrophe was seen to be impending. Toward evening the ground cracked at the lower part of the south side of the mountain, at the limit of the cultivated zone, and at four kilometers to the north of the village of Nicolosi. There formed on the earth a large number of very wide fissures, through which escaped great volumes of steam and gases which enveloped the mountain in a thick haze; and toward night, a very bright red light, which, seen from Catania, seemed to come out in great waves from the foot of the mountain, announced the coming of the lava.\n","--------------------------\n","-2.778515087\n","-1.333335041999817\n","Probably the most widely accepted theory explaining the evolution of larval stages is the need for dispersal. Sessile organisms such as barnacles and tunicates, and sea-floor groups like mussels and crabs, need some way to move their young into new territory, since they cannot move long distances as adults. Many species have relatively long pelagic larval stages (how long a larva is in the water column). During this time, larvae feed and grow, and many species move through several stages of development. For example, most barnacles molt through six nauplius larva stages before molting to a cipris, when they look to settle. The larvae eat different food from the adults, and disperse.\n","The other consideration is the small size of the eggs. If animals lay many small eggs (and most do), then the young stages cannot live the life the adults lead. They must live a separate life until they have the size and capability to live as an adult. This is what the larvae do.\n","--------------------------\n","1.583846826\n","0.00697714788839221\n","Every day, Emeka's father took him to school in his car. He also brought Emeka home after school. One afternoon on their way home, Emeka's father stopped to buy something at a big shop. From the car, Emeka looked across the road and saw an old man. He was carrying a big load on his head. He was tired and walked slowly. Emeka kept looking at him. The old man sat under the shade of a tree on the walkway and opened his bag. He had two flat plastic water bottles, which he was making into shoes. Emeka thought about that old man for a long time. He felt sad. When he got home, he could not eat. He thought about what he could do. He got up and took some money from his money bag. He called Chita and jumped on his bicycle. Emeka rode to the shop where his father had shopped. The boy ran into the shop and came out with a bag. He went to where the old man was resting against a tree. Emeka called out, \"Good afternoon, sir.\" The man answered, \"Peace to you, my child.\"\n","--------------------------\n","-3.31528229\n","-1.4629158973693848\n","While I was hailing the brig, I spied a tract of water lying between us, where no great waves came, but which yet boiled white all over and bristled in the moon with rings and bubbles. Sometimes the whole tract swung to one side, like the tail of a live serpent; sometimes, for a glimpse, it would all disappear and then boil up again. What it was I had no guess, which for the time increased my fear of it; but I now know it must have been the roost or tide-race, which had carried me away so fast and tumbled me about so cruelly, and at last, as if tired of that play, had flung out me and the spare yard upon its landward margin.\n","I now lay quite becalmed, and began to feel that a man can die of cold as well as of drowning. The shores of Earraid were close in; I could see in the moonlight the dots of heather and the sparkling of the mica in the rocks.\n","--------------------------\n","-3.2955761010000004\n","-1.8773919343948364\n","Machine learning is a subfield of computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"Field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,:2 rather than following strictly static program instructions.\n","Machine learning is closely related to (and often overlaps with) computational statistics; a discipline which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is unfeasible. Example applications include spam filtering, optical character recognition (OCR), search engines and computer vision.\n","--------------------------\n","1.103341259\n","-0.37595510482788086\n","The biggest desert in the world is in Africa, and is called the Sahara. It is almost as large as the Atlantic Ocean, but instead of water it is all sands and rocks. Like the ocean, it is visited with storms; dreadful gales, when the wind scoops up thousands of tons of sand and drives them forward, burying and crushing all they meet. And it has islands, too—small green patches, where springs bubble through the ground, and ferns and acacias and palm-trees grow. When a traveler sees one of these fertile spots afar off, he feels as a tempest-tossed sailor does at sight of land. It is delightful to quit the hot, baking sun, sit in shadow under the trees, and rest the eyes, long wearied with dazzling sands, on the sweet green and the clear spring. Oases, these islands are called. Long distances divide them. It is often a race for life to get across from one to the other.\n","--------------------------\n","-3.1371432610000003\n","-1.432657241821289\n","Environmental science is an interdisciplinary academic field that integrates physical, biological and information sciences (including ecology, biology, physics, chemistry, zoology, mineralogy, oceanology, limnology, soil science, geology, atmospheric science, and geodesy) to the study of the environment, and the solution of environmental problems. Environmental science emerged from the fields of natural history and medicine during the Enlightenment. Today it provides an integrated, quantitative, and interdisciplinary approach to the study of environmental systems.\n","Related areas of study include environmental studies and environmental engineering. Environmental studies incorporate more of the social sciences for understanding human relationships, perceptions and policies towards the environment. Environmental engineering focuses on design and technology for improving environmental quality in every aspect. Environmental scientists work on subjects like the understanding of earth processes, evaluating alternative energy systems, pollution control and mitigation, natural resource management, and the effects of global climate change. Environmental issues almost always include an interaction of physical, chemical, and biological processes.\n","--------------------------\n"],"name":"stdout"}]}]}