{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"051-train-03.ipynb","provenance":[{"file_id":"1rYFkiaQb9GijWty1a1J6F79auTXNP0mN","timestamp":1627219895484},{"file_id":"17a4F4aC9L0QBqU8BRTrdqPn0WwJ0b08b","timestamp":1626746992716},{"file_id":"1G_W9irFTrEmDeHR0S6_u0bjpk8nxipXW","timestamp":1626689695352},{"file_id":"1bhhkorT--y8XXaVLM8hibVgC-tLqZ16P","timestamp":1626358153868},{"file_id":"1WtT2hX6O9Qbt_hb9sF50nM2QmDXFi-XA","timestamp":1626338366006},{"file_id":"1k_p5wftcUeo711Xho1-T5an2Xkneau-J","timestamp":1626323813472},{"file_id":"1Vz2GB2BNTWuefEFkCSh3TBPEIel7KG1t","timestamp":1626317426487},{"file_id":"1djoMWojeaIPopG5tS1jNMohn8ineblRh","timestamp":1626306831897},{"file_id":"1-6tlDO8158Pi6TpptIF884oFaEiT4Uxb","timestamp":1626276420047},{"file_id":"1js8eA3mDNS8mwSpCiHuzPeARFlUPAVrg","timestamp":1626272452526},{"file_id":"1yhcPgulwJtjJKUK9IuRKmNMhJ-4YXGol","timestamp":1626267205517},{"file_id":"1mnnSv0Pofn1QxArywV81VYqnZPB8uUWN","timestamp":1626180468522},{"file_id":"1RRdjt_UAeHmr5QQBAMyC82Fq1s31OWdK","timestamp":1625833136005},{"file_id":"1JPgg44HFemzwk8VSCXih3PejL0idy-C4","timestamp":1625825483466},{"file_id":"1Ye6wqVX71xAAAhmjXkw9IpRvTqeUyJDA","timestamp":1625812137500}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucCbvGD1XvG7","executionInfo":{"status":"ok","timestamp":1627236066097,"user_tz":-540,"elapsed":343,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"afd527cb-56ef-4042-a70b-3fc266b0d599"},"source":["import sys\n","if 'google.colab' in sys.modules:  # colab特有の処理_2回目以降\n","  # Google Driveのマウント\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  # ライブラリのパス指定\n","  sys.path.append('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FACwJ6icpxrR","executionInfo":{"status":"ok","timestamp":1627236071061,"user_tz":-540,"elapsed":4657,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# データセットをDriveから取得\n","!mkdir -p 'input'\n","!mkdir -p 'clrp-pre-trained'\n","\n","!cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input/commonlitreadabilityprize/' '/content/input'\n","!cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/clrp_pretrained_manish_epoch5/pre-trained-roberta/clrp_roberta_large/' '/content/clrp-pre-trained'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"RV9-VwbpZLZ9","executionInfo":{"status":"ok","timestamp":1627236071062,"user_tz":-540,"elapsed":12,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["from pathlib import Path\n","\n","# input\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    DATA_DIR = Path('../input/commonlitreadabilityprize/')\n","\n","elif 'google.colab' in sys.modules: # Colab環境\n","    DATA_DIR = Path('/content/input/commonlitreadabilityprize')\n","\n","else:\n","    DATA_DIR = Path('../00_input/commonlitreadabilityprize/')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5difyXe00UV","executionInfo":{"status":"ok","timestamp":1627236071062,"user_tz":-540,"elapsed":9,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["from pathlib import Path\n","\n","# tokenizer\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    TOKENIZER_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    TOKENIZER_DIR = '/content/clrp-pre-trained/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    TOKENIZER_DIR = 'roberta-large'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"tKjsUxnOeDYl","executionInfo":{"status":"ok","timestamp":1627236071063,"user_tz":-540,"elapsed":10,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["from pathlib import Path\n","\n","# pre-trained model\n","if 'kaggle_web_client' in sys.modules:  # kaggle環境\n","    PRE_TRAINED_MODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n","elif 'google.colab' in sys.modules: # Colab環境\n","    PRE_TRAINED_MODEL_DIR = '/content/clrp-pre-trained/clrp_roberta_large' # 仮で、毎回DLする想定のモデル名を指定。あとで変更予定。\n","else:\n","    PRE_TRAINED_MODEL_DIR = 'roberta-large'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLaT2V0ReoAZ","executionInfo":{"status":"ok","timestamp":1627236071064,"user_tz":-540,"elapsed":10,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["UPLOAD_DIR = Path('/content/model')\n","EX_NO = '051-train-03'  # 実験番号などを入れる、folderのpathにする\n","USERID = 'calpis10000'"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOGjAb4pAJ0F","executionInfo":{"status":"ok","timestamp":1627236071064,"user_tz":-540,"elapsed":9,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["import subprocess\n","import shlex\n","\n","def gpuinfo():\n","    \"\"\"\n","    Returns size of total GPU RAM and used GPU RAM.\n","\n","    Parameters\n","    ----------\n","    None\n","\n","    Returns\n","    -------\n","    info : dict\n","        Total GPU RAM in integer for key 'total_MiB'.\n","        Used GPU RAM in integer for key 'used_MiB'.\n","    \"\"\"\n","\n","    command = 'nvidia-smi -q -d MEMORY | sed -n \"/FB Memory Usage/,/Free/p\" | sed -e \"1d\" -e \"4d\" -e \"s/ MiB//g\" | cut -d \":\" -f 2 | cut -c2-'\n","    commands = [shlex.split(part) for part in command.split(' | ')]\n","    for i, cmd in enumerate(commands):\n","        if i==0:\n","            res = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n","        else:\n","            res = subprocess.Popen(cmd, stdin=res.stdout, stdout=subprocess.PIPE)\n","    total, used = map(int, res.communicate()[0].decode('utf-8').strip().split('\\n'))\n","    info = {'total_MiB':total, 'used_MiB':used}\n","    return info\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g3-6m5MKXecB"},"source":["# Overview\n","This nb is based on copy from https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch .\n","\n","Acknowledgments(from base nb): \n","some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T06:26:32.834365Z","iopub.execute_input":"2021-07-04T06:26:32.834903Z","iopub.status.idle":"2021-07-04T06:26:40.143740Z","shell.execute_reply.started":"2021-07-04T06:26:32.834785Z","shell.execute_reply":"2021-07-04T06:26:40.142864Z"},"trusted":true,"id":"HRsRZ06WXecD","executionInfo":{"status":"ok","timestamp":1627236076863,"user_tz":-540,"elapsed":5806,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["import os\n","import math\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from transformers import AdamW # optimizer\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoConfig\n","from transformers import get_cosine_schedule_with_warmup # scheduler\n","from pytorch_memlab import profile\n","import pytorch_memlab\n","from pytorch_memlab import MemReporter\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import gc\n","gc.enable()"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.145217Z","iopub.execute_input":"2021-07-04T06:26:40.145539Z","iopub.status.idle":"2021-07-04T06:26:40.201326Z","shell.execute_reply.started":"2021-07-04T06:26:40.145504Z","shell.execute_reply":"2021-07-04T06:26:40.200136Z"},"trusted":true,"id":"omBfwshTXecE","executionInfo":{"status":"ok","timestamp":1627236076863,"user_tz":-540,"elapsed":24,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["NUM_FOLDS = 5 # K Fold\n","NUM_EPOCHS = 8 # Epochs\n","BATCH_SIZE = 8 # Batch Size\n","MAX_LEN = 300 # ベクトル長\n","EVAL_SCHEDULE = [(0.55, 64), (-1., 32)] # schedulerの何らかの設定？\n","ROBERTA_PATH = PRE_TRAINED_MODEL_DIR # roberta pre-trainedモデル(モデルとして指定)\n","TOKENIZER_PATH = TOKENIZER_DIR # roberta pre-trainedモデル(Tokenizerとして指定)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # cudaがなければcpuを使えばいいじゃない"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.203398Z","iopub.execute_input":"2021-07-04T06:26:40.204055Z","iopub.status.idle":"2021-07-04T06:26:40.211572Z","shell.execute_reply.started":"2021-07-04T06:26:40.204015Z","shell.execute_reply":"2021-07-04T06:26:40.210762Z"},"trusted":true,"id":"4qcuXqwtXecF","executionInfo":{"status":"ok","timestamp":1627236076864,"user_tz":-540,"elapsed":23,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["def set_random_seed(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n","\n","    torch.manual_seed(random_seed)\n","    torch.cuda.manual_seed(random_seed)\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","    torch.backends.cudnn.deterministic = True# cudnnによる最適化で結果が変わらないためのおまじない "],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.214188Z","iopub.execute_input":"2021-07-04T06:26:40.214809Z","iopub.status.idle":"2021-07-04T06:26:40.309744Z","shell.execute_reply.started":"2021-07-04T06:26:40.214769Z","shell.execute_reply":"2021-07-04T06:26:40.308926Z"},"trusted":true,"id":"70PyLsJTXecF","executionInfo":{"status":"ok","timestamp":1627236076864,"user_tz":-540,"elapsed":22,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# train, testを読む\n","train_df = pd.read_csv(DATA_DIR/\"train.csv\")\n","\n","# Remove incomplete entries if any.\n","train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n","              inplace=True)\n","train_df.reset_index(drop=True, inplace=True)\n","\n","test_df = pd.read_csv(DATA_DIR/\"test.csv\")\n","submission_df = pd.read_csv(DATA_DIR/\"sample_submission.csv\")"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"9ZYOB59L8qtA","executionInfo":{"status":"ok","timestamp":1627236076865,"user_tz":-540,"elapsed":23,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"308a5884-62dd-4989-8eb8-13b4fee17821"},"source":["train_df.head()\n"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>url_legal</th>\n","      <th>license</th>\n","      <th>excerpt</th>\n","      <th>target</th>\n","      <th>standard_error</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>c12129c31</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>When the young people returned to the ballroom...</td>\n","      <td>-0.340259</td>\n","      <td>0.464009</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>85aa80a4c</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n","      <td>-0.315372</td>\n","      <td>0.480805</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>b69ac6792</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>As Roger had predicted, the snow departed as q...</td>\n","      <td>-0.580118</td>\n","      <td>0.476676</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>dd1000b26</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>And outside before the palace a great garden w...</td>\n","      <td>-1.054013</td>\n","      <td>0.450007</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>37c1b32fb</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Once upon a time there were Three Bears who li...</td>\n","      <td>0.247197</td>\n","      <td>0.510845</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id url_legal  ...    target standard_error\n","0  c12129c31       NaN  ... -0.340259       0.464009\n","1  85aa80a4c       NaN  ... -0.315372       0.480805\n","2  b69ac6792       NaN  ... -0.580118       0.476676\n","3  dd1000b26       NaN  ... -1.054013       0.450007\n","4  37c1b32fb       NaN  ...  0.247197       0.510845\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.311021Z","iopub.execute_input":"2021-07-04T06:26:40.311347Z","iopub.status.idle":"2021-07-04T06:26:40.624393Z","shell.execute_reply.started":"2021-07-04T06:26:40.311314Z","shell.execute_reply":"2021-07-04T06:26:40.623347Z"},"trusted":true,"id":"xf0662k4XecF","executionInfo":{"status":"ok","timestamp":1627236076866,"user_tz":-540,"elapsed":11,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# tokenizerを指定\n","tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6aaghNkXecG"},"source":["# Dataset"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.628883Z","iopub.execute_input":"2021-07-04T06:26:40.629347Z","iopub.status.idle":"2021-07-04T06:26:40.644338Z","shell.execute_reply.started":"2021-07-04T06:26:40.629309Z","shell.execute_reply":"2021-07-04T06:26:40.643336Z"},"trusted":true,"id":"zkopT0U1XecG","executionInfo":{"status":"ok","timestamp":1627236076868,"user_tz":-540,"elapsed":12,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# Dataset用のClass。おそらく、trainとtestでインスタンスを生成し、DataFrameと同じように扱えるような思想。\n","class LitDataset(Dataset):\n","    def __init__(self, df, inference_only=False):\n","        super().__init__()\n","\n","        self.df = df        \n","        self.inference_only = inference_only # Testデータ用フラグ\n","        self.text = df.excerpt.tolist() # 分析対象カラムをlistにする。(分かち書きではなく、Seriesをlistへ変換するような処理)\n","        #self.text = [text.replace(\"\\n\", \" \") for text in self.text] # 単語単位で分かち書きする場合\n","        \n","        if not self.inference_only:\n","            self.target = torch.tensor(df.target.values, dtype=torch.float32) # trainのみ、targetをtensorに変換\n","            self.standard_error = torch.tensor(df.standard_error.values, dtype=torch.float32) \n","\n","        self.encoded = tokenizer.batch_encode_plus( # textをtokenize\n","            self.text,\n","            padding = 'max_length',            \n","            max_length = MAX_LEN,\n","            truncation = True, # 最大長を超える文字は切り捨て\n","            return_attention_mask=True\n","        )        \n"," \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, index): # 変換結果を返す\n","        input_ids = torch.tensor(self.encoded['input_ids'][index])\n","        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n","        \n","        if self.inference_only:\n","            return (input_ids, attention_mask)            \n","        else:\n","            target = self.target[index]\n","            standard_error = self.standard_error[index]\n","            return (input_ids, attention_mask, target, standard_error)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKtdy32wXecG"},"source":["# Model\n","The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.649629Z","iopub.execute_input":"2021-07-04T06:26:40.650066Z","iopub.status.idle":"2021-07-04T06:26:40.666374Z","shell.execute_reply.started":"2021-07-04T06:26:40.650002Z","shell.execute_reply":"2021-07-04T06:26:40.665211Z"},"trusted":true,"id":"BpkxjXEUXecH","executionInfo":{"status":"ok","timestamp":1627236076868,"user_tz":-540,"elapsed":12,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["class LitModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(ROBERTA_PATH) # pretrainedからconfigを読み込み\n","        config.update({\"output_hidden_states\":True, # config更新: embedding層を抽出\n","                       \"hidden_dropout_prob\": 0.0, # config更新: dropoutしない\n","                       \"layer_norm_eps\": 1e-7}) # config更新: layer normalizationのepsilon                      \n","        \n","        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config) # cpuで処理する\n","            \n","        self.attention = nn.Sequential(# attentionレイヤー            \n","            nn.Linear(config.hidden_size, 512),      \n","            nn.Tanh(),                       \n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","        \n","        self.layer_norm = nn.LayerNorm(config.hidden_size) # layer_normレイヤー\n","        \n","        self.regressor = nn.Sequential( # 出力レイヤー                    \n","            nn.Linear(config.hidden_size, 2)                        \n","        )\n","\n","    def forward(self, input_ids, attention_mask):\n","        roberta_output = self.roberta(input_ids=input_ids, # robertaに入力データを流し、出力としてrobertaモデル(layerの複合体)を得る\n","                                      attention_mask=attention_mask)     \n","\n","        last_hidden_state = roberta_output.hidden_states[-1] # robertaモデルの最後のlayerを得る\n","        weights = self.attention(last_hidden_state) # robertaの最後のlayerをattentionへ入力し、出力として重みを得る                \n","        context_vector = torch.sum(weights * last_hidden_state, dim=1) # 重み×最後の層を足し合わせて文書ベクトルとする。\n","        norm_embeddings = self.layer_norm(context_vector)\n","        return self.regressor(norm_embeddings) # 文書ベクトルを線形層に入力し、targetを出力する\n","\n","        # https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently\n","        #last_hidden_state = roberta_output[0]\n","        #input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        #sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        #sum_mask = input_mask_expanded.sum(1)\n","        #sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        #mean_embeddings = sum_embeddings / sum_mask\n","\n","        \n","        # Now we reduce the context vector to the prediction score.\n","        #return self.regressor(mean_embeddings) # 文書ベクトルを線形層に入力し、targetを出力する"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.672515Z","iopub.execute_input":"2021-07-04T06:26:40.672944Z","iopub.status.idle":"2021-07-04T06:26:40.684593Z","shell.execute_reply.started":"2021-07-04T06:26:40.672908Z","shell.execute_reply":"2021-07-04T06:26:40.683569Z"},"trusted":true,"id":"bB4jvQTxXecH","executionInfo":{"status":"ok","timestamp":1627236076869,"user_tz":-540,"elapsed":12,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# 評価指標(MSE)の計算。最終的に、ルートしてRMSEにすると思われる。\n","def eval_mse(model, data_loader):\n","    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモードを選択。Batch Normとかdropoutをしなくなる           \n","    mse_mean_sum = 0\n","    mse_std_sum = 0\n","\n","    with torch.no_grad(): # 勾配の計算をしないBlock\n","        for batch_num, (input_ids, attention_mask, target, standard_error) in enumerate(data_loader): # data_loaderからinput, attentin_mask, targetをbatchごとに取り出す\n","            input_ids = input_ids.to(DEVICE)   \n","            attention_mask = attention_mask.to(DEVICE)   \n","            target = target.to(DEVICE)      \n","            standard_error = standard_error.to(DEVICE) \n","            \n","            output = model(input_ids, attention_mask) # 取得した値をモデルへ入力し、出力として予測値を得る。\n","\n","            mse_mean_sum += nn.MSELoss(reduction=\"sum\")(output[:,0].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","            mse_std_sum += nn.MSELoss(reduction=\"sum\")(output[:,1].flatten(), target).item() # 誤差の合計を得る(Batchごとに計算した誤差を足し上げる)\n","\n","    del input_ids\n","    del attention_mask\n","    del target\n","\n","    mse_mean_result = mse_mean_sum / len(data_loader.dataset)\n","    mse_std_result = mse_std_sum / len(data_loader.dataset)\n","  \n","    return mse_mean_result, mse_std_result # 誤差の合計をdataset長で除し、mseを取得＆返す"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.690155Z","iopub.execute_input":"2021-07-04T06:26:40.692530Z","iopub.status.idle":"2021-07-04T06:26:40.703425Z","shell.execute_reply.started":"2021-07-04T06:26:40.692488Z","shell.execute_reply":"2021-07-04T06:26:40.702366Z"},"trusted":true,"id":"47bDno_LXecI","executionInfo":{"status":"ok","timestamp":1627236077318,"user_tz":-540,"elapsed":461,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# 推論結果を返す\n","def predict(model, data_loader):\n","    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n","    model.eval() # evalモード(dropout, batch_normしない)\n","\n","    result = np.zeros(len(data_loader.dataset)) # 結果をdataset長のzero配列として用意\n","    index = 0\n","    \n","    with torch.no_grad(): # 勾配の計算をしないblock(inputすると、現状の重みによる推論結果を返す)\n","        for batch_num, (input_ids, attention_mask) in enumerate(data_loader): # data_loaderからbatchごとにinputを得る\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)\n","                        \n","            output = model(input_ids, attention_mask) # modelにinputを入力し、予測結果を得る。\n","\n","            result[index : index + output[:,0].shape[0]] = output[:,0].flatten().to(\"cpu\") # result[index ~ predの長さ]へ、予測結果を格納\n","            index += pred.shape[0] # indexを更新\n","\n","    return result # 全batchで推論が終わったら、結果を返す"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.708605Z","iopub.execute_input":"2021-07-04T06:26:40.709024Z","iopub.status.idle":"2021-07-04T06:26:40.730675Z","shell.execute_reply.started":"2021-07-04T06:26:40.708983Z","shell.execute_reply":"2021-07-04T06:26:40.729705Z"},"trusted":true,"id":"oInneuAmXecI","executionInfo":{"status":"ok","timestamp":1627236077319,"user_tz":-540,"elapsed":11,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# 学習\n","def train(model, # モデル\n","          model_path, # モデルのアウトプット先\n","          train_loader, # train-setのdata_loader\n","          val_loader, # valid-setのdata_loader\n","          optimizer, # optimizer\n","          scheduler=None, # scheduler, デフォルトはNone\n","          num_epochs=NUM_EPOCHS # epoch数、notebook冒頭で指定した値\n","         ):    \n","    \n","    best_val_rmse = None\n","    best_epoch = 0\n","    step = 0\n","    last_eval_step = 0\n","    eval_period = EVAL_SCHEDULE[0][1] # eval期間(って何？) 冒頭で決めたEVAL_SCHEDULEの最初のtupleの[1]を取得\n","\n","    start = time.time() # 時間計測用\n","\n","    for epoch in range(num_epochs): # 指定したEpoch数だけ繰り返し\n","        val_rmse = None         \n","\n","        for batch_num, (input_ids, attention_mask, target, standard_error) in enumerate(train_loader): # train_loaderからinput, targetを取得\n","            input_ids = input_ids.to(DEVICE) # inputをDEVICEへ突っ込む\n","            attention_mask = attention_mask.to(DEVICE)       \n","            target = target.to(DEVICE)\n","            standard_error = standard_error.to(DEVICE)  \n","\n","            optimizer.zero_grad() # 勾配を初期化            \n","            model.train() # 学習モード開始\n","\n","            output = model(input_ids, attention_mask) # input,attention_maskを入力し、予測結果を得る\n","            \n","            # by Yirun Zhang: https://www.kaggle.com/c/commonlitreadabilityprize/discussion/239421\n","            #p = torch.distributions.Normal(output[:,0], torch.sqrt(output[:,1]**2))\n","            #q = torch.distributions.Normal(target, standard_error)\n","            #kl_vector = torch.distributions.kl_divergence(p, q)\n","            #loss = kl_vector.mean()\n","\n","            # by cccntu: https://www.kaggle.com/c/commonlitreadabilityprize/discussion/239421\n","            crit = torch.nn.GaussianNLLLoss()\n","            logits = output[:,0] # num_labels = 2\n","            standard_error = output[:,1]\n","            loss = crit(input=logits, target=target, var=standard_error ** 2) # var needs to be positive\n","\n","            loss.backward() # 誤差逆伝播法により勾配を得る\n","            optimizer.step() # 重みを更新する\n","\n","            if scheduler:\n","                scheduler.step() # schedulerが与えられた場合は、schedulerの学習率更新\n","            \n","            if step >= last_eval_step + eval_period: # batchを回すごとにstepを増やしていって、「前回evalしたstep + eval_period(16)」を超えたら実行。\n","                # Evaluate the model on val_loader.\n","                elapsed_seconds = time.time() - start # 経過時間\n","                num_steps = step - last_eval_step # 経過ステップ数\n","                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n","                last_eval_step = step # 前回stepの更新\n","                \n","                # valid-setによるrmse計算\n","                train_mean_mse = nn.MSELoss(reduction=\"mean\")(output[:,0].flatten(), target) \n","                train_std_mse = nn.MSELoss(reduction=\"mean\")(torch.sqrt(output[:,1]**2).flatten(), standard_error) \n","\n","                train_mean_rmse = math.sqrt(train_mean_mse)\n","                train_std_rmse = math.sqrt(train_std_mse)\n","\n","                val_mean_mse, val_std_mse = eval_mse(model, val_loader)\n","                val_mean_rmse = math.sqrt(val_mean_mse)                            \n","                val_std_rmse = math.sqrt(val_std_mse)                            \n","\n","                print(f\"Epoch: {epoch} batch_num: {batch_num}\")\n","                print(f\"train_rmse_target: {train_mean_rmse:0.4}\",\n","                      f\"train_rmse_stderror: {train_std_rmse:0.4}\",\n","                      f\"train_kl_div: {loss:0.4}\",\n","                      )\n","                print(f\"val_rmse_target: {val_mean_rmse:0.4}\",\n","                      f\"val_rmse_stderror: {val_std_rmse:0.4}\"\n","                      )\n","\n","                for rmse, period in EVAL_SCHEDULE: # eval_periodをvalid-rmseで切り替える処理\n","                    if val_mean_rmse >= rmse: # valid rmseをEVAL_SCHEDULEと比較し、0項 > valid rmseとなるまで回す : EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n","                        eval_period = period # eval_periodを更新\n","                        break                               \n","\n","                if not best_val_rmse or val_mean_rmse < best_val_rmse: # 初回(best_val_rmse==None), またはbest_val_rmseを更新したらモデルを保存する\n","                    best_val_rmse = val_mean_rmse\n","                    best_epoch = epoch\n","                    torch.save(model.state_dict(), model_path) # 最高の自分を保存\n","                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n","                else:       \n","                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\", # 更新されない場合は、元のスコアを表示\n","                          f\"(from epoch {best_epoch})\")      \n","                                                  \n","                start = time.time()\n","            \n","            # batchごとにメモリ解放\n","            del input_ids\n","            del attention_mask\n","            del target\n","            torch.cuda.empty_cache()                                            \n","            step += 1\n","    \n","    return best_val_rmse"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.735798Z","iopub.execute_input":"2021-07-04T06:26:40.738398Z","iopub.status.idle":"2021-07-04T06:26:40.750876Z","shell.execute_reply.started":"2021-07-04T06:26:40.738356Z","shell.execute_reply":"2021-07-04T06:26:40.749635Z"},"trusted":true,"id":"rMY0fjXwXecJ","executionInfo":{"status":"ok","timestamp":1627236077319,"user_tz":-540,"elapsed":10,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# optimizerの作成\n","def create_optimizer(model):\n","    named_parameters = list(model.named_parameters()) # モデルパラメータの取得\n","    \n","    roberta_parameters = list(model.roberta.named_parameters())[:-2] # パラメータをroberta用、attention用、regressor用に格納。(直接引っ張ってくる形式に変更)\n","    attention_parameters = list(model.attention.named_parameters())\n","    regressor_parameters = list(model.regressor.named_parameters())\n","    norm_parameters = list(model.layer_norm.named_parameters())\n","\n","    attention_group = [params for (name, params) in attention_parameters] # attention用パラメータをリストとして取得\n","    regressor_group = [params for (name, params) in regressor_parameters] # reg用パラメータをリストとして取得\n","    norm_group = [params for (name, params) in norm_parameters] # reg用パラメータをリストとして取得\n","\n","    parameters = []\n","    parameters.append({\"params\": attention_group}) # パラメータをリストに辞書として格納していく\n","    parameters.append({\"params\": regressor_group})\n","    parameters.append({\"params\": norm_group})\n","\n","    for layer_num, (name, params) in enumerate(roberta_parameters): # レイヤーごとにname, paramsを取得していろんな処理\n","        weight_decay = 0.0 if \"bias\" in name else 0.01\n","\n","        lr = 1e-5\n","\n","        if layer_num >= 69:        \n","            lr = 2e-5\n","\n","        if layer_num >= 133:\n","            lr = 5e-5\n","\n","        parameters.append({\"params\": params,\n","                           \"weight_decay\": weight_decay,\n","                           \"lr\": lr})\n","\n","    return AdamW(parameters) # 最終的に、AdamWにパラメータを入力する。\n"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbaJojz0Zjif","executionInfo":{"status":"ok","timestamp":1627236077320,"user_tz":-540,"elapsed":10,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["# https://www.kaggle.com/abhishek/step-1-create-folds\n","def create_folds(data, num_splits, SEED, return_df=False):\n","    # we create a new column called kfold and fill it with -1\n","    data[\"kfold\"] = -1\n","    \n","    # the next step is to randomize the rows of the data\n","    data = data.sample(frac=1).reset_index(drop=True)\n","\n","    # calculate number of bins by Sturge's rule\n","    # I take the floor of the value, you can also\n","    # just round it\n","    num_bins = int(np.floor(1 + np.log2(len(data))))\n","    \n","    # bin targets\n","    data.loc[:, \"bins_tg\"] = pd.cut(\n","        data[\"target\"], bins=num_bins, labels=False\n","    ).map(lambda x: str(x))\n","\n","    # bin standard_error\n","    data.loc[:, \"bins_std\"] = pd.cut(\n","        data[\"standard_error\"], bins=num_bins, labels=False\n","    )\n","\n","    # bins\n","    data.loc[:, \"bins\"] = data['bins_tg'].map(lambda x: str(x)) + data['bins_std'].map(lambda x: str(x))\n","\n","    # initiate the kfold class from model_selection module\n","    kf = StratifiedKFold(n_splits=num_splits, random_state=SEED, shuffle=True)\n","\n","    # note that, instead of targets, we use bins!\n","    if return_df:\n","      for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n","        data.loc[v_, 'kfold'] = f\n","      return data\n","    else:\n","      return kf.split(X=data, y=data.bins.values)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PLKHwvKtNBn","executionInfo":{"status":"ok","timestamp":1627236077320,"user_tz":-540,"elapsed":10,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["def train_and_save_model(train_indices, val_indices, model_path):\n","    train_dataset = LitDataset(train_df.loc[train_indices]) # train, validのDataset\n","    val_dataset = LitDataset(train_df.loc[val_indices])\n","        \n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              drop_last=True, shuffle=True, num_workers=2) # train, validのDataLoader\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                            drop_last=False, shuffle=False, num_workers=2)    \n","\n","    model = LitModel().to(DEVICE) # modelをDEVICEへぶち込む\n","    optimizer = create_optimizer(model) # optimizerをモデルから作成\n","    scheduler = get_cosine_schedule_with_warmup( # schedulerを作成\n","        optimizer,\n","        num_training_steps=NUM_EPOCHS * len(train_loader),\n","        num_warmup_steps=50)    \n","    rmse = train(model, model_path, train_loader, val_loader, optimizer, scheduler=scheduler)\n","\n","    del train_dataset\n","    del val_dataset\n","    del train_loader\n","    del val_loader\n","    del model\n","    del optimizer\n","    del scheduler\n","    gc.collect() \n","    torch.cuda.empty_cache()\n","    return rmse"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:26:40.755813Z","iopub.execute_input":"2021-07-04T06:26:40.758373Z","iopub.status.idle":"2021-07-04T06:27:12.493221Z","shell.execute_reply.started":"2021-07-04T06:26:40.758265Z","shell.execute_reply":"2021-07-04T06:27:12.490139Z"},"trusted":true,"id":"k2LGJD3XXecK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627253712266,"user_tz":-540,"elapsed":17634955,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"7117fb9d-a5dd-4236-a00a-c15c66ae3853"},"source":["# 実行処理。 KFold & 学習\n","SEED = 1000\n","list_val_rmse = []\n","\n","set_random_seed(SEED)\n","kfold = create_folds(train_df, NUM_FOLDS, SEED=SEED, return_df=False) # binsで切る場合\n","\n","for fold, (train_indices, val_indices) in enumerate(kfold):    \n","    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n","    print(gpuinfo())\n","    model_path = f\"model_{fold + 1}.pth\" # model_fold数_.pth\n","    set_random_seed(SEED + fold) # SEEDはfold別に変わるようにする\n","    list_val_rmse.append(train_and_save_model(train_indices, val_indices, model_path))\n","\n","    print(\"\\nPerformance estimates:\")\n","    print(list_val_rmse)\n","    print(\"Mean:\", np.array(list_val_rmse).mean())\n","    print(gpuinfo())"],"execution_count":22,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n","  % (min_groups, self.n_splits)), UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Fold 1/5\n","{'total_MiB': 16280, 'used_MiB': 2}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 72.8 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 1.159 train_rmse_stderror: 0.0 train_kl_div: 0.8463\n","val_rmse_target: 1.211 val_rmse_stderror: 3.217\n","New best_val_rmse: 1.211\n","\n","64 steps took 71.8 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.7985 train_rmse_stderror: 0.0 train_kl_div: 0.4679\n","val_rmse_target: 1.116 val_rmse_stderror: 2.59\n","New best_val_rmse: 1.116\n","\n","64 steps took 71.8 seconds\n","Epoch: 0 batch_num: 192\n","train_rmse_target: 0.7539 train_rmse_stderror: 0.0 train_kl_div: 0.2347\n","val_rmse_target: 1.036 val_rmse_stderror: 2.303\n","New best_val_rmse: 1.036\n","\n","64 steps took 71.8 seconds\n","Epoch: 0 batch_num: 256\n","train_rmse_target: 0.8315 train_rmse_stderror: 0.0 train_kl_div: 0.2171\n","val_rmse_target: 0.8661 val_rmse_stderror: 2.222\n","New best_val_rmse: 0.8661\n","\n","64 steps took 72.0 seconds\n","Epoch: 1 batch_num: 37\n","train_rmse_target: 0.7237 train_rmse_stderror: 0.0 train_kl_div: 0.2056\n","val_rmse_target: 0.7146 val_rmse_stderror: 2.167\n","New best_val_rmse: 0.7146\n","\n","64 steps took 71.7 seconds\n","Epoch: 1 batch_num: 101\n","train_rmse_target: 0.4007 train_rmse_stderror: 0.0 train_kl_div: -0.2763\n","val_rmse_target: 0.7741 val_rmse_stderror: 1.962\n","Still best_val_rmse: 0.7146 (from epoch 1)\n","\n","64 steps took 71.7 seconds\n","Epoch: 1 batch_num: 165\n","train_rmse_target: 0.5423 train_rmse_stderror: 0.0 train_kl_div: -0.1061\n","val_rmse_target: 0.6003 val_rmse_stderror: 2.022\n","New best_val_rmse: 0.6003\n","\n","64 steps took 71.8 seconds\n","Epoch: 1 batch_num: 229\n","train_rmse_target: 0.4932 train_rmse_stderror: 0.0 train_kl_div: -0.09913\n","val_rmse_target: 0.5864 val_rmse_stderror: 2.023\n","New best_val_rmse: 0.5864\n","\n","64 steps took 72.0 seconds\n","Epoch: 2 batch_num: 10\n","train_rmse_target: 0.3212 train_rmse_stderror: 0.0 train_kl_div: -0.2894\n","val_rmse_target: 0.5387 val_rmse_stderror: 2.016\n","New best_val_rmse: 0.5387\n","\n","32 steps took 35.9 seconds\n","Epoch: 2 batch_num: 42\n","train_rmse_target: 0.6082 train_rmse_stderror: 0.0 train_kl_div: 0.0781\n","val_rmse_target: 0.5762 val_rmse_stderror: 1.922\n","Still best_val_rmse: 0.5387 (from epoch 2)\n","\n","64 steps took 71.7 seconds\n","Epoch: 2 batch_num: 106\n","train_rmse_target: 0.398 train_rmse_stderror: 0.0 train_kl_div: -0.401\n","val_rmse_target: 0.5377 val_rmse_stderror: 1.807\n","New best_val_rmse: 0.5377\n","\n","32 steps took 35.9 seconds\n","Epoch: 2 batch_num: 138\n","train_rmse_target: 0.5109 train_rmse_stderror: 0.0 train_kl_div: -0.2288\n","val_rmse_target: 0.5553 val_rmse_stderror: 1.966\n","Still best_val_rmse: 0.5377 (from epoch 2)\n","\n","64 steps took 71.7 seconds\n","Epoch: 2 batch_num: 202\n","train_rmse_target: 0.2992 train_rmse_stderror: 0.0 train_kl_div: -0.4537\n","val_rmse_target: 0.5395 val_rmse_stderror: 1.912\n","Still best_val_rmse: 0.5377 (from epoch 2)\n","\n","32 steps took 35.9 seconds\n","Epoch: 2 batch_num: 234\n","train_rmse_target: 0.2515 train_rmse_stderror: 0.0 train_kl_div: -0.5772\n","val_rmse_target: 0.5321 val_rmse_stderror: 1.866\n","New best_val_rmse: 0.5321\n","\n","32 steps took 35.9 seconds\n","Epoch: 2 batch_num: 266\n","train_rmse_target: 0.4616 train_rmse_stderror: 0.0 train_kl_div: -0.1662\n","val_rmse_target: 0.5077 val_rmse_stderror: 1.882\n","New best_val_rmse: 0.5077\n","\n","32 steps took 36.1 seconds\n","Epoch: 3 batch_num: 15\n","train_rmse_target: 0.398 train_rmse_stderror: 0.0 train_kl_div: -0.45\n","val_rmse_target: 0.5149 val_rmse_stderror: 1.75\n","Still best_val_rmse: 0.5077 (from epoch 2)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 47\n","train_rmse_target: 0.3165 train_rmse_stderror: 0.0 train_kl_div: -0.6265\n","val_rmse_target: 0.5049 val_rmse_stderror: 1.813\n","New best_val_rmse: 0.5049\n","\n","32 steps took 35.9 seconds\n","Epoch: 3 batch_num: 79\n","train_rmse_target: 0.341 train_rmse_stderror: 0.0 train_kl_div: -0.6703\n","val_rmse_target: 0.5259 val_rmse_stderror: 1.782\n","Still best_val_rmse: 0.5049 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 111\n","train_rmse_target: 0.2644 train_rmse_stderror: 0.0 train_kl_div: -0.7408\n","val_rmse_target: 0.5186 val_rmse_stderror: 1.794\n","Still best_val_rmse: 0.5049 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 143\n","train_rmse_target: 0.483 train_rmse_stderror: 0.0 train_kl_div: -0.06512\n","val_rmse_target: 0.5116 val_rmse_stderror: 1.747\n","Still best_val_rmse: 0.5049 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 175\n","train_rmse_target: 0.3599 train_rmse_stderror: 0.0 train_kl_div: -0.526\n","val_rmse_target: 0.4958 val_rmse_stderror: 1.774\n","New best_val_rmse: 0.4958\n","\n","32 steps took 35.9 seconds\n","Epoch: 3 batch_num: 207\n","train_rmse_target: 0.439 train_rmse_stderror: 0.0 train_kl_div: -0.3641\n","val_rmse_target: 0.536 val_rmse_stderror: 1.812\n","Still best_val_rmse: 0.4958 (from epoch 3)\n","\n","32 steps took 35.9 seconds\n","Epoch: 3 batch_num: 239\n","train_rmse_target: 0.2873 train_rmse_stderror: 0.0 train_kl_div: -0.6061\n","val_rmse_target: 0.6136 val_rmse_stderror: 1.826\n","Still best_val_rmse: 0.4958 (from epoch 3)\n","\n","64 steps took 71.9 seconds\n","Epoch: 4 batch_num: 20\n","train_rmse_target: 0.2773 train_rmse_stderror: 0.0 train_kl_div: -0.7866\n","val_rmse_target: 0.5807 val_rmse_stderror: 1.664\n","Still best_val_rmse: 0.4958 (from epoch 3)\n","\n","64 steps took 71.7 seconds\n","Epoch: 4 batch_num: 84\n","train_rmse_target: 0.1895 train_rmse_stderror: 0.0 train_kl_div: -1.028\n","val_rmse_target: 0.4895 val_rmse_stderror: 1.705\n","New best_val_rmse: 0.4895\n","\n","32 steps took 35.9 seconds\n","Epoch: 4 batch_num: 116\n","train_rmse_target: 0.2415 train_rmse_stderror: 0.0 train_kl_div: -0.7938\n","val_rmse_target: 0.5459 val_rmse_stderror: 1.815\n","Still best_val_rmse: 0.4895 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 148\n","train_rmse_target: 0.2929 train_rmse_stderror: 0.0 train_kl_div: -0.849\n","val_rmse_target: 0.5133 val_rmse_stderror: 1.69\n","Still best_val_rmse: 0.4895 (from epoch 4)\n","\n","32 steps took 35.9 seconds\n","Epoch: 4 batch_num: 180\n","train_rmse_target: 0.2409 train_rmse_stderror: 0.0 train_kl_div: -0.9478\n","val_rmse_target: 0.4917 val_rmse_stderror: 1.688\n","Still best_val_rmse: 0.4895 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 212\n","train_rmse_target: 0.2802 train_rmse_stderror: 0.0 train_kl_div: -0.6995\n","val_rmse_target: 0.4834 val_rmse_stderror: 1.704\n","New best_val_rmse: 0.4834\n","\n","32 steps took 35.9 seconds\n","Epoch: 4 batch_num: 244\n","train_rmse_target: 0.1164 train_rmse_stderror: 0.0 train_kl_div: -1.011\n","val_rmse_target: 0.518 val_rmse_stderror: 1.737\n","Still best_val_rmse: 0.4834 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 276\n","train_rmse_target: 0.2385 train_rmse_stderror: 0.0 train_kl_div: -0.8472\n","val_rmse_target: 0.4955 val_rmse_stderror: 1.734\n","Still best_val_rmse: 0.4834 (from epoch 4)\n","\n","32 steps took 36.0 seconds\n","Epoch: 5 batch_num: 25\n","train_rmse_target: 0.3461 train_rmse_stderror: 0.0 train_kl_div: -0.7242\n","val_rmse_target: 0.4993 val_rmse_stderror: 1.691\n","Still best_val_rmse: 0.4834 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 57\n","train_rmse_target: 0.3377 train_rmse_stderror: 0.0 train_kl_div: -0.5761\n","val_rmse_target: 0.5612 val_rmse_stderror: 1.737\n","Still best_val_rmse: 0.4834 (from epoch 4)\n","\n","64 steps took 71.7 seconds\n","Epoch: 5 batch_num: 121\n","train_rmse_target: 0.2225 train_rmse_stderror: 0.0 train_kl_div: -0.9471\n","val_rmse_target: 0.5039 val_rmse_stderror: 1.725\n","Still best_val_rmse: 0.4834 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 153\n","train_rmse_target: 0.2179 train_rmse_stderror: 0.0 train_kl_div: -0.9013\n","val_rmse_target: 0.4817 val_rmse_stderror: 1.739\n","New best_val_rmse: 0.4817\n","\n","32 steps took 35.9 seconds\n","Epoch: 5 batch_num: 185\n","train_rmse_target: 0.1676 train_rmse_stderror: 0.0 train_kl_div: -1.295\n","val_rmse_target: 0.4866 val_rmse_stderror: 1.627\n","Still best_val_rmse: 0.4817 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 217\n","train_rmse_target: 0.2311 train_rmse_stderror: 0.0 train_kl_div: -0.979\n","val_rmse_target: 0.5032 val_rmse_stderror: 1.652\n","Still best_val_rmse: 0.4817 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 249\n","train_rmse_target: 0.226 train_rmse_stderror: 0.0 train_kl_div: -1.043\n","val_rmse_target: 0.5084 val_rmse_stderror: 1.648\n","Still best_val_rmse: 0.4817 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 281\n","train_rmse_target: 0.2197 train_rmse_stderror: 0.0 train_kl_div: -1.233\n","val_rmse_target: 0.4865 val_rmse_stderror: 1.613\n","Still best_val_rmse: 0.4817 (from epoch 5)\n","\n","32 steps took 36.1 seconds\n","Epoch: 6 batch_num: 30\n","train_rmse_target: 0.1985 train_rmse_stderror: 0.0 train_kl_div: -0.2377\n","val_rmse_target: 0.4806 val_rmse_stderror: 1.583\n","New best_val_rmse: 0.4806\n","\n","32 steps took 35.9 seconds\n","Epoch: 6 batch_num: 62\n","train_rmse_target: 0.1258 train_rmse_stderror: 0.0 train_kl_div: -1.425\n","val_rmse_target: 0.4837 val_rmse_stderror: 1.65\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 94\n","train_rmse_target: 0.1635 train_rmse_stderror: 0.0 train_kl_div: -1.265\n","val_rmse_target: 0.4892 val_rmse_stderror: 1.643\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 126\n","train_rmse_target: 0.1467 train_rmse_stderror: 0.0 train_kl_div: -1.452\n","val_rmse_target: 0.4955 val_rmse_stderror: 1.6\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 158\n","train_rmse_target: 0.1086 train_rmse_stderror: 0.0 train_kl_div: -1.471\n","val_rmse_target: 0.4976 val_rmse_stderror: 1.618\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 190\n","train_rmse_target: 0.1008 train_rmse_stderror: 0.0 train_kl_div: -1.647\n","val_rmse_target: 0.4883 val_rmse_stderror: 1.588\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 222\n","train_rmse_target: 0.1419 train_rmse_stderror: 0.0 train_kl_div: -1.366\n","val_rmse_target: 0.4886 val_rmse_stderror: 1.598\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 254\n","train_rmse_target: 0.08435 train_rmse_stderror: 0.0 train_kl_div: -1.8\n","val_rmse_target: 0.4862 val_rmse_stderror: 1.596\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 36.0 seconds\n","Epoch: 7 batch_num: 3\n","train_rmse_target: 0.07101 train_rmse_stderror: 0.0 train_kl_div: -1.963\n","val_rmse_target: 0.4855 val_rmse_stderror: 1.595\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 35\n","train_rmse_target: 0.1407 train_rmse_stderror: 0.0 train_kl_div: -1.419\n","val_rmse_target: 0.4839 val_rmse_stderror: 1.557\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 67\n","train_rmse_target: 0.1511 train_rmse_stderror: 0.0 train_kl_div: -1.16\n","val_rmse_target: 0.4906 val_rmse_stderror: 1.56\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 99\n","train_rmse_target: 0.07371 train_rmse_stderror: 0.0 train_kl_div: -2.024\n","val_rmse_target: 0.4858 val_rmse_stderror: 1.559\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.9 seconds\n","Epoch: 7 batch_num: 131\n","train_rmse_target: 0.2276 train_rmse_stderror: 0.0 train_kl_div: 0.01317\n","val_rmse_target: 0.4952 val_rmse_stderror: 1.563\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 163\n","train_rmse_target: 0.1856 train_rmse_stderror: 0.0 train_kl_div: -1.272\n","val_rmse_target: 0.4844 val_rmse_stderror: 1.568\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 195\n","train_rmse_target: 0.0984 train_rmse_stderror: 0.0 train_kl_div: -1.623\n","val_rmse_target: 0.4864 val_rmse_stderror: 1.571\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 227\n","train_rmse_target: 0.09663 train_rmse_stderror: 0.0 train_kl_div: -1.677\n","val_rmse_target: 0.4841 val_rmse_stderror: 1.568\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 259\n","train_rmse_target: 0.09436 train_rmse_stderror: 0.0 train_kl_div: -1.84\n","val_rmse_target: 0.4854 val_rmse_stderror: 1.567\n","Still best_val_rmse: 0.4806 (from epoch 6)\n","\n","Performance estimates:\n","[0.4806371926256924]\n","Mean: 0.4806371926256924\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 2/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 72.5 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 1.009 train_rmse_stderror: 2.21 train_kl_div: 0.5084\n","val_rmse_target: 0.9098 val_rmse_stderror: 0.9929\n","New best_val_rmse: 0.9098\n","\n","64 steps took 71.7 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.7558 train_rmse_stderror: 2.343 train_kl_div: 0.3723\n","val_rmse_target: 0.6756 val_rmse_stderror: 0.956\n","New best_val_rmse: 0.6756\n","\n","64 steps took 71.6 seconds\n","Epoch: 0 batch_num: 192\n","train_rmse_target: 0.3471 train_rmse_stderror: 1.652 train_kl_div: -0.1112\n","val_rmse_target: 0.6438 val_rmse_stderror: 0.9626\n","New best_val_rmse: 0.6438\n","\n","64 steps took 71.7 seconds\n","Epoch: 0 batch_num: 256\n","train_rmse_target: 1.405 train_rmse_stderror: 4.461 train_kl_div: 0.9919\n","val_rmse_target: 1.201 val_rmse_stderror: 1.645\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.9 seconds\n","Epoch: 1 batch_num: 37\n","train_rmse_target: 1.885 train_rmse_stderror: 5.953 train_kl_div: 1.291\n","val_rmse_target: 1.611 val_rmse_stderror: 2.32\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 1 batch_num: 101\n","train_rmse_target: 1.917 train_rmse_stderror: 6.28 train_kl_div: 1.328\n","val_rmse_target: 1.698 val_rmse_stderror: 2.452\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.7 seconds\n","Epoch: 1 batch_num: 165\n","train_rmse_target: 1.169 train_rmse_stderror: 6.049 train_kl_div: 1.182\n","val_rmse_target: 1.511 val_rmse_stderror: 2.33\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.7 seconds\n","Epoch: 1 batch_num: 229\n","train_rmse_target: 1.323 train_rmse_stderror: 5.949 train_kl_div: 1.189\n","val_rmse_target: 1.402 val_rmse_stderror: 2.295\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.9 seconds\n","Epoch: 2 batch_num: 10\n","train_rmse_target: 1.146 train_rmse_stderror: 5.936 train_kl_div: 1.162\n","val_rmse_target: 1.315 val_rmse_stderror: 2.277\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.7 seconds\n","Epoch: 2 batch_num: 74\n","train_rmse_target: 1.101 train_rmse_stderror: 5.923 train_kl_div: 1.154\n","val_rmse_target: 1.262 val_rmse_stderror: 2.266\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 2 batch_num: 138\n","train_rmse_target: 1.013 train_rmse_stderror: 5.91 train_kl_div: 1.142\n","val_rmse_target: 1.221 val_rmse_stderror: 2.254\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 2 batch_num: 202\n","train_rmse_target: 1.543 train_rmse_stderror: 5.849 train_kl_div: 1.212\n","val_rmse_target: 1.181 val_rmse_stderror: 2.239\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 2 batch_num: 266\n","train_rmse_target: 1.739 train_rmse_stderror: 5.837 train_kl_div: 1.249\n","val_rmse_target: 1.149 val_rmse_stderror: 2.22\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.9 seconds\n","Epoch: 3 batch_num: 47\n","train_rmse_target: 0.9557 train_rmse_stderror: 5.751 train_kl_div: 1.112\n","val_rmse_target: 1.106 val_rmse_stderror: 2.196\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 3 batch_num: 111\n","train_rmse_target: 1.179 train_rmse_stderror: 5.695 train_kl_div: 1.131\n","val_rmse_target: 1.068 val_rmse_stderror: 2.166\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 3 batch_num: 175\n","train_rmse_target: 0.5965 train_rmse_stderror: 5.517 train_kl_div: 1.038\n","val_rmse_target: 1.033 val_rmse_stderror: 2.119\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 3 batch_num: 239\n","train_rmse_target: 1.381 train_rmse_stderror: 5.238 train_kl_div: 1.102\n","val_rmse_target: 1.039 val_rmse_stderror: 2.041\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.9 seconds\n","Epoch: 4 batch_num: 20\n","train_rmse_target: 0.9896 train_rmse_stderror: 5.437 train_kl_div: 1.066\n","val_rmse_target: 1.054 val_rmse_stderror: 2.068\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 4 batch_num: 84\n","train_rmse_target: 1.178 train_rmse_stderror: 5.022 train_kl_div: 1.028\n","val_rmse_target: 1.06 val_rmse_stderror: 1.952\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 4 batch_num: 148\n","train_rmse_target: 1.568 train_rmse_stderror: 4.949 train_kl_div: 1.106\n","val_rmse_target: 1.135 val_rmse_stderror: 1.873\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 4 batch_num: 212\n","train_rmse_target: 1.144 train_rmse_stderror: 4.594 train_kl_div: 0.9654\n","val_rmse_target: 1.406 val_rmse_stderror: 1.657\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 4 batch_num: 276\n","train_rmse_target: 1.128 train_rmse_stderror: 4.593 train_kl_div: 0.9414\n","val_rmse_target: 1.309 val_rmse_stderror: 1.58\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.9 seconds\n","Epoch: 5 batch_num: 57\n","train_rmse_target: 0.8922 train_rmse_stderror: 4.84 train_kl_div: 0.952\n","val_rmse_target: 1.223 val_rmse_stderror: 1.614\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 5 batch_num: 121\n","train_rmse_target: 1.561 train_rmse_stderror: 5.742 train_kl_div: 1.202\n","val_rmse_target: 1.283 val_rmse_stderror: 2.189\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.7 seconds\n","Epoch: 5 batch_num: 185\n","train_rmse_target: 1.211 train_rmse_stderror: 5.676 train_kl_div: 1.133\n","val_rmse_target: 1.243 val_rmse_stderror: 2.168\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 5 batch_num: 249\n","train_rmse_target: 1.463 train_rmse_stderror: 5.663 train_kl_div: 1.174\n","val_rmse_target: 1.216 val_rmse_stderror: 2.154\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.9 seconds\n","Epoch: 6 batch_num: 30\n","train_rmse_target: 1.253 train_rmse_stderror: 5.62 train_kl_div: 1.133\n","val_rmse_target: 1.197 val_rmse_stderror: 2.145\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 6 batch_num: 94\n","train_rmse_target: 1.229 train_rmse_stderror: 5.624 train_kl_div: 1.129\n","val_rmse_target: 1.186 val_rmse_stderror: 2.139\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 6 batch_num: 158\n","train_rmse_target: 1.181 train_rmse_stderror: 5.599 train_kl_div: 1.118\n","val_rmse_target: 1.177 val_rmse_stderror: 2.134\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 6 batch_num: 222\n","train_rmse_target: 1.314 train_rmse_stderror: 5.617 train_kl_div: 1.141\n","val_rmse_target: 1.171 val_rmse_stderror: 2.132\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.9 seconds\n","Epoch: 7 batch_num: 3\n","train_rmse_target: 1.053 train_rmse_stderror: 5.598 train_kl_div: 1.099\n","val_rmse_target: 1.168 val_rmse_stderror: 2.13\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 7 batch_num: 67\n","train_rmse_target: 0.8261 train_rmse_stderror: 5.584 train_kl_div: 1.071\n","val_rmse_target: 1.166 val_rmse_stderror: 2.129\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 7 batch_num: 131\n","train_rmse_target: 0.7579 train_rmse_stderror: 5.6 train_kl_div: 1.066\n","val_rmse_target: 1.165 val_rmse_stderror: 2.128\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 7 batch_num: 195\n","train_rmse_target: 1.079 train_rmse_stderror: 5.602 train_kl_div: 1.104\n","val_rmse_target: 1.164 val_rmse_stderror: 2.128\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 7 batch_num: 259\n","train_rmse_target: 0.974 train_rmse_stderror: 5.596 train_kl_div: 1.09\n","val_rmse_target: 1.164 val_rmse_stderror: 2.128\n","Still best_val_rmse: 0.6438 (from epoch 0)\n","\n","Performance estimates:\n","[0.4806371926256924, 0.6437857646853802]\n","Mean: 0.5622114786555363\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 3/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 72.5 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 1.211 train_rmse_stderror: 0.0 train_kl_div: 0.6875\n","val_rmse_target: 1.007 val_rmse_stderror: 2.425\n","New best_val_rmse: 1.007\n","\n","64 steps took 71.6 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.6009 train_rmse_stderror: 0.0 train_kl_div: 0.02684\n","val_rmse_target: 1.036 val_rmse_stderror: 1.928\n","Still best_val_rmse: 1.007 (from epoch 0)\n","\n","64 steps took 72.1 seconds\n","Epoch: 0 batch_num: 192\n","train_rmse_target: 0.5558 train_rmse_stderror: 0.0 train_kl_div: -0.04053\n","val_rmse_target: 0.7149 val_rmse_stderror: 1.907\n","New best_val_rmse: 0.7149\n","\n","64 steps took 71.7 seconds\n","Epoch: 0 batch_num: 256\n","train_rmse_target: 0.5628 train_rmse_stderror: 0.0 train_kl_div: -0.02533\n","val_rmse_target: 0.6877 val_rmse_stderror: 1.997\n","New best_val_rmse: 0.6877\n","\n","64 steps took 71.9 seconds\n","Epoch: 1 batch_num: 37\n","train_rmse_target: 0.5272 train_rmse_stderror: 0.0 train_kl_div: -0.1555\n","val_rmse_target: 0.6016 val_rmse_stderror: 1.87\n","New best_val_rmse: 0.6016\n","\n","64 steps took 71.6 seconds\n","Epoch: 1 batch_num: 101\n","train_rmse_target: 0.6521 train_rmse_stderror: 0.0 train_kl_div: 0.107\n","val_rmse_target: 0.5424 val_rmse_stderror: 1.95\n","New best_val_rmse: 0.5424\n","\n","32 steps took 35.8 seconds\n","Epoch: 1 batch_num: 133\n","train_rmse_target: 0.4356 train_rmse_stderror: 0.0 train_kl_div: -0.2491\n","val_rmse_target: 0.5647 val_rmse_stderror: 1.843\n","Still best_val_rmse: 0.5424 (from epoch 1)\n","\n","64 steps took 71.6 seconds\n","Epoch: 1 batch_num: 197\n","train_rmse_target: 0.6206 train_rmse_stderror: 0.0 train_kl_div: 0.0135\n","val_rmse_target: 0.5218 val_rmse_stderror: 1.906\n","New best_val_rmse: 0.5218\n","\n","32 steps took 35.8 seconds\n","Epoch: 1 batch_num: 229\n","train_rmse_target: 0.3432 train_rmse_stderror: 0.0 train_kl_div: -0.3797\n","val_rmse_target: 0.5793 val_rmse_stderror: 1.838\n","Still best_val_rmse: 0.5218 (from epoch 1)\n","\n","64 steps took 71.8 seconds\n","Epoch: 2 batch_num: 10\n","train_rmse_target: 0.6018 train_rmse_stderror: 0.0 train_kl_div: 0.2787\n","val_rmse_target: 0.5269 val_rmse_stderror: 1.71\n","Still best_val_rmse: 0.5218 (from epoch 1)\n","\n","32 steps took 35.8 seconds\n","Epoch: 2 batch_num: 42\n","train_rmse_target: 0.4869 train_rmse_stderror: 0.0 train_kl_div: -0.1851\n","val_rmse_target: 0.5408 val_rmse_stderror: 1.815\n","Still best_val_rmse: 0.5218 (from epoch 1)\n","\n","32 steps took 35.8 seconds\n","Epoch: 2 batch_num: 74\n","train_rmse_target: 0.4393 train_rmse_stderror: 0.0 train_kl_div: -0.3543\n","val_rmse_target: 0.5168 val_rmse_stderror: 1.706\n","New best_val_rmse: 0.5168\n","\n","32 steps took 35.8 seconds\n","Epoch: 2 batch_num: 106\n","train_rmse_target: 0.5317 train_rmse_stderror: 0.0 train_kl_div: -0.002467\n","val_rmse_target: 0.5335 val_rmse_stderror: 1.866\n","Still best_val_rmse: 0.5168 (from epoch 2)\n","\n","32 steps took 35.8 seconds\n","Epoch: 2 batch_num: 138\n","train_rmse_target: 0.4979 train_rmse_stderror: 0.0 train_kl_div: -0.1859\n","val_rmse_target: 0.5498 val_rmse_stderror: 1.758\n","Still best_val_rmse: 0.5168 (from epoch 2)\n","\n","32 steps took 35.8 seconds\n","Epoch: 2 batch_num: 170\n","train_rmse_target: 0.511 train_rmse_stderror: 0.0 train_kl_div: -0.1644\n","val_rmse_target: 0.5381 val_rmse_stderror: 1.829\n","Still best_val_rmse: 0.5168 (from epoch 2)\n","\n","32 steps took 35.8 seconds\n","Epoch: 2 batch_num: 202\n","train_rmse_target: 0.4162 train_rmse_stderror: 0.0 train_kl_div: -0.3595\n","val_rmse_target: 0.5587 val_rmse_stderror: 1.687\n","Still best_val_rmse: 0.5168 (from epoch 2)\n","\n","64 steps took 71.6 seconds\n","Epoch: 2 batch_num: 266\n","train_rmse_target: 0.4487 train_rmse_stderror: 0.0 train_kl_div: -0.1771\n","val_rmse_target: 0.5162 val_rmse_stderror: 1.901\n","New best_val_rmse: 0.5162\n","\n","32 steps took 36.0 seconds\n","Epoch: 3 batch_num: 15\n","train_rmse_target: 0.3127 train_rmse_stderror: 0.0 train_kl_div: -0.6037\n","val_rmse_target: 0.5403 val_rmse_stderror: 1.622\n","Still best_val_rmse: 0.5162 (from epoch 2)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 47\n","train_rmse_target: 0.248 train_rmse_stderror: 0.0 train_kl_div: -0.8124\n","val_rmse_target: 0.4997 val_rmse_stderror: 1.646\n","New best_val_rmse: 0.4997\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 79\n","train_rmse_target: 0.3791 train_rmse_stderror: 0.0 train_kl_div: -0.3364\n","val_rmse_target: 0.5372 val_rmse_stderror: 1.708\n","Still best_val_rmse: 0.4997 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 111\n","train_rmse_target: 0.3134 train_rmse_stderror: 0.0 train_kl_div: -0.6138\n","val_rmse_target: 0.5156 val_rmse_stderror: 1.655\n","Still best_val_rmse: 0.4997 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 143\n","train_rmse_target: 0.4395 train_rmse_stderror: 0.0 train_kl_div: -0.1979\n","val_rmse_target: 0.5119 val_rmse_stderror: 1.615\n","Still best_val_rmse: 0.4997 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 175\n","train_rmse_target: 0.2996 train_rmse_stderror: 0.0 train_kl_div: -0.4791\n","val_rmse_target: 0.5268 val_rmse_stderror: 1.768\n","Still best_val_rmse: 0.4997 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 207\n","train_rmse_target: 0.4898 train_rmse_stderror: 0.0 train_kl_div: -0.1527\n","val_rmse_target: 0.5328 val_rmse_stderror: 1.709\n","Still best_val_rmse: 0.4997 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 239\n","train_rmse_target: 0.4059 train_rmse_stderror: 0.0 train_kl_div: -0.3139\n","val_rmse_target: 0.5432 val_rmse_stderror: 1.851\n","Still best_val_rmse: 0.4997 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 271\n","train_rmse_target: 0.3792 train_rmse_stderror: 0.0 train_kl_div: -0.2339\n","val_rmse_target: 0.493 val_rmse_stderror: 1.588\n","New best_val_rmse: 0.493\n","\n","32 steps took 36.0 seconds\n","Epoch: 4 batch_num: 20\n","train_rmse_target: 0.3417 train_rmse_stderror: 0.0 train_kl_div: -0.5775\n","val_rmse_target: 0.4997 val_rmse_stderror: 1.632\n","Still best_val_rmse: 0.493 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 52\n","train_rmse_target: 0.1813 train_rmse_stderror: 0.0 train_kl_div: -1.158\n","val_rmse_target: 0.5508 val_rmse_stderror: 1.535\n","Still best_val_rmse: 0.493 (from epoch 3)\n","\n","64 steps took 71.6 seconds\n","Epoch: 4 batch_num: 116\n","train_rmse_target: 0.3841 train_rmse_stderror: 0.0 train_kl_div: -0.5311\n","val_rmse_target: 0.4991 val_rmse_stderror: 1.624\n","Still best_val_rmse: 0.493 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 148\n","train_rmse_target: 0.2344 train_rmse_stderror: 0.0 train_kl_div: -0.9171\n","val_rmse_target: 0.4857 val_rmse_stderror: 1.612\n","New best_val_rmse: 0.4857\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 180\n","train_rmse_target: 0.2749 train_rmse_stderror: 0.0 train_kl_div: -0.7507\n","val_rmse_target: 0.5162 val_rmse_stderror: 1.63\n","Still best_val_rmse: 0.4857 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 212\n","train_rmse_target: 0.3179 train_rmse_stderror: 0.0 train_kl_div: -0.9034\n","val_rmse_target: 0.5165 val_rmse_stderror: 1.558\n","Still best_val_rmse: 0.4857 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 244\n","train_rmse_target: 0.2504 train_rmse_stderror: 0.0 train_kl_div: -0.8662\n","val_rmse_target: 0.4939 val_rmse_stderror: 1.627\n","Still best_val_rmse: 0.4857 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 276\n","train_rmse_target: 0.2927 train_rmse_stderror: 0.0 train_kl_div: -0.6882\n","val_rmse_target: 0.4885 val_rmse_stderror: 1.62\n","Still best_val_rmse: 0.4857 (from epoch 4)\n","\n","32 steps took 36.0 seconds\n","Epoch: 5 batch_num: 25\n","train_rmse_target: 0.1352 train_rmse_stderror: 0.0 train_kl_div: -1.362\n","val_rmse_target: 0.4968 val_rmse_stderror: 1.536\n","Still best_val_rmse: 0.4857 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 57\n","train_rmse_target: 0.1657 train_rmse_stderror: 0.0 train_kl_div: -0.9908\n","val_rmse_target: 0.4968 val_rmse_stderror: 1.621\n","Still best_val_rmse: 0.4857 (from epoch 4)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 89\n","train_rmse_target: 0.1806 train_rmse_stderror: 0.0 train_kl_div: -1.088\n","val_rmse_target: 0.4804 val_rmse_stderror: 1.571\n","New best_val_rmse: 0.4804\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 121\n","train_rmse_target: 0.2146 train_rmse_stderror: 0.0 train_kl_div: -0.9594\n","val_rmse_target: 0.4906 val_rmse_stderror: 1.583\n","Still best_val_rmse: 0.4804 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 153\n","train_rmse_target: 0.2739 train_rmse_stderror: 0.0 train_kl_div: -0.6657\n","val_rmse_target: 0.5188 val_rmse_stderror: 1.547\n","Still best_val_rmse: 0.4804 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 185\n","train_rmse_target: 0.2388 train_rmse_stderror: 0.0 train_kl_div: -0.9409\n","val_rmse_target: 0.4833 val_rmse_stderror: 1.51\n","Still best_val_rmse: 0.4804 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 217\n","train_rmse_target: 0.1436 train_rmse_stderror: 0.0 train_kl_div: -1.147\n","val_rmse_target: 0.4844 val_rmse_stderror: 1.586\n","Still best_val_rmse: 0.4804 (from epoch 5)\n","\n","32 steps took 35.7 seconds\n","Epoch: 5 batch_num: 249\n","train_rmse_target: 0.1493 train_rmse_stderror: 0.0 train_kl_div: -1.343\n","val_rmse_target: 0.4953 val_rmse_stderror: 1.52\n","Still best_val_rmse: 0.4804 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 281\n","train_rmse_target: 0.1959 train_rmse_stderror: 0.0 train_kl_div: -1.334\n","val_rmse_target: 0.4908 val_rmse_stderror: 1.499\n","Still best_val_rmse: 0.4804 (from epoch 5)\n","\n","32 steps took 35.9 seconds\n","Epoch: 6 batch_num: 30\n","train_rmse_target: 0.07688 train_rmse_stderror: 0.0 train_kl_div: -1.617\n","val_rmse_target: 0.4928 val_rmse_stderror: 1.5\n","Still best_val_rmse: 0.4804 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 62\n","train_rmse_target: 0.232 train_rmse_stderror: 0.0 train_kl_div: -1.071\n","val_rmse_target: 0.4869 val_rmse_stderror: 1.48\n","Still best_val_rmse: 0.4804 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 94\n","train_rmse_target: 0.1509 train_rmse_stderror: 0.0 train_kl_div: -1.362\n","val_rmse_target: 0.4804 val_rmse_stderror: 1.502\n","New best_val_rmse: 0.4804\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 126\n","train_rmse_target: 0.09136 train_rmse_stderror: 0.0 train_kl_div: -1.864\n","val_rmse_target: 0.4924 val_rmse_stderror: 1.451\n","Still best_val_rmse: 0.4804 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 158\n","train_rmse_target: 0.1384 train_rmse_stderror: 0.0 train_kl_div: -1.474\n","val_rmse_target: 0.4954 val_rmse_stderror: 1.464\n","Still best_val_rmse: 0.4804 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 190\n","train_rmse_target: 0.1008 train_rmse_stderror: 0.0 train_kl_div: -1.799\n","val_rmse_target: 0.481 val_rmse_stderror: 1.459\n","Still best_val_rmse: 0.4804 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 222\n","train_rmse_target: 0.09955 train_rmse_stderror: 0.0 train_kl_div: -1.817\n","val_rmse_target: 0.4814 val_rmse_stderror: 1.452\n","Still best_val_rmse: 0.4804 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 254\n","train_rmse_target: 0.1165 train_rmse_stderror: 0.0 train_kl_div: -1.638\n","val_rmse_target: 0.4972 val_rmse_stderror: 1.478\n","Still best_val_rmse: 0.4804 (from epoch 6)\n","\n","32 steps took 36.0 seconds\n","Epoch: 7 batch_num: 3\n","train_rmse_target: 0.1098 train_rmse_stderror: 0.0 train_kl_div: -1.709\n","val_rmse_target: 0.478 val_rmse_stderror: 1.454\n","New best_val_rmse: 0.478\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 35\n","train_rmse_target: 0.0835 train_rmse_stderror: 0.0 train_kl_div: -2.007\n","val_rmse_target: 0.478 val_rmse_stderror: 1.436\n","New best_val_rmse: 0.478\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 67\n","train_rmse_target: 0.06637 train_rmse_stderror: 0.0 train_kl_div: -2.216\n","val_rmse_target: 0.4827 val_rmse_stderror: 1.433\n","Still best_val_rmse: 0.478 (from epoch 7)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 99\n","train_rmse_target: 0.03946 train_rmse_stderror: 0.0 train_kl_div: -2.312\n","val_rmse_target: 0.4825 val_rmse_stderror: 1.445\n","Still best_val_rmse: 0.478 (from epoch 7)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 131\n","train_rmse_target: 0.09146 train_rmse_stderror: 0.0 train_kl_div: -1.745\n","val_rmse_target: 0.4834 val_rmse_stderror: 1.432\n","Still best_val_rmse: 0.478 (from epoch 7)\n","\n","32 steps took 35.7 seconds\n","Epoch: 7 batch_num: 163\n","train_rmse_target: 0.05869 train_rmse_stderror: 0.0 train_kl_div: -2.251\n","val_rmse_target: 0.4846 val_rmse_stderror: 1.432\n","Still best_val_rmse: 0.478 (from epoch 7)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 195\n","train_rmse_target: 0.06888 train_rmse_stderror: 0.0 train_kl_div: -2.142\n","val_rmse_target: 0.4809 val_rmse_stderror: 1.432\n","Still best_val_rmse: 0.478 (from epoch 7)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 227\n","train_rmse_target: 0.1263 train_rmse_stderror: 0.0 train_kl_div: -1.891\n","val_rmse_target: 0.4798 val_rmse_stderror: 1.434\n","Still best_val_rmse: 0.478 (from epoch 7)\n","\n","32 steps took 35.7 seconds\n","Epoch: 7 batch_num: 259\n","train_rmse_target: 0.06587 train_rmse_stderror: 0.0 train_kl_div: -2.163\n","val_rmse_target: 0.4804 val_rmse_stderror: 1.433\n","Still best_val_rmse: 0.478 (from epoch 7)\n","\n","Performance estimates:\n","[0.4806371926256924, 0.6437857646853802, 0.47795954871135965]\n","Mean: 0.5341275020074775\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 4/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 72.4 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.9348 train_rmse_stderror: 0.0 train_kl_div: 0.4592\n","val_rmse_target: 0.8217 val_rmse_stderror: 2.053\n","New best_val_rmse: 0.8217\n","\n","64 steps took 71.6 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.7085 train_rmse_stderror: 0.0 train_kl_div: 0.4601\n","val_rmse_target: 0.9222 val_rmse_stderror: 2.574\n","Still best_val_rmse: 0.8217 (from epoch 0)\n","\n","64 steps took 72.2 seconds\n","Epoch: 0 batch_num: 192\n","train_rmse_target: 0.6004 train_rmse_stderror: 0.0 train_kl_div: 0.1748\n","val_rmse_target: 0.9558 val_rmse_stderror: 2.265\n","Still best_val_rmse: 0.8217 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 0 batch_num: 256\n","train_rmse_target: 0.9017 train_rmse_stderror: 0.0 train_kl_div: 0.4631\n","val_rmse_target: 0.7249 val_rmse_stderror: 1.992\n","New best_val_rmse: 0.7249\n","\n","64 steps took 71.9 seconds\n","Epoch: 1 batch_num: 37\n","train_rmse_target: 0.9268 train_rmse_stderror: 0.0 train_kl_div: 0.5096\n","val_rmse_target: 0.8051 val_rmse_stderror: 2.507\n","Still best_val_rmse: 0.7249 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 1 batch_num: 101\n","train_rmse_target: 0.7783 train_rmse_stderror: 0.0 train_kl_div: 0.4702\n","val_rmse_target: 0.6485 val_rmse_stderror: 1.889\n","New best_val_rmse: 0.6485\n","\n","64 steps took 71.6 seconds\n","Epoch: 1 batch_num: 165\n","train_rmse_target: 0.8698 train_rmse_stderror: 0.0 train_kl_div: 0.6738\n","val_rmse_target: 0.5744 val_rmse_stderror: 1.845\n","New best_val_rmse: 0.5744\n","\n","64 steps took 71.6 seconds\n","Epoch: 1 batch_num: 229\n","train_rmse_target: 0.5305 train_rmse_stderror: 0.0 train_kl_div: -0.1207\n","val_rmse_target: 0.5389 val_rmse_stderror: 1.836\n","New best_val_rmse: 0.5389\n","\n","32 steps took 35.8 seconds\n","Epoch: 1 batch_num: 261\n","train_rmse_target: 0.5235 train_rmse_stderror: 0.0 train_kl_div: -0.1112\n","val_rmse_target: 0.5555 val_rmse_stderror: 1.968\n","Still best_val_rmse: 0.5389 (from epoch 1)\n","\n","64 steps took 71.8 seconds\n","Epoch: 2 batch_num: 42\n","train_rmse_target: 0.4065 train_rmse_stderror: 0.0 train_kl_div: -0.4124\n","val_rmse_target: 0.5863 val_rmse_stderror: 1.677\n","Still best_val_rmse: 0.5389 (from epoch 1)\n","\n","64 steps took 71.5 seconds\n","Epoch: 2 batch_num: 106\n","train_rmse_target: 0.4401 train_rmse_stderror: 0.0 train_kl_div: -0.323\n","val_rmse_target: 0.543 val_rmse_stderror: 1.751\n","Still best_val_rmse: 0.5389 (from epoch 1)\n","\n","32 steps took 35.8 seconds\n","Epoch: 2 batch_num: 138\n","train_rmse_target: 0.3969 train_rmse_stderror: 0.0 train_kl_div: -0.4042\n","val_rmse_target: 0.6048 val_rmse_stderror: 1.765\n","Still best_val_rmse: 0.5389 (from epoch 1)\n","\n","64 steps took 71.6 seconds\n","Epoch: 2 batch_num: 202\n","train_rmse_target: 0.3698 train_rmse_stderror: 0.0 train_kl_div: -0.454\n","val_rmse_target: 0.5147 val_rmse_stderror: 1.801\n","New best_val_rmse: 0.5147\n","\n","32 steps took 35.8 seconds\n","Epoch: 2 batch_num: 234\n","train_rmse_target: 0.5495 train_rmse_stderror: 0.0 train_kl_div: 0.1214\n","val_rmse_target: 0.5665 val_rmse_stderror: 1.675\n","Still best_val_rmse: 0.5147 (from epoch 2)\n","\n","64 steps took 71.8 seconds\n","Epoch: 3 batch_num: 15\n","train_rmse_target: 0.3197 train_rmse_stderror: 0.0 train_kl_div: -0.5491\n","val_rmse_target: 0.5027 val_rmse_stderror: 1.674\n","New best_val_rmse: 0.5027\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 47\n","train_rmse_target: 0.195 train_rmse_stderror: 0.0 train_kl_div: -0.3703\n","val_rmse_target: 0.5133 val_rmse_stderror: 1.937\n","Still best_val_rmse: 0.5027 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 79\n","train_rmse_target: 0.3672 train_rmse_stderror: 0.0 train_kl_div: -0.513\n","val_rmse_target: 0.559 val_rmse_stderror: 1.753\n","Still best_val_rmse: 0.5027 (from epoch 3)\n","\n","64 steps took 71.5 seconds\n","Epoch: 3 batch_num: 143\n","train_rmse_target: 0.3812 train_rmse_stderror: 0.0 train_kl_div: -0.3871\n","val_rmse_target: 0.5552 val_rmse_stderror: 1.831\n","Still best_val_rmse: 0.5027 (from epoch 3)\n","\n","64 steps took 71.5 seconds\n","Epoch: 3 batch_num: 207\n","train_rmse_target: 0.2752 train_rmse_stderror: 0.0 train_kl_div: -0.7471\n","val_rmse_target: 0.4969 val_rmse_stderror: 1.685\n","New best_val_rmse: 0.4969\n","\n","32 steps took 35.8 seconds\n","Epoch: 3 batch_num: 239\n","train_rmse_target: 0.376 train_rmse_stderror: 0.0 train_kl_div: -0.3972\n","val_rmse_target: 0.6236 val_rmse_stderror: 1.816\n","Still best_val_rmse: 0.4969 (from epoch 3)\n","\n","64 steps took 71.7 seconds\n","Epoch: 4 batch_num: 20\n","train_rmse_target: 0.563 train_rmse_stderror: 0.0 train_kl_div: 0.6555\n","val_rmse_target: 0.5902 val_rmse_stderror: 1.671\n","Still best_val_rmse: 0.4969 (from epoch 3)\n","\n","64 steps took 71.5 seconds\n","Epoch: 4 batch_num: 84\n","train_rmse_target: 0.3485 train_rmse_stderror: 0.0 train_kl_div: -0.6127\n","val_rmse_target: 0.4993 val_rmse_stderror: 1.669\n","Still best_val_rmse: 0.4969 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 116\n","train_rmse_target: 0.2638 train_rmse_stderror: 0.0 train_kl_div: -0.7944\n","val_rmse_target: 0.5234 val_rmse_stderror: 1.643\n","Still best_val_rmse: 0.4969 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 148\n","train_rmse_target: 0.3066 train_rmse_stderror: 0.0 train_kl_div: -0.6957\n","val_rmse_target: 0.5345 val_rmse_stderror: 1.648\n","Still best_val_rmse: 0.4969 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 180\n","train_rmse_target: 0.2243 train_rmse_stderror: 0.0 train_kl_div: -0.9122\n","val_rmse_target: 0.5306 val_rmse_stderror: 1.618\n","Still best_val_rmse: 0.4969 (from epoch 3)\n","\n","32 steps took 35.7 seconds\n","Epoch: 4 batch_num: 212\n","train_rmse_target: 0.2244 train_rmse_stderror: 0.0 train_kl_div: -0.8711\n","val_rmse_target: 0.5253 val_rmse_stderror: 1.654\n","Still best_val_rmse: 0.4969 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 244\n","train_rmse_target: 0.4135 train_rmse_stderror: 0.0 train_kl_div: -0.3127\n","val_rmse_target: 0.5402 val_rmse_stderror: 1.769\n","Still best_val_rmse: 0.4969 (from epoch 3)\n","\n","32 steps took 35.8 seconds\n","Epoch: 4 batch_num: 276\n","train_rmse_target: 0.294 train_rmse_stderror: 0.0 train_kl_div: -0.5904\n","val_rmse_target: 0.4983 val_rmse_stderror: 1.613\n","Still best_val_rmse: 0.4969 (from epoch 3)\n","\n","32 steps took 36.0 seconds\n","Epoch: 5 batch_num: 25\n","train_rmse_target: 0.1447 train_rmse_stderror: 0.0 train_kl_div: -1.235\n","val_rmse_target: 0.4829 val_rmse_stderror: 1.593\n","New best_val_rmse: 0.4829\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 57\n","train_rmse_target: 0.1705 train_rmse_stderror: 0.0 train_kl_div: -1.196\n","val_rmse_target: 0.4877 val_rmse_stderror: 1.588\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 89\n","train_rmse_target: 0.2437 train_rmse_stderror: 0.0 train_kl_div: -0.8522\n","val_rmse_target: 0.491 val_rmse_stderror: 1.638\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 121\n","train_rmse_target: 0.2392 train_rmse_stderror: 0.0 train_kl_div: -0.8184\n","val_rmse_target: 0.4898 val_rmse_stderror: 1.604\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 153\n","train_rmse_target: 0.1906 train_rmse_stderror: 0.0 train_kl_div: -1.142\n","val_rmse_target: 0.4922 val_rmse_stderror: 1.568\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 185\n","train_rmse_target: 0.2408 train_rmse_stderror: 0.0 train_kl_div: -0.8837\n","val_rmse_target: 0.4882 val_rmse_stderror: 1.599\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 217\n","train_rmse_target: 0.2782 train_rmse_stderror: 0.0 train_kl_div: -0.6972\n","val_rmse_target: 0.5104 val_rmse_stderror: 1.569\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 5 batch_num: 249\n","train_rmse_target: 0.2493 train_rmse_stderror: 0.0 train_kl_div: -0.8447\n","val_rmse_target: 0.4919 val_rmse_stderror: 1.592\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 35.7 seconds\n","Epoch: 5 batch_num: 281\n","train_rmse_target: 0.242 train_rmse_stderror: 0.0 train_kl_div: -0.9731\n","val_rmse_target: 0.4886 val_rmse_stderror: 1.56\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 36.0 seconds\n","Epoch: 6 batch_num: 30\n","train_rmse_target: 0.1476 train_rmse_stderror: 0.0 train_kl_div: -1.397\n","val_rmse_target: 0.5032 val_rmse_stderror: 1.521\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 62\n","train_rmse_target: 0.1401 train_rmse_stderror: 0.0 train_kl_div: -1.548\n","val_rmse_target: 0.4988 val_rmse_stderror: 1.531\n","Still best_val_rmse: 0.4829 (from epoch 5)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 94\n","train_rmse_target: 0.1521 train_rmse_stderror: 0.0 train_kl_div: -1.277\n","val_rmse_target: 0.4826 val_rmse_stderror: 1.537\n","New best_val_rmse: 0.4826\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 126\n","train_rmse_target: 0.09171 train_rmse_stderror: 0.0 train_kl_div: -1.752\n","val_rmse_target: 0.4922 val_rmse_stderror: 1.523\n","Still best_val_rmse: 0.4826 (from epoch 6)\n","\n","32 steps took 35.7 seconds\n","Epoch: 6 batch_num: 158\n","train_rmse_target: 0.1518 train_rmse_stderror: 0.0 train_kl_div: -1.208\n","val_rmse_target: 0.4831 val_rmse_stderror: 1.522\n","Still best_val_rmse: 0.4826 (from epoch 6)\n","\n","32 steps took 35.7 seconds\n","Epoch: 6 batch_num: 190\n","train_rmse_target: 0.1744 train_rmse_stderror: 0.0 train_kl_div: -1.091\n","val_rmse_target: 0.486 val_rmse_stderror: 1.52\n","Still best_val_rmse: 0.4826 (from epoch 6)\n","\n","32 steps took 35.8 seconds\n","Epoch: 6 batch_num: 222\n","train_rmse_target: 0.1273 train_rmse_stderror: 0.0 train_kl_div: -1.675\n","val_rmse_target: 0.487 val_rmse_stderror: 1.515\n","Still best_val_rmse: 0.4826 (from epoch 6)\n","\n","32 steps took 35.7 seconds\n","Epoch: 6 batch_num: 254\n","train_rmse_target: 0.1402 train_rmse_stderror: 0.0 train_kl_div: -1.434\n","val_rmse_target: 0.4855 val_rmse_stderror: 1.524\n","Still best_val_rmse: 0.4826 (from epoch 6)\n","\n","32 steps took 36.0 seconds\n","Epoch: 7 batch_num: 3\n","train_rmse_target: 0.1062 train_rmse_stderror: 0.0 train_kl_div: -1.708\n","val_rmse_target: 0.4817 val_rmse_stderror: 1.509\n","New best_val_rmse: 0.4817\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 35\n","train_rmse_target: 0.06195 train_rmse_stderror: 0.0 train_kl_div: -2.269\n","val_rmse_target: 0.4829 val_rmse_stderror: 1.486\n","Still best_val_rmse: 0.4817 (from epoch 7)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 67\n","train_rmse_target: 0.09504 train_rmse_stderror: 0.0 train_kl_div: -1.878\n","val_rmse_target: 0.4857 val_rmse_stderror: 1.485\n","Still best_val_rmse: 0.4817 (from epoch 7)\n","\n","32 steps took 35.7 seconds\n","Epoch: 7 batch_num: 99\n","train_rmse_target: 0.111 train_rmse_stderror: 0.0 train_kl_div: -1.709\n","val_rmse_target: 0.4865 val_rmse_stderror: 1.487\n","Still best_val_rmse: 0.4817 (from epoch 7)\n","\n","32 steps took 35.7 seconds\n","Epoch: 7 batch_num: 131\n","train_rmse_target: 0.05495 train_rmse_stderror: 0.0 train_kl_div: -2.303\n","val_rmse_target: 0.4872 val_rmse_stderror: 1.475\n","Still best_val_rmse: 0.4817 (from epoch 7)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 163\n","train_rmse_target: 0.1074 train_rmse_stderror: 0.0 train_kl_div: -1.643\n","val_rmse_target: 0.4837 val_rmse_stderror: 1.48\n","Still best_val_rmse: 0.4817 (from epoch 7)\n","\n","32 steps took 35.7 seconds\n","Epoch: 7 batch_num: 195\n","train_rmse_target: 0.1369 train_rmse_stderror: 0.0 train_kl_div: -1.739\n","val_rmse_target: 0.4842 val_rmse_stderror: 1.481\n","Still best_val_rmse: 0.4817 (from epoch 7)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 227\n","train_rmse_target: 0.07126 train_rmse_stderror: 0.0 train_kl_div: -2.053\n","val_rmse_target: 0.4848 val_rmse_stderror: 1.48\n","Still best_val_rmse: 0.4817 (from epoch 7)\n","\n","32 steps took 35.8 seconds\n","Epoch: 7 batch_num: 259\n","train_rmse_target: 0.1941 train_rmse_stderror: 0.0 train_kl_div: -1.477\n","val_rmse_target: 0.4845 val_rmse_stderror: 1.481\n","Still best_val_rmse: 0.4817 (from epoch 7)\n","\n","Performance estimates:\n","[0.4806371926256924, 0.6437857646853802, 0.47795954871135965, 0.4817149298321294]\n","Mean: 0.5210243589636404\n","{'total_MiB': 16280, 'used_MiB': 927}\n","\n","Fold 5/5\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at /content/clrp-pre-trained/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","64 steps took 72.4 seconds\n","Epoch: 0 batch_num: 64\n","train_rmse_target: 0.592 train_rmse_stderror: 0.0 train_kl_div: -0.004132\n","val_rmse_target: 0.8503 val_rmse_stderror: 2.084\n","New best_val_rmse: 0.8503\n","\n","64 steps took 71.9 seconds\n","Epoch: 0 batch_num: 128\n","train_rmse_target: 0.7921 train_rmse_stderror: 0.0 train_kl_div: 0.5019\n","val_rmse_target: 0.9865 val_rmse_stderror: 2.557\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 0 batch_num: 192\n","train_rmse_target: 1.038 train_rmse_stderror: 0.0 train_kl_div: 0.5391\n","val_rmse_target: 1.01 val_rmse_stderror: 2.261\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 0 batch_num: 256\n","train_rmse_target: 1.112 train_rmse_stderror: 0.0 train_kl_div: 0.6096\n","val_rmse_target: 1.007 val_rmse_stderror: 2.27\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.8 seconds\n","Epoch: 1 batch_num: 37\n","train_rmse_target: 1.125 train_rmse_stderror: 0.0 train_kl_div: 0.6189\n","val_rmse_target: 1.005 val_rmse_stderror: 2.335\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 1 batch_num: 101\n","train_rmse_target: 1.032 train_rmse_stderror: 0.0 train_kl_div: 0.5343\n","val_rmse_target: 1.004 val_rmse_stderror: 2.269\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 1 batch_num: 165\n","train_rmse_target: 0.9378 train_rmse_stderror: 0.0 train_kl_div: 0.4416\n","val_rmse_target: 1.004 val_rmse_stderror: 2.215\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 1 batch_num: 229\n","train_rmse_target: 0.9908 train_rmse_stderror: 0.0 train_kl_div: 0.4911\n","val_rmse_target: 1.004 val_rmse_stderror: 2.166\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.8 seconds\n","Epoch: 2 batch_num: 10\n","train_rmse_target: 0.9126 train_rmse_stderror: 0.0 train_kl_div: 0.428\n","val_rmse_target: 1.01 val_rmse_stderror: 2.256\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 2 batch_num: 74\n","train_rmse_target: 0.9643 train_rmse_stderror: 0.0 train_kl_div: 0.4671\n","val_rmse_target: 1.004 val_rmse_stderror: 2.216\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 2 batch_num: 138\n","train_rmse_target: 1.192 train_rmse_stderror: 0.0 train_kl_div: 0.6903\n","val_rmse_target: 1.002 val_rmse_stderror: 2.259\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 2 batch_num: 202\n","train_rmse_target: 0.8597 train_rmse_stderror: 0.0 train_kl_div: 0.3647\n","val_rmse_target: 1.005 val_rmse_stderror: 2.181\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 2 batch_num: 266\n","train_rmse_target: 1.408 train_rmse_stderror: 0.0 train_kl_div: 0.9855\n","val_rmse_target: 1.001 val_rmse_stderror: 2.204\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.8 seconds\n","Epoch: 3 batch_num: 47\n","train_rmse_target: 1.001 train_rmse_stderror: 0.0 train_kl_div: 0.515\n","val_rmse_target: 0.9917 val_rmse_stderror: 2.312\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 3 batch_num: 111\n","train_rmse_target: 0.5068 train_rmse_stderror: 0.0 train_kl_div: 0.0763\n","val_rmse_target: 0.9819 val_rmse_stderror: 2.156\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.5 seconds\n","Epoch: 3 batch_num: 175\n","train_rmse_target: 1.008 train_rmse_stderror: 0.0 train_kl_div: 0.508\n","val_rmse_target: 0.9612 val_rmse_stderror: 2.269\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.6 seconds\n","Epoch: 3 batch_num: 239\n","train_rmse_target: 0.9108 train_rmse_stderror: 0.0 train_kl_div: 0.4315\n","val_rmse_target: 0.8811 val_rmse_stderror: 2.078\n","Still best_val_rmse: 0.8503 (from epoch 0)\n","\n","64 steps took 71.8 seconds\n","Epoch: 4 batch_num: 20\n","train_rmse_target: 0.6119 train_rmse_stderror: 0.0 train_kl_div: 0.07948\n","val_rmse_target: 0.7307 val_rmse_stderror: 1.994\n","New best_val_rmse: 0.7307\n","\n","64 steps took 71.6 seconds\n","Epoch: 4 batch_num: 84\n","train_rmse_target: 0.9578 train_rmse_stderror: 0.0 train_kl_div: 0.5137\n","val_rmse_target: 0.7836 val_rmse_stderror: 1.955\n","Still best_val_rmse: 0.7307 (from epoch 4)\n","\n","64 steps took 71.6 seconds\n","Epoch: 4 batch_num: 148\n","train_rmse_target: 0.648 train_rmse_stderror: 0.0 train_kl_div: 0.114\n","val_rmse_target: 0.688 val_rmse_stderror: 2.037\n","New best_val_rmse: 0.688\n","\n","64 steps took 71.5 seconds\n","Epoch: 4 batch_num: 212\n","train_rmse_target: 0.7647 train_rmse_stderror: 0.0 train_kl_div: 0.2132\n","val_rmse_target: 0.6581 val_rmse_stderror: 1.97\n","New best_val_rmse: 0.6581\n","\n","64 steps took 71.6 seconds\n","Epoch: 4 batch_num: 276\n","train_rmse_target: 0.6934 train_rmse_stderror: 0.0 train_kl_div: 0.1482\n","val_rmse_target: 0.6715 val_rmse_stderror: 1.901\n","Still best_val_rmse: 0.6581 (from epoch 4)\n","\n","64 steps took 71.9 seconds\n","Epoch: 5 batch_num: 57\n","train_rmse_target: 0.825 train_rmse_stderror: 0.0 train_kl_div: 0.3779\n","val_rmse_target: 0.6892 val_rmse_stderror: 1.94\n","Still best_val_rmse: 0.6581 (from epoch 4)\n","\n","64 steps took 71.5 seconds\n","Epoch: 5 batch_num: 121\n","train_rmse_target: 0.6535 train_rmse_stderror: 0.0 train_kl_div: 0.1163\n","val_rmse_target: 0.6656 val_rmse_stderror: 1.838\n","Still best_val_rmse: 0.6581 (from epoch 4)\n","\n","64 steps took 71.5 seconds\n","Epoch: 5 batch_num: 185\n","train_rmse_target: 0.5703 train_rmse_stderror: 0.0 train_kl_div: -0.06292\n","val_rmse_target: 0.6365 val_rmse_stderror: 1.828\n","New best_val_rmse: 0.6365\n","\n","64 steps took 71.6 seconds\n","Epoch: 5 batch_num: 249\n","train_rmse_target: 0.584 train_rmse_stderror: 0.0 train_kl_div: -0.02817\n","val_rmse_target: 0.6107 val_rmse_stderror: 1.85\n","New best_val_rmse: 0.6107\n","\n","64 steps took 71.8 seconds\n","Epoch: 6 batch_num: 30\n","train_rmse_target: 0.6256 train_rmse_stderror: 0.0 train_kl_div: 0.03653\n","val_rmse_target: 0.6076 val_rmse_stderror: 1.896\n","New best_val_rmse: 0.6076\n","\n","64 steps took 71.5 seconds\n","Epoch: 6 batch_num: 94\n","train_rmse_target: 0.6186 train_rmse_stderror: 0.0 train_kl_div: 0.01292\n","val_rmse_target: 0.6062 val_rmse_stderror: 1.84\n","New best_val_rmse: 0.6062\n","\n","64 steps took 71.6 seconds\n","Epoch: 6 batch_num: 158\n","train_rmse_target: 0.7245 train_rmse_stderror: 0.0 train_kl_div: 0.2053\n","val_rmse_target: 0.6076 val_rmse_stderror: 1.884\n","Still best_val_rmse: 0.6062 (from epoch 6)\n","\n","64 steps took 71.5 seconds\n","Epoch: 6 batch_num: 222\n","train_rmse_target: 0.4802 train_rmse_stderror: 0.0 train_kl_div: -0.2265\n","val_rmse_target: 0.6005 val_rmse_stderror: 1.821\n","New best_val_rmse: 0.6005\n","\n","64 steps took 71.8 seconds\n","Epoch: 7 batch_num: 3\n","train_rmse_target: 0.4892 train_rmse_stderror: 0.0 train_kl_div: -0.1639\n","val_rmse_target: 0.5947 val_rmse_stderror: 1.855\n","New best_val_rmse: 0.5947\n","\n","64 steps took 71.6 seconds\n","Epoch: 7 batch_num: 67\n","train_rmse_target: 0.4342 train_rmse_stderror: 0.0 train_kl_div: -0.263\n","val_rmse_target: 0.6017 val_rmse_stderror: 1.835\n","Still best_val_rmse: 0.5947 (from epoch 7)\n","\n","64 steps took 71.6 seconds\n","Epoch: 7 batch_num: 131\n","train_rmse_target: 0.5054 train_rmse_stderror: 0.0 train_kl_div: -0.1617\n","val_rmse_target: 0.5983 val_rmse_stderror: 1.841\n","Still best_val_rmse: 0.5947 (from epoch 7)\n","\n","64 steps took 71.6 seconds\n","Epoch: 7 batch_num: 195\n","train_rmse_target: 0.6882 train_rmse_stderror: 0.0 train_kl_div: 0.1587\n","val_rmse_target: 0.5961 val_rmse_stderror: 1.839\n","Still best_val_rmse: 0.5947 (from epoch 7)\n","\n","64 steps took 71.5 seconds\n","Epoch: 7 batch_num: 259\n","train_rmse_target: 0.3399 train_rmse_stderror: 0.0 train_kl_div: -0.3639\n","val_rmse_target: 0.5956 val_rmse_stderror: 1.84\n","Still best_val_rmse: 0.5947 (from epoch 7)\n","\n","Performance estimates:\n","[0.4806371926256924, 0.6437857646853802, 0.47795954871135965, 0.4817149298321294, 0.5946783716826446]\n","Mean: 0.5357551615074413\n","{'total_MiB': 16280, 'used_MiB': 927}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m4v-cGx-Mv7S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627253712494,"user_tz":-540,"elapsed":18,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"564852c2-0a9d-4c2a-8f10-9ccbf1fca578"},"source":["print(list_val_rmse)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[0.4806371926256924, 0.6437857646853802, 0.47795954871135965, 0.4817149298321294, 0.5946783716826446]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q2CdCMuIKDMP","executionInfo":{"status":"ok","timestamp":1627253712495,"user_tz":-540,"elapsed":14,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["#rep = MemReporter(model)\n","#rep.report()"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLl1yDOOKIe7","executionInfo":{"status":"ok","timestamp":1627253712496,"user_tz":-540,"elapsed":14,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["#rep = MemReporter(model.roberta)\n","#rep.report()"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qkqnknA_m9D","executionInfo":{"status":"ok","timestamp":1627253712496,"user_tz":-540,"elapsed":13,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["#gpuinfo()"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwrqSMdYA6Pu","executionInfo":{"status":"ok","timestamp":1627253712496,"user_tz":-540,"elapsed":12,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":["#del model\n","#del optimizer \n","#del train_loader\n","#del val_loader\n","#del scheduler \n","#del list_val_rmse\n","#del train_indices\n","#del val_indices\n","#del tokenizer\n","#torch.cuda.empty_cache()\n","#gpuinfo()"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wXcHyUSJXecL"},"source":["# upload models"]},{"cell_type":"code","metadata":{"id":"YIV6UllSIGoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627253830189,"user_tz":-540,"elapsed":117705,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"60fabf55-ada1-4fd4-eba5-6fd05b0a52d7"},"source":["%cd\n","!mkdir .kaggle\n","!mkdir /content/model\n","!cp /content/drive/MyDrive/Colab_Files/kaggle-api/kaggle.json .kaggle/\n","\n","!cp -r /content/model_1.pth /content/model/model_1.pth\n","!cp -r /content/model_2.pth /content/model/model_2.pth\n","!cp -r /content/model_3.pth /content/model/model_3.pth\n","!cp -r /content/model_4.pth /content/model/model_4.pth\n","!cp -r /content/model_5.pth /content/model/model_5.pth"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"14ddOZH4IMam","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627253924669,"user_tz":-540,"elapsed":94496,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}},"outputId":"3e6920f9-30cf-4de4-89ca-4b21dd1f850d"},"source":["def dataset_upload():\n","    import json\n","    from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","    id = f'{USERID}/{EX_NO}'\n","\n","    dataset_metadata = {}\n","    dataset_metadata['id'] = id\n","    dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","    dataset_metadata['title'] = f'{EX_NO}'\n","\n","    with open(UPLOAD_DIR / 'dataset-metadata.json', 'w') as f:\n","        json.dump(dataset_metadata, f, indent=4)\n","\n","    api = KaggleApi()\n","    api.authenticate()\n","\n","    # データセットがない場合\n","    if f'{USERID}/{EX_NO}' not in [str(d) for d in api.dataset_list(user=USERID, search=f'\"{EX_NO}\"')]:\n","        api.dataset_create_new(folder=UPLOAD_DIR,\n","                               convert_to_csv=False,\n","                               dir_mode='skip')\n","    # データセットがある場合\n","    else:\n","        api.dataset_create_version(folder=UPLOAD_DIR,\n","                                   version_notes='update',\n","                                   convert_to_csv=False,\n","                                   delete_old_versions=True,\n","                                   dir_mode='skip')\n","dataset_upload()\n","\n"],"execution_count":29,"outputs":[{"output_type":"stream","text":["  1%|          | 8.87M/1.33G [00:00<00:21, 66.1MB/s]"],"name":"stderr"},{"output_type":"stream","text":["Starting upload for file model_2.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:20<00:00, 68.5MB/s]\n","  1%|          | 10.9M/1.33G [00:00<00:12, 112MB/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_2.pth (1GB)\n","Starting upload for file model_5.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:12<00:00, 113MB/s]\n","  0%|          | 0.00/1.33G [00:00<?, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_5.pth (1GB)\n","Starting upload for file model_4.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:21<00:00, 65.2MB/s]\n","  1%|          | 6.87M/1.33G [00:00<00:23, 61.5MB/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_4.pth (1GB)\n","Starting upload for file model_3.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:15<00:00, 92.6MB/s]\n","  1%|          | 6.87M/1.33G [00:00<00:22, 61.7MB/s]"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_3.pth (1GB)\n","Starting upload for file model_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1.33G/1.33G [00:20<00:00, 70.6MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Upload successful: model_1.pth (1GB)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"huJwVMSAPuDO","executionInfo":{"status":"ok","timestamp":1627253924671,"user_tz":-540,"elapsed":24,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zzuBPobmLFu","executionInfo":{"status":"ok","timestamp":1627253924672,"user_tz":-540,"elapsed":23,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wpc8ro9hmNci","executionInfo":{"status":"ok","timestamp":1627253924672,"user_tz":-540,"elapsed":22,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"ceDI72NumT5-","executionInfo":{"status":"ok","timestamp":1627253924672,"user_tz":-540,"elapsed":21,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvRi_JQgwcKI","executionInfo":{"status":"ok","timestamp":1627253924673,"user_tz":-540,"elapsed":21,"user":{"displayName":"堂込一智","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg0KUBIwJ38GUlukN0OqZB0q5CTq1FUqQQ45Eml7w=s64","userId":"12219727497971505625"}}},"source":[""],"execution_count":29,"outputs":[]}]}