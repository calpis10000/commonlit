{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 739.992704,
      "end_time": "2021-06-12T04:24:42.758595",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-06-12T04:12:22.765891",
      "version": "2.3.3"
    },
    "colab": {
      "name": "032-01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calpis10000/commonlit/blob/main/02_nb/032_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "-ILQV9f9LjB7",
        "outputId": "8ed456f9-73ad-417e-be7d-e9e8ed92158f"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "if 'google.colab' in sys.modules:  # colab環境特有の処理_初回のみ\n",
        "  # Google Driveのマウント\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules' \\\n",
        "   -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/requirements.txt' \\\n",
        "   --ignore-installed\n",
        "\n",
        "  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules' \\\n",
        "   transformers -U\n",
        "  !pip install gensim==4.0.1 --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\n",
        "\"\"\""
      ],
      "id": "-ILQV9f9LjB7",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nif 'google.colab' in sys.modules:  # colab環境特有の処理_初回のみ\\n  # Google Driveのマウント\\n  from google.colab import drive\\n  drive.mount('/content/drive')\\n\\n  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'    -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/requirements.txt'    --ignore-installed\\n\\n  !pip install --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'    transformers -U\\n  !pip install gensim==4.0.1 --target '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules'\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuIP4iaRWwmu"
      },
      "source": [
        ""
      ],
      "id": "zuIP4iaRWwmu",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUxJBg4jKDiV",
        "outputId": "fda7a0a4-e514-4c05-fcdf-200cd12e0200"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:  # colab特有の処理_2回目移行\n",
        "  # Google Driveのマウント\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # データセットをDriveから取得\n",
        "  !mkdir -p 'input'\n",
        "  !cp -r '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input' '/content/input'\n",
        "\n",
        "  # ライブラリのパス指定\n",
        "  sys.path.append('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/XX_modules')"
      ],
      "id": "nUxJBg4jKDiV",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 6.051579,
          "end_time": "2021-06-12T04:12:35.921582",
          "exception": false,
          "start_time": "2021-06-12T04:12:29.870003",
          "status": "completed"
        },
        "tags": [],
        "id": "jnI6rNwDGJKD"
      },
      "source": [
        "# basic\n",
        "import os\n",
        "import gc\n",
        "import sys\n",
        "import yaml\n",
        "import warnings\n",
        "import random\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import hashlib\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# usual\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "# preprocess\n",
        "from fasttext import load_model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "#import texthero as hero\n",
        "import nltk\n",
        "import collections\n",
        "import gensim\n",
        "from gensim.models import word2vec, KeyedVectors\n",
        "import cv2\n",
        "import string\n",
        "import re\n",
        "import fasttext\n",
        "\n",
        "# LightGBM\n",
        "import lightgbm as lgb\n",
        "#import optuna.integration.lightgbm as lgb  # チューニング用\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "from pandas_profiling import ProfileReport  # profile report を作る用\n",
        "\n",
        "# preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# plot settings\n",
        "plt.rcParams[\"patch.force_edgecolor\"] = False\n",
        "plt.rcParams['font.family'] = 'sans_serif'\n",
        "sns.set(style=\"whitegrid\",  palette=\"muted\", color_codes=True, rc={'grid.linestyle': '--'})\n",
        "red = sns.xkcd_rgb[\"light red\"]\n",
        "green = sns.xkcd_rgb[\"medium green\"]\n",
        "blue = sns.xkcd_rgb[\"denim blue\"]\n",
        "\n",
        "# plot extentions\n",
        "#import japanize_matplotlib\n",
        "from matplotlib_venn import venn2\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "id": "jnI6rNwDGJKD",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.044898,
          "end_time": "2021-06-12T04:12:36.003748",
          "exception": false,
          "start_time": "2021-06-12T04:12:35.958850",
          "status": "completed"
        },
        "tags": [],
        "id": "lm5sI6LlGJKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4add12-8f94-4a51-9f95-9c08397a4162"
      },
      "source": [
        "# 試験ID生成\n",
        "trial_prefix = 'nb032'  # ←手動で指定 \n",
        "dttm_now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "trial_id = f'{trial_prefix}_{dttm_now}'\n",
        "\n",
        "print(trial_prefix)\n",
        "print(trial_id)"
      ],
      "id": "lm5sI6LlGJKG",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb032\n",
            "nb032_20210619_091653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.043839,
          "end_time": "2021-06-12T04:12:36.083942",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.040103",
          "status": "completed"
        },
        "tags": [],
        "id": "lQpccp-VGJKH"
      },
      "source": [
        "# アウトプットの出力先指定\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    OUTPUT_DIR = Path(\".\")\n",
        "elif 'google.colab' in sys.modules:\n",
        "    OUTPUT_DIR = Path(\"/content/drive/MyDrive/Colab_Files/kaggle/commonlit/03_outputs\")\n",
        "else:\n",
        "    OUTPUT_DIR = Path(f\"../03_outputs/{trial_prefix}\")\n",
        "    OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
      ],
      "id": "lQpccp-VGJKH",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.042944,
          "end_time": "2021-06-12T04:12:36.163441",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.120497",
          "status": "completed"
        },
        "tags": [],
        "id": "NWrez3qyGJKH"
      },
      "source": [
        "# seed固定\n",
        "def set_seed(seed=2021):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "SEED = 2021\n",
        "set_seed(SEED)"
      ],
      "id": "NWrez3qyGJKH",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.042234,
          "end_time": "2021-06-12T04:12:36.243086",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.200852",
          "status": "completed"
        },
        "tags": [],
        "id": "6O_5deZoGJKH"
      },
      "source": [
        "# インプットフォルダ指定\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    DATA_DIR = '../input/commonlitreadabilityprize/'\n",
        "elif 'google.colab' in sys.modules:\n",
        "    DATA_DIR = '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/00_input/commonlitreadabilityprize/'\n",
        "else:\n",
        "    DATA_DIR = '../00_input/commonlitreadabilityprize/'"
      ],
      "id": "6O_5deZoGJKH",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.120829,
          "end_time": "2021-06-12T04:12:36.400695",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.279866",
          "status": "completed"
        },
        "tags": [],
        "id": "KxOQu4miGJKI"
      },
      "source": [
        "# read_data\n",
        "train_base = pd.read_csv(DATA_DIR + 'train.csv')\n",
        "test_base = pd.read_csv(DATA_DIR + 'test.csv')\n",
        "sample = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
      ],
      "id": "KxOQu4miGJKI",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.036408,
          "end_time": "2021-06-12T04:12:36.473580",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.437172",
          "status": "completed"
        },
        "tags": [],
        "id": "yA1vUylZGJKI"
      },
      "source": [
        "## 特徴作成_共通処理"
      ],
      "id": "yA1vUylZGJKI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.043267,
          "end_time": "2021-06-12T04:12:36.554272",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.511005",
          "status": "completed"
        },
        "tags": [],
        "id": "5HIY2BfaGJKI"
      },
      "source": [
        "# ベースとなる継承元のクラス\n",
        "class BaseBlock(object):\n",
        "    def fit(self, input_df, y=None):\n",
        "        return self.transform(input_df)\n",
        "    def transform(self, input_df):\n",
        "        raise NotImplementedError()"
      ],
      "id": "5HIY2BfaGJKI",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.036407,
          "end_time": "2021-06-12T04:12:36.626866",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.590459",
          "status": "completed"
        },
        "tags": [],
        "id": "IhC7R4DNGJKJ"
      },
      "source": [
        "## テキスト特徴_共通処理"
      ],
      "id": "IhC7R4DNGJKJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.043001,
          "end_time": "2021-06-12T04:12:36.706394",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.663393",
          "status": "completed"
        },
        "tags": [],
        "id": "GA9ujg-PGJKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7079cf-3f14-4a8f-a893-e683ebed24b2"
      },
      "source": [
        "# ローカルの場合、stopwordsをダウンロード\n",
        "import nltk\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    pass\n",
        "else:\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    os.listdir(os.path.expanduser('~/nltk_data/corpora/stopwords/'))"
      ],
      "id": "GA9ujg-PGJKJ",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.061229,
          "end_time": "2021-06-12T04:12:36.803981",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.742752",
          "status": "completed"
        },
        "tags": [],
        "id": "2z0uB87rGJKJ"
      },
      "source": [
        "# テキスト前処理\n",
        "# https://www.kaggle.com/alaasedeeq/commonlit-readability-eda\n",
        "\n",
        "#filtering the unwanted symbols, spaces, ....etc\n",
        "to_replace_by_space = re.compile('[/(){}\\[\\]|@,;]')\n",
        "punctuation = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "bad_symbols = re.compile('[^0-9a-z #+_]')\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def text_prepare(text):\n",
        "    '''\n",
        "    text: a string\n",
        "    returna modified version of the string\n",
        "    '''\n",
        "    text = text.lower() # lowercase text\n",
        "    text = re.sub(punctuation, '',text)\n",
        "    text = re.sub(to_replace_by_space, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = re.sub(bad_symbols, \"\", text)         # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = \" \".join([word for word in text.split(\" \") if word not in stopwords]) # delete stopwords from text\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text\n"
      ],
      "id": "2z0uB87rGJKJ",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.042912,
          "end_time": "2021-06-12T04:12:36.883404",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.840492",
          "status": "completed"
        },
        "tags": [],
        "id": "-JQ2zWeLGJKK"
      },
      "source": [
        "def text_normalization(s:pd.Series):\n",
        "    x = s.apply(text_prepare)\n",
        "    return x\n",
        "\n",
        "# Counterオブジェクトを取得\n",
        "def get_counter(text:str):\n",
        "    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n",
        "    counter = collections.Counter(text_list)\n",
        "    return counter"
      ],
      "id": "-JQ2zWeLGJKK",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.037032,
          "end_time": "2021-06-12T04:12:36.956794",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.919762",
          "status": "completed"
        },
        "tags": [],
        "id": "49-Mfs-_GJKK"
      },
      "source": [
        "## 前処理_品詞変換"
      ],
      "id": "49-Mfs-_GJKK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.044646,
          "end_time": "2021-06-12T04:12:37.037712",
          "exception": false,
          "start_time": "2021-06-12T04:12:36.993066",
          "status": "completed"
        },
        "tags": [],
        "id": "r0M4RfijGJKK"
      },
      "source": [
        "# テキスト情報を品詞に変換\n",
        "def get_pos_tag(text:str):\n",
        "    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n",
        "    pos_tag = [i[1] for i in nltk.pos_tag(text_list)]\n",
        "    return pos_tag\n",
        "\n",
        "def get_pos_tag_to_text(text:str):\n",
        "    text_list = [wrd for wrd in text.split(\" \") if wrd not in ('', '\\n')]\n",
        "    pos_tag = [i[1] for i in nltk.pos_tag(text_list)]\n",
        "    return \" \".join(pos_tag)"
      ],
      "id": "r0M4RfijGJKK",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.038386,
          "end_time": "2021-06-12T04:12:37.112346",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.073960",
          "status": "completed"
        },
        "tags": [],
        "id": "ypJnPS5SGJKK"
      },
      "source": [
        "## テキスト特徴_シンプルなTF-IDF"
      ],
      "id": "ypJnPS5SGJKK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.05767,
          "end_time": "2021-06-12T04:12:37.213368",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.155698",
          "status": "completed"
        },
        "tags": [],
        "id": "Ua4PSnv9GJKL"
      },
      "source": [
        "# 参考: https://www.guruguru.science/competitions/16/discussions/556029f7-484d-40d4-ad6a-9d86337487e2/\n",
        "\n",
        "class TfidfSimpleBlock(BaseBlock):\n",
        "    \"\"\"シンプルなTF-IDF特徴を作成する block\"\"\"\n",
        "    def __init__(self, column: str, max_features=50, ngram_range=(1,1), use_idf=True):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            column: str\n",
        "                変換対象のカラム名\n",
        "        \"\"\"\n",
        "        self.column = column\n",
        "        self.max_features=max_features\n",
        "        self.ngram_range=ngram_range\n",
        "        self.use_idf=use_idf\n",
        "        self.param_prefix=f\"col={column}_max_features={max_features}_\\\n",
        "                              ngram={ngram_range[0]}_{ngram_range[1]}_use_idf={use_idf}\"\n",
        "\n",
        "    def preprocess(self, input_df):\n",
        "        x = text_normalization(input_df[self.column])\n",
        "        return x\n",
        "\n",
        "    def get_master(self, _master_df):\n",
        "        \"\"\"tdidfを計算するための全体集合を返す.\"\"\"\n",
        "        return _master_df\n",
        "\n",
        "    def fit(self, \n",
        "            input_df, \n",
        "            _master_df=None, \n",
        "            y=None\n",
        "           ):\n",
        "        master_df = input_df if _master_df is None else self.get_master(_master_df)\n",
        "        text = self.preprocess(master_df)\n",
        "        self.vectorizer_ = TfidfVectorizer(max_features=self.max_features\n",
        "                                      ,ngram_range=self.ngram_range\n",
        "                                      ,use_idf=self.use_idf)\n",
        "\n",
        "        self.vectorizer_.fit(text)\n",
        "        self.prefix = 'tfidf' if self.use_idf == True else 'tf'\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def transform(self, input_df):\n",
        "        text = self.preprocess(input_df)\n",
        "        z = self.vectorizer_.transform(text)\n",
        "\n",
        "        out_df = pd.DataFrame(z.toarray())\n",
        "        out_df.columns = self.vectorizer_.get_feature_names()\n",
        "        return out_df.add_prefix(f'{self.prefix}_')\n"
      ],
      "id": "Ua4PSnv9GJKL",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.042255,
          "end_time": "2021-06-12T04:12:37.298884",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.256629",
          "status": "completed"
        },
        "tags": [],
        "id": "a_YjPLBcGJKN"
      },
      "source": [
        "## テキスト特徴_学習済みモデル（gemsim経由）"
      ],
      "id": "a_YjPLBcGJKN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.055099,
          "end_time": "2021-06-12T04:12:37.391700",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.336601",
          "status": "completed"
        },
        "tags": [],
        "id": "HkHVOrKQGJKN"
      },
      "source": [
        "# 参考: https://zenn.dev/koukyo1994/articles/9b1da2482d8ba1\n",
        "# 参考: https://github.com/yagays/swem\n",
        "\n",
        "class GensimPreTrainedBlock(BaseBlock):\n",
        "    \"\"\"\n",
        "    文書をgemsim経由で学習済みモデルのベクトル表現へ変換するblock\n",
        "    モデルは別途入手し、インスタンス作成時に指定する。\n",
        "    モデル名は手動で入力する想定（デフォルトではgensim_pretrained で入る）\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 column: str,\n",
        "                 model:KeyedVectors,\n",
        "                 model_name='gensim_pretrained',\n",
        "                 swem='aver'):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            column: str\n",
        "                変換対象のカラム名\n",
        "        \"\"\"\n",
        "        self.column = column\n",
        "        self.model = model\n",
        "        self.model_name = model_name\n",
        "        self.swem = swem # TODO:例外処理['aver', 'max', 'concat', 'hier']\n",
        "        self.param_prefix= f\"col={column}_model_name={model_name}_swem={swem}\"\n",
        "\n",
        "    def fit(self, input_df, y=None):\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def transform(self, input_df):\n",
        "        text = self.preprocess(input_df)\n",
        "        \n",
        "        if self.swem == 'aver':\n",
        "            feat = text.map(lambda x: self.average_pooling(x))\n",
        "        elif self.swem == 'max': \n",
        "            feat = text.map(lambda x: self.max_pooling(x))\n",
        "        elif self.swem == 'concat': \n",
        "            feat = text.map(lambda x: self.concat_average_max_pooling(x))\n",
        "        elif self.swem == 'hier': \n",
        "            feat = text.map(lambda x: self.hierarchical_pooling(x, n=3))\n",
        "            \n",
        "        out_df = pd.DataFrame(np.stack(feat.values))\n",
        "\n",
        "        return out_df.add_prefix(f'{self.model_name}_{self.column}_{self.swem}')\n",
        "    \n",
        "    # 前処理\n",
        "    def preprocess(self, input_df):\n",
        "        x = text_normalization(input_df[self.column])\n",
        "        return x\n",
        "    \n",
        "    # 文書ベクトルの取得\n",
        "    def get_sentence_vector(self, x: str):\n",
        "        ndim = self.model.vector_size\n",
        "        embeddings = [\n",
        "            self.model[word]\n",
        "            if word in self.model\n",
        "            else np.zeros(ndim)\n",
        "            for word in x.split()\n",
        "        ]\n",
        "\n",
        "        if len(embeddings) == 0:\n",
        "            return np.zeros(ndim, dtype=np.float32)\n",
        "        else:\n",
        "            return embeddings\n",
        "    \n",
        "    # SWEMの各処理\n",
        "    def average_pooling(self, text):\n",
        "        word_embeddings = self.get_sentence_vector(text)\n",
        "        return np.mean(word_embeddings, axis=0)\n",
        "\n",
        "    def max_pooling(self, text):\n",
        "        word_embeddings = self.get_sentence_vector(text)\n",
        "        return np.max(word_embeddings, axis=0)\n",
        "\n",
        "    def concat_average_max_pooling(self, text):\n",
        "        word_embeddings = self.get_sentence_vector(text)\n",
        "        return np.r_[np.mean(word_embeddings, axis=0), np.max(word_embeddings, axis=0)]\n",
        "\n",
        "    def hierarchical_pooling(self, text, n):\n",
        "        word_embeddings = self.get_sentence_vector(text)\n",
        "\n",
        "        text_len = len(word_embeddings) # TODO: これで合ってるか要確認\n",
        "        if n > text_len:\n",
        "            raise ValueError(f\"window size must be less than text length / window_size:{n} text_length:{text_len}\")\n",
        "        window_average_pooling_vec = [np.mean(word_embeddings[i:i + n], axis=0) for i in range(text_len - n + 1)]\n",
        "\n",
        "        return np.max(window_average_pooling_vec, axis=0)    \n",
        "            \n"
      ],
      "id": "HkHVOrKQGJKN",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.036269,
          "end_time": "2021-06-12T04:12:37.464408",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.428139",
          "status": "completed"
        },
        "tags": [],
        "id": "mg0N6qS4GJKO"
      },
      "source": [
        "## テキスト特徴_fasttest"
      ],
      "id": "mg0N6qS4GJKO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.045006,
          "end_time": "2021-06-12T04:12:37.546708",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.501702",
          "status": "completed"
        },
        "tags": [],
        "id": "IDgINZMXGJKO"
      },
      "source": [
        "# 参考: https://zenn.dev/koukyo1994/articles/9b1da2482d8ba1\n",
        "class FasttextBlock(BaseBlock):\n",
        "    \"\"\"文書をfasttextのテキスト表現へ変換する block\"\"\"\n",
        "    def __init__(self, column: str, ft_model:fasttext.FastText._FastText):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            column: str\n",
        "                変換対象のカラム名\n",
        "        \"\"\"\n",
        "        self.column = column\n",
        "        self.ft_model = ft_model\n",
        "        self.param_prefix= f\"col={column}\"\n",
        "\n",
        "    # 前処理\n",
        "    def preprocess(self, input_df):\n",
        "        x = text_normalization(input_df[self.column])\n",
        "        return x\n",
        "        \n",
        "    def fit(self, input_df, y=None):\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def transform(self, input_df):\n",
        "        text = self.preprocess(input_df)\n",
        "        feat = text.map(lambda x: ft_model.get_sentence_vector(x))\n",
        "        out_df = pd.DataFrame(np.stack(feat.values))\n",
        "\n",
        "        return out_df.add_prefix(f'fasttext_{self.column}_')"
      ],
      "id": "IDgINZMXGJKO",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.036498,
          "end_time": "2021-06-12T04:12:37.619495",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.582997",
          "status": "completed"
        },
        "tags": [],
        "id": "r7A1XXT0GJKP"
      },
      "source": [
        "## テキスト特徴_統計量"
      ],
      "id": "r7A1XXT0GJKP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.049015,
          "end_time": "2021-06-12T04:12:37.704812",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.655797",
          "status": "completed"
        },
        "tags": [],
        "id": "R0qvzof6GJKP"
      },
      "source": [
        "class TextDescriptionBlock(BaseBlock):\n",
        "    \"\"\"テキストに関する統計量を返す block\"\"\"\n",
        "    def __init__(self, column: str):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            column: str\n",
        "                変換対象のカラム名\n",
        "        \"\"\"\n",
        "        self.column = column\n",
        "        self.param_prefix = f'col={column}'\n",
        "\n",
        "    # 前処理\n",
        "    def preprocess(self, input_df):\n",
        "        x = text_normalization(input_df[self.column])\n",
        "        return x\n",
        "        \n",
        "    def fit(self, input_df, y=None):\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def transform(self, input_df):\n",
        "        # 前処理\n",
        "        self.text = self.preprocess(input_df)\n",
        "        self.counters = self.text.map(get_counter)\n",
        "\n",
        "        # 変換処理\n",
        "        _length = input_df[self.column].fillna('').map(lambda x: len(x) if x!='' else np.nan)\n",
        "        _wrd_cnt = self.counters.map(lambda x: sum(x.values()))\n",
        "        _wrd_nuniq = self.counters.map(lambda x: len(x))\n",
        "        _wrd_mean = self.counters.map(lambda x: np.mean(list(x.values())))\n",
        "        _wrd_max = self.counters.map(lambda x: np.max(list(x.values())))\n",
        "        \n",
        "        word_length = self.counters.map(lambda x: np.array([len(i) for i in x.keys()]))\n",
        "        word_length_desc = word_length.map(lambda x: pd.Series(x.ravel()).describe())\n",
        "        _word_length_desc_df = pd.DataFrame(word_length_desc.tolist()).iloc[:,1:]\n",
        "        _word_length_desc_df = _word_length_desc_df.add_prefix('word_length_')\n",
        "        \n",
        "        out_df = pd.concat([_length, _wrd_cnt, _wrd_nuniq, _wrd_mean, _wrd_max], axis=1)\n",
        "        out_df.columns = ['text_length', 'word_count', 'word_nunique', 'word_appearance_mean', 'word_appearance_max']\n",
        "        out_df = pd.concat([out_df, _word_length_desc_df], axis=1)\n",
        "        return out_df.add_suffix(f'_{self.column}')"
      ],
      "id": "R0qvzof6GJKP",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.03615,
          "end_time": "2021-06-12T04:12:37.777053",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.740903",
          "status": "completed"
        },
        "tags": [],
        "id": "K8KA35xKGJKP"
      },
      "source": [
        "## テキスト特徴_CountVectorizer"
      ],
      "id": "K8KA35xKGJKP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.052501,
          "end_time": "2021-06-12T04:12:37.869270",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.816769",
          "status": "completed"
        },
        "tags": [],
        "id": "hUDuslqgGJKP"
      },
      "source": [
        "class CountVectorizerBlock(BaseBlock):\n",
        "    \"\"\"CountVectorizer x SVD による圧縮を行なう block\"\"\"\n",
        "    def __init__(self, column: str, master_df=None, n_components=50, ngram_range=(1,1)):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            column: str\n",
        "                変換対象のカラム名\n",
        "        \"\"\"\n",
        "        self.column = column\n",
        "        self.master_df=master_df\n",
        "        self.n_components=n_components\n",
        "        self.ngram_range=ngram_range\n",
        "        self.param_prefix = f\"col={column}_comp={n_components}_ngram={''.join([str(i) for i in self.ngram_range])}\"\n",
        "\n",
        "    def preprocess(self, input_df):\n",
        "        x = text_normalization(input_df[self.column])\n",
        "        return x\n",
        "\n",
        "    def fit(self, \n",
        "            input_df, \n",
        "            y=None\n",
        "           ):\n",
        "        master_df = input_df if self.master_df is None else self.master_df\n",
        "        text = self.preprocess(master_df)\n",
        "        self.pileline_ = Pipeline([\n",
        "            ('tfidf', CountVectorizer(ngram_range=self.ngram_range)),\n",
        "            ('svd', TruncatedSVD(n_components=self.n_components, random_state=SEED)),\n",
        "        ])\n",
        "\n",
        "        self.pileline_.fit(text)\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def transform(self, input_df):\n",
        "        text = self.preprocess(input_df)\n",
        "        z = self.pileline_.transform(text)\n",
        "\n",
        "        out_df = pd.DataFrame(z)\n",
        "        return out_df.add_prefix(f'countvect_{self.column}_{\"_\".join([str(i) for i in self.ngram_range])}_')\n"
      ],
      "id": "hUDuslqgGJKP",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.039945,
          "end_time": "2021-06-12T04:12:37.946997",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.907052",
          "status": "completed"
        },
        "tags": [],
        "id": "lHad_XvWGJKP"
      },
      "source": [
        "## テキスト特徴_TF-IDF"
      ],
      "id": "lHad_XvWGJKP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.051996,
          "end_time": "2021-06-12T04:12:38.040718",
          "exception": false,
          "start_time": "2021-06-12T04:12:37.988722",
          "status": "completed"
        },
        "tags": [],
        "id": "57eDPHwLGJKQ"
      },
      "source": [
        "# 参考: https://www.guruguru.science/competitions/16/discussions/556029f7-484d-40d4-ad6a-9d86337487e2/\n",
        "\n",
        "class TfidfBlock(BaseBlock):\n",
        "    \"\"\"tfidf x SVD による圧縮を行なう block\"\"\"\n",
        "    def __init__(self, column: str, master_df=None, n_components=50, ngram_range=(1,1)):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            column: str\n",
        "                変換対象のカラム名\n",
        "        \"\"\"\n",
        "        self.column = column\n",
        "        self.master_df=master_df\n",
        "        self.n_components=n_components\n",
        "        self.ngram_range=ngram_range\n",
        "        self.param_prefix = f\"col={column}_comp={n_components}_ngram={''.join([str(i) for i in self.ngram_range])}\"\n",
        "\n",
        "    def preprocess(self, input_df):\n",
        "        x = text_normalization(input_df[self.column])\n",
        "        return x\n",
        "\n",
        "    def fit(self, \n",
        "            input_df, \n",
        "            y=None\n",
        "           ):\n",
        "        master_df = input_df if self.master_df is None else self.master_df\n",
        "        text = self.preprocess(master_df)\n",
        "        self.pileline_ = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=100000, ngram_range=self.ngram_range)),\n",
        "            ('svd', TruncatedSVD(n_components=self.n_components, random_state=SEED)),\n",
        "        ])\n",
        "\n",
        "        self.pileline_.fit(text)\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def transform(self, input_df):\n",
        "        text = self.preprocess(input_df)\n",
        "        z = self.pileline_.transform(text)\n",
        "\n",
        "        out_df = pd.DataFrame(z)\n",
        "        return out_df.add_prefix(f'tfidf_{self.column}_{\"_\".join([str(i) for i in self.ngram_range])}_')\n"
      ],
      "id": "57eDPHwLGJKQ",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.038549,
          "end_time": "2021-06-12T04:12:38.120283",
          "exception": false,
          "start_time": "2021-06-12T04:12:38.081734",
          "status": "completed"
        },
        "tags": [],
        "id": "2agA8GJvGJKQ"
      },
      "source": [
        "## テキスト特徴_W2V(データセットから学習)"
      ],
      "id": "2agA8GJvGJKQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.050697,
          "end_time": "2021-06-12T04:12:38.208101",
          "exception": false,
          "start_time": "2021-06-12T04:12:38.157404",
          "status": "completed"
        },
        "tags": [],
        "id": "AUTdZA3OGJKQ"
      },
      "source": [
        "# https://www.guruguru.science/competitions/16/discussions/2fafef06-5a26-4d33-b535-a94cc9549ac4/\n",
        "# https://www.guruguru.science/competitions/16/discussions/4a6f5f84-8491-4324-ba69-dec49dc648cd/\n",
        "\n",
        "def hashfxn(x):\n",
        "    return int(hashlib.md5(str(x).encode()).hexdigest(), 16)\n",
        "\n",
        "class W2VTrainBlock(BaseBlock):\n",
        "    \"\"\"Word2Vecを学習し、文書のベクトル表現を得るブロック。\n",
        "       学習済みモデルを使うパターンは、別に作成するものとする。\"\"\"\n",
        "    def __init__(self, \n",
        "                 column: str, \n",
        "                 master_df=None,\n",
        "                 model_size=50, \n",
        "                 min_count=1, \n",
        "                 window=5,\n",
        "                 n_iter=100\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            column: str\n",
        "                変換対象のカラム名\n",
        "        \"\"\"\n",
        "        self.column = column\n",
        "        self.master_df = master_df\n",
        "        self.model_size = model_size\n",
        "        self.min_count = min_count\n",
        "        self.window = window\n",
        "        self.n_iter = n_iter\n",
        "        self.param_prefix = f\"col={column}_model_size={model_size}_min_count={min_count}_window={window}_n_iter={n_iter}\"\n",
        "\n",
        "    def preprocess(self, input_df):\n",
        "        x = text_normalization(input_df[self.column])\n",
        "        return x\n",
        "\n",
        "    def fit(self, \n",
        "            input_df, \n",
        "            y=None\n",
        "           ):\n",
        "        master_df = input_df if self.master_df is None else self.master_df\n",
        "        text = self.preprocess(master_df)\n",
        "        word_lists = text.map(lambda x: [i for i in x.split(' ') if i not in (' ')])\n",
        "        self.w2v_model = word2vec.Word2Vec(word_lists.values.tolist(),\n",
        "                                      vector_size=self.model_size,\n",
        "                                      min_count=self.min_count,\n",
        "                                      window=self.window,\n",
        "                                      seed=SEED,\n",
        "                                      workers=1,\n",
        "                                      hashfxn=hashfxn,\n",
        "                                      epochs=self.n_iter)\n",
        "\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def transform(self, input_df):\n",
        "        text = self.preprocess(input_df)\n",
        "        word_lists = text.map(lambda x: [i for i in x.split(' ') if i not in (' ')])\n",
        "\n",
        "        # 各文章ごとにそれぞれの単語をベクトル表現に直し、平均をとって文章ベクトルにする\n",
        "        sentence_vectors = word_lists.progress_apply(\n",
        "            lambda x: np.mean([self.w2v_model.wv[e] for e in x], axis=0))\n",
        "        sentence_vectors = np.vstack([x for x in sentence_vectors])\n",
        "        sentence_vector_df = pd.DataFrame(sentence_vectors,\n",
        "                                          columns=[f\"w2v_{self.column}_w{self.window}_{i}\"\n",
        "                                                   for i in range(self.model_size)])\n",
        "        \n",
        "        return sentence_vector_df\n"
      ],
      "id": "AUTdZA3OGJKQ",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.049208,
          "end_time": "2021-06-12T04:12:38.293556",
          "exception": false,
          "start_time": "2021-06-12T04:12:38.244348",
          "status": "completed"
        },
        "tags": [],
        "id": "9u9hHhn-GJKQ"
      },
      "source": [
        "# 使用モデル指定\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    _bert_model_name = '../input/huggingface-bert-variants/bert-base-cased/bert-base-cased/'\n",
        "else: # ローカル or Colab\n",
        "    _bert_model_name = 'bert-base-cased'\n",
        "\n",
        "# 学習済みBERT\n",
        "class BertSequenceVectorizer:\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' # cudaが無いならcpuを使えばいいじゃない\n",
        "        self.model_name = _bert_model_name # 学習済みモデルの名前を指定\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.model_name) # 指定したmodel_nameでtokenizerを作成\n",
        "        self.bert_model = transformers.BertModel.from_pretrained(self.model_name) # 指定したmodel_nameで学習済みmodelを作成\n",
        "        self.bert_model = self.bert_model.to(self.device)\n",
        "        self.max_len = 176\n",
        "\n",
        "    def get_imp(self, sentence : str) -> np.array:\n",
        "        inp = self.tokenizer.encode(sentence)\n",
        "        len_inp = len(inp)\n",
        "        return [inp, len_inp]\n",
        "\n",
        "    def vectorize(self, sentence : str) -> np.array:\n",
        "        inp = self.tokenizer.encode(sentence)\n",
        "        len_inp = len(inp)\n",
        "\n",
        "        if len_inp >= self.max_len:\n",
        "            inputs = inp[:self.max_len]\n",
        "            masks = [1] * self.max_len\n",
        "        else:\n",
        "            inputs = inp + [0] * (self.max_len - len_inp)\n",
        "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
        "\n",
        "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
        "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
        "\n",
        "        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n",
        "        seq_out, pooled_out = bert_out['last_hidden_state'], bert_out['pooler_output']\n",
        "\n",
        "        if torch.cuda.is_available():    \n",
        "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
        "        else:\n",
        "            return seq_out[0][0].detach().numpy()\n"
      ],
      "id": "9u9hHhn-GJKQ",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCLEOnw-hp6C"
      },
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "# 使用モデル指定\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    _roberta_model_name = '../input/roberta-base/'\n",
        "else: # ローカル or Colab\n",
        "    _roberta_model_name = 'roberta-base'\n",
        "\n",
        "# 学習済みRoBerta\n",
        "class RoBertaSequenceVectorizer:\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' # cudaが無いならcpuを使えばいいじゃない\n",
        "        self.model_name = _roberta_model_name # 学習済みモデルの名前を指定\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name) # 指定したmodel_nameでtokenizerを作成\n",
        "        self.bert_model = transformers.RobertaModel.from_pretrained(self.model_name) # 指定したmodel_nameで学習済みmodelを作成\n",
        "        self.bert_model = self.bert_model.to(self.device)\n",
        "        self.max_len = 176\n",
        "\n",
        "    def get_imp(self, sentence : str) -> np.array:\n",
        "        inp = self.tokenizer.encode(sentence)\n",
        "        len_inp = len(inp)\n",
        "        return [inp, len_inp]\n",
        "\n",
        "    def vectorize(self, sentence : str) -> np.array:\n",
        "        inp = self.tokenizer.encode(sentence)\n",
        "        len_inp = len(inp)\n",
        "\n",
        "        if len_inp >= self.max_len:\n",
        "            inputs = inp[:self.max_len]\n",
        "            masks = [1] * self.max_len\n",
        "        else:\n",
        "            inputs = inp + [0] * (self.max_len - len_inp)\n",
        "            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n",
        "\n",
        "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n",
        "        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n",
        "\n",
        "        bert_out = self.bert_model(inputs_tensor, masks_tensor)\n",
        "        seq_out, pooled_out = bert_out['last_hidden_state'], bert_out['pooler_output']\n",
        "\n",
        "        if torch.cuda.is_available():    \n",
        "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
        "        else:\n",
        "            return seq_out[0][0].detach().numpy()\n"
      ],
      "id": "qCLEOnw-hp6C",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5laImyczGJKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1061de93-b7d2-4380-9951-f21c82b0478e"
      },
      "source": [
        "class BERTPreTrainedBlock(BaseBlock):\n",
        "    \"\"\"\n",
        "    学習済みBERTモデルを用いて、文書をベクトル表現へ変換するblock\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 column: str,\n",
        "                 model=BertSequenceVectorizer(),\n",
        "                 model_name='bert-base-cased'\n",
        "                ):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            column: str\n",
        "                変換対象のカラム名\n",
        "        \"\"\"\n",
        "        self.column = column\n",
        "        self.model = model\n",
        "        self.model_name = model_name\n",
        "        self.param_prefix = f\"col={column}_model_name={model_name}\"\n",
        "\n",
        "    def fit(self, input_df, y=None):\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def transform(self, input_df):            \n",
        "        bert_out = input_df[self.column].progress_apply(lambda x: self.model.vectorize(x))\n",
        "        out_df = pd.DataFrame(np.stack(bert_out))\n",
        "        return out_df.add_prefix(f'{self.model_name}_{self.column}')"
      ],
      "id": "5laImyczGJKR",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.036164,
          "end_time": "2021-06-12T04:12:38.365751",
          "exception": false,
          "start_time": "2021-06-12T04:12:38.329587",
          "status": "completed"
        },
        "tags": [],
        "id": "aMuHGMgMGJKR"
      },
      "source": [
        "# make_feat"
      ],
      "id": "aMuHGMgMGJKR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62EQQyzcGJKR"
      },
      "source": [
        "# 特徴量の出力先指定\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    FEAT_DIR = Path(\".\") # 仮で指定。吐き出さない処理を考えた方が良いか？\n",
        "elif 'google.colab' in sys.modules:\n",
        "    FEAT_DIR = Path('/content/drive/MyDrive/Colab_Files/kaggle/commonlit/04_feature')\n",
        "else:\n",
        "    FEAT_DIR = Path(f\"../04_feature\")\n",
        "    FEAT_DIR.mkdir(exist_ok=True, parents=True)"
      ],
      "id": "62EQQyzcGJKR",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 232.489126,
          "end_time": "2021-06-12T04:17:40.020806",
          "exception": false,
          "start_time": "2021-06-12T04:13:47.531680",
          "status": "completed"
        },
        "tags": [],
        "id": "BnVRE4lUGJKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6503f90c-a256-4598-d498-cf3ef2c6958e"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# モデル入手\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    fast_path = '../input/fasttext-pretrained-crawl-vector-en-bin/cc.en.300.bin'\n",
        "    gen_glv_wiki_path = '../input/stanfords-glove-pretrained-word-vectors/glove.6B.300d.txt'\n",
        "    gen_glv_twi_path = '../input/glovetwitter27b-in-gensim-kv-format/glove.twitter.27B.200d.kv'\n",
        "    gen_ggl_path = '../input/gensim-google-data/GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "    ft_model = fasttext.load_model(fast_path)\n",
        "    gen_glv_wiki_model = KeyedVectors.load_word2vec_format(gen_glv_wiki_path, binary=False)\n",
        "    #gen_glv_twi_model = KeyedVectors.load(gen_glv_twi_path)\n",
        "    #gen_ggl_model = KeyedVectors.load_word2vec_format(gen_ggl_path, binary=True)\n",
        "\n",
        "elif 'google.colab' in sys.modules:\n",
        "    fast_path = '/content/drive/MyDrive/Colab_Files/kaggle/commonlit/97_pre_trained/cc.en.300.bin'\n",
        "    gen_glv_wiki_model = api.load('glove-wiki-gigaword-300')\n",
        "    ft_model = fasttext.load_model(fast_path)\n",
        "\n",
        "else: # ローカルまたは自前のクラウド環境を想定\n",
        "    fast_path = '../97_pre_trained/cc.en.300.bin'\n",
        "    gen_glv_wiki_path = '~/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300'\n",
        "    gen_glv_twi_path = '~/gensim-data/glove-twitter-200/glove-twitter-200.gz'\n",
        "    gen_ggl_path = '~/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'\n",
        "\n",
        "    ft_model = fasttext.load_model('../97_pre_trained/cc.en.300.bin')\n",
        "    gen_glv_wiki_model = KeyedVectors.load_word2vec_format(gen_glv_wiki_path, binary=False)\n",
        "    #gen_glv_twi_model = KeyedVectors.load_word2vec_format(gen_glv_twi_path, binary=False)\n",
        "    #gen_ggl_model = KeyedVectors.load_word2vec_format(gen_ggl_path, binary=True)"
      ],
      "id": "BnVRE4lUGJKR",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.173573,
          "end_time": "2021-06-12T04:13:47.053264",
          "exception": false,
          "start_time": "2021-06-12T04:13:46.879691",
          "status": "completed"
        },
        "tags": [],
        "id": "t_EaEnThGJKR"
      },
      "source": [
        "# 初期化\n",
        "train_feat = pd.DataFrame()\n",
        "test_feat = pd.DataFrame()\n",
        "train_target = train_base['target'].copy()\n",
        "\n",
        "# tfidf作成用のdf作成\n",
        "whole_df = pd.concat([train_base[['id', 'excerpt']], test_base[['id', 'excerpt']]], axis='rows')"
      ],
      "id": "t_EaEnThGJKR",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2AS4-TuGJKS"
      },
      "source": [
        "# 特徴量の出力\n",
        "import feather \n",
        "def save_feather(input_df:pd.DataFrame, filename_:str):\n",
        "    input_df.to_feather(FEAT_DIR/filename_)\n",
        "    \n",
        "def load_feather(filename_:str):\n",
        "    return pd.read_feather(FEAT_DIR/filename_)\n",
        "\n",
        "def save_pickle(obj, filename_:str):\n",
        "    with open(FEAT_DIR/filename_, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "    \n",
        "def load_pickle(filename_:str):\n",
        "    with open(FEAT_DIR/filename_, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    return obj"
      ],
      "id": "Y2AS4-TuGJKS",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fajQ6ho4GJKS"
      },
      "source": [
        "# https://www.guruguru.science/competitions/16/discussions/95b7f8ec-a741-444f-933a-94c33b9e66be/\n",
        "# https://github.com/nyk510/vivid/blob/master/vivid/utils.py\n",
        "\n",
        "from time import time\n",
        "\n",
        "# 文字列を「*」で囲っていい感じに見せる関数？\n",
        "def decorate(s: str, decoration=None):\n",
        "    if decoration is None:\n",
        "        decoration = '★' * 20\n",
        "\n",
        "    return ' '.join([decoration, str(s), decoration])\n",
        "\n",
        "# 時間計測用\n",
        "class Timer:\n",
        "    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' ', verbose=0):\n",
        "\n",
        "        if prefix: format_str = str(prefix) + sep + format_str\n",
        "        if suffix: format_str = format_str + sep + str(suffix)\n",
        "        self.format_str = format_str\n",
        "        self.logger = logger\n",
        "        self.start = None\n",
        "        self.end = None\n",
        "        self.verbose = verbose\n",
        "\n",
        "    @property\n",
        "    def duration(self):\n",
        "        if self.end is None:\n",
        "            return 0\n",
        "        return self.end - self.start\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start = time()\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.end = time() \n",
        "        if self.verbose is None:\n",
        "            return\n",
        "        out_str = self.format_str.format(self.duration)\n",
        "        if self.logger:\n",
        "            self.logger.info(out_str)\n",
        "        else:\n",
        "            print(out_str)"
      ],
      "id": "fajQ6ho4GJKS",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Dzkz0CGJKS"
      },
      "source": [
        "# 一旦これで進めるが、もうちょっとスマートにしたい。\n",
        "def run_blocks(input_df, blocks, y=None, test=False, overwrite=False):\n",
        "    out_df = pd.DataFrame()\n",
        "\n",
        "    print(decorate('start run blocks...'))\n",
        "\n",
        "    with Timer(prefix='run test={}'.format(test)): \n",
        "        for block in blocks:\n",
        "            with Timer(prefix='\\t- {}'.format(str(block))):\n",
        "                name = block.__class__.__name__\n",
        "                params = block.param_prefix\n",
        "                filename_df = f\"{'Test' if test else 'Train'}_{name}_{params}.ftr\"\n",
        "                filepath_df = FEAT_DIR/filename_df\n",
        "\n",
        "                if filepath_df.exists():\n",
        "                    out_i = pd.read_feather(filepath_df)\n",
        "                    print(f'skip -> read:{filename_df}')\n",
        "                else:\n",
        "                    if not test:\n",
        "                        out_i = block.fit(input_df, y=y)\n",
        "                    else:\n",
        "                        out_i = block.transform(input_df)\n",
        "\n",
        "                    save_feather(out_i, filename_df)\n",
        "\n",
        "                assert len(input_df) == len(out_i)\n",
        "                out_df = pd.concat([out_df, out_i.add_suffix(f'@{name}')], axis=1)\n",
        "    return out_df"
      ],
      "id": "b-Dzkz0CGJKS",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoIfLx2aaNWM",
        "outputId": "4a032c5f-367c-49e0-f532-166bfd6af393"
      },
      "source": [
        "type(FEAT_DIR)"
      ],
      "id": "QoIfLx2aaNWM",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pathlib.PosixPath"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGYTL_EkGJKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dea5172-f1cd-45c5-9d91-e549ee5c68c0"
      },
      "source": [
        "# 特徴作成: 実行\n",
        "feat_blocks = [TextDescriptionBlock('excerpt')\n",
        "              ,TfidfBlock('excerpt', master_df=whole_df, ngram_range=(1,1))\n",
        "              #,TfidfBlock('excerpt', master_df=whole_df, ngram_range=(2,2))\n",
        "              #,TfidfBlock('excerpt', master_df=whole_df, ngram_range=(1,2))\n",
        "              ,FasttextBlock('excerpt', ft_model)\n",
        "              #,W2VTrainBlock('excerpt', master_df=whole_df, window=3)\n",
        "              #,W2VTrainBlock('excerpt', master_df=whole_df, window=10)\n",
        "              #,W2VTrainBlock('excerpt', master_df=whole_df, window=100)\n",
        "              ,GensimPreTrainedBlock('excerpt'\n",
        "                                     ,gen_glv_wiki_model\n",
        "                                     ,model_name='glove_wiki_giga300'\n",
        "                                     ,swem='aver')\n",
        "             ,BERTPreTrainedBlock('excerpt')\n",
        "             ,BERTPreTrainedBlock('excerpt', model=RoBertaSequenceVectorizer(), model_name='roberta-base')\n",
        "              ]\n",
        "\n",
        "overwrite = False\n",
        "train_feat = run_blocks(train_base, feat_blocks, overwrite=overwrite)\n",
        "test_feat = run_blocks(test_base, feat_blocks, test=True, overwrite=overwrite)"
      ],
      "id": "JGYTL_EkGJKS",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "★★★★★★★★★★★★★★★★★★★★ start run blocks... ★★★★★★★★★★★★★★★★★★★★\n",
            "skip -> read:Train_TextDescriptionBlock_col=excerpt.ftr\n",
            "\t- <__main__.TextDescriptionBlock object at 0x7f7a0fa99610> 1.587[s]\n",
            "skip -> read:Train_TfidfBlock_col=excerpt_comp=50_ngram=11.ftr\n",
            "\t- <__main__.TfidfBlock object at 0x7f7a0fa99690> 1.013[s]\n",
            "skip -> read:Train_FasttextBlock_col=excerpt.ftr\n",
            "\t- <__main__.FasttextBlock object at 0x7f7a0fa996d0> 1.265[s]\n",
            "skip -> read:Train_GensimPreTrainedBlock_col=excerpt_model_name=glove_wiki_giga300_swem=aver.ftr\n",
            "\t- <__main__.GensimPreTrainedBlock object at 0x7f7a0fa99750> 1.125[s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2834 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "skip -> read:Train_BERTPreTrainedBlock_col=excerpt_model_name=bert-base-cased.ftr\n",
            "\t- <__main__.BERTPreTrainedBlock object at 0x7f7a0fa99790> 1.159[s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2834/2834 [00:47<00:00, 59.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t- <__main__.BERTPreTrainedBlock object at 0x7f7a0faa7150> 48.159[s]\n",
            "run test=False 54.309[s]\n",
            "★★★★★★★★★★★★★★★★★★★★ start run blocks... ★★★★★★★★★★★★★★★★★★★★\n",
            "skip -> read:Test_TextDescriptionBlock_col=excerpt.ftr\n",
            "\t- <__main__.TextDescriptionBlock object at 0x7f7a0fa99610> 0.719[s]\n",
            "skip -> read:Test_TfidfBlock_col=excerpt_comp=50_ngram=11.ftr\n",
            "\t- <__main__.TfidfBlock object at 0x7f7a0fa99690> 0.491[s]\n",
            "skip -> read:Test_FasttextBlock_col=excerpt.ftr\n",
            "\t- <__main__.FasttextBlock object at 0x7f7a0fa996d0> 0.505[s]\n",
            "skip -> read:Test_GensimPreTrainedBlock_col=excerpt_model_name=glove_wiki_giga300_swem=aver.ftr\n",
            "\t- <__main__.GensimPreTrainedBlock object at 0x7f7a0fa99750> 1.213[s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 57.49it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "skip -> read:Test_BERTPreTrainedBlock_col=excerpt_model_name=bert-base-cased.ftr\n",
            "\t- <__main__.BERTPreTrainedBlock object at 0x7f7a0fa99790> 0.957[s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t- <__main__.BERTPreTrainedBlock object at 0x7f7a0faa7150> 0.295[s]\n",
            "run test=True 4.181[s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj1JHlRsGJKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72dd8831-0fd1-4553-a7a0-c42f64e94ed4"
      },
      "source": [
        "print(train_feat.shape)\n",
        "print(test_feat.shape)"
      ],
      "id": "Cj1JHlRsGJKS",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2834, 2198)\n",
            "(7, 2198)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.169598,
          "end_time": "2021-06-12T04:21:52.135361",
          "exception": false,
          "start_time": "2021-06-12T04:21:51.965763",
          "status": "completed"
        },
        "tags": [],
        "id": "uTU9KEd-GJKT"
      },
      "source": [
        "# train & predict"
      ],
      "id": "uTU9KEd-GJKT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.175368,
          "end_time": "2021-06-12T04:21:52.481305",
          "exception": false,
          "start_time": "2021-06-12T04:21:52.305937",
          "status": "completed"
        },
        "tags": [],
        "id": "qPcFm5qEGJKT"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def kfold_cv(X, y, n_splits=5, random_state=0):\n",
        "    folds = KFold(n_splits=n_splits, random_state=0, shuffle=True)\n",
        "    return list(folds.split(X, y))"
      ],
      "id": "qPcFm5qEGJKT",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.176101,
          "end_time": "2021-06-12T04:21:52.825126",
          "exception": false,
          "start_time": "2021-06-12T04:21:52.649025",
          "status": "completed"
        },
        "tags": [],
        "id": "Dlho26FmGJKT"
      },
      "source": [
        "target = 'target'\n",
        "cv = kfold_cv(train_feat, train_target)"
      ],
      "id": "Dlho26FmGJKT",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.176348,
          "end_time": "2021-06-12T04:21:53.175239",
          "exception": false,
          "start_time": "2021-06-12T04:21:52.998891",
          "status": "completed"
        },
        "tags": [],
        "id": "Uid3tvmXGJKT"
      },
      "source": [
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metrics': 'rmse',\n",
        "    'seed': SEED\n",
        "}"
      ],
      "id": "Uid3tvmXGJKT",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.173937,
          "end_time": "2021-06-12T04:21:53.517873",
          "exception": false,
          "start_time": "2021-06-12T04:21:53.343936",
          "status": "completed"
        },
        "tags": [],
        "id": "HvI1sHBcGJKT"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_squared_log_error"
      ],
      "id": "HvI1sHBcGJKT",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vsazJ9BGJKT"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import BayesianRidge\n"
      ],
      "id": "7vsazJ9BGJKT",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 155.785956,
          "end_time": "2021-06-12T04:24:29.472958",
          "exception": false,
          "start_time": "2021-06-12T04:21:53.687002",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "id": "H29q6JW2GJKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7670408-748a-4f1e-80b8-d3eb862e3c1f"
      },
      "source": [
        "oof_preds = np.zeros(len(train_feat))\n",
        "test_preds = np.zeros(len(test_feat))\n",
        "\n",
        "importances = pd.DataFrame()\n",
        "scores = []\n",
        "models = []\n",
        "\n",
        "for i, (train_index, valid_index) in enumerate(cv):\n",
        "    print(f'\\nFold {i + 1}')\n",
        "    trn_x, trn_y = train_feat.iloc[train_index], train_target.iloc[train_index]\n",
        "    val_x, val_y = train_feat.iloc[valid_index], train_target.iloc[valid_index]\n",
        "    \n",
        "    model = BayesianRidge(n_iter=300, verbose=True)\n",
        "    model.fit(\n",
        "        X=trn_x,\n",
        "        y=trn_y\n",
        "    )\n",
        "    \n",
        "    val_preds = model.predict(val_x)\n",
        "    oof_preds[valid_index] = val_preds\n",
        "    test_preds += model.predict(test_feat) / 5\n",
        "    \n",
        "    val_score = np.sqrt(mean_squared_error(val_y, val_preds))\n",
        "    scores.append(val_score)\n",
        "    models.append(model)\n",
        "    \n",
        "mean_score = np.mean(scores)\n",
        "std_score  = np.std(scores)\n",
        "all_score  = np.sqrt(mean_squared_error(train_target, oof_preds))\n",
        "metrics_name = 'RMSE'\n",
        "print(f'Mean {metrics_name}: {mean_score}, std: {std_score}, All {metrics_name}: {all_score}')"
      ],
      "id": "H29q6JW2GJKU",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 1\n",
            "Convergence after  20  iterations\n",
            "\n",
            "Fold 2\n",
            "Convergence after  20  iterations\n",
            "\n",
            "Fold 3\n",
            "Convergence after  20  iterations\n",
            "\n",
            "Fold 4\n",
            "Convergence after  20  iterations\n",
            "\n",
            "Fold 5\n",
            "Convergence after  19  iterations\n",
            "Mean RMSE: 0.5515685350305982, std: 0.015312457015436619, All RMSE: 0.551787257195358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.193544,
          "end_time": "2021-06-12T04:24:29.845584",
          "exception": false,
          "start_time": "2021-06-12T04:24:29.652040",
          "status": "completed"
        },
        "tags": [],
        "id": "78Z6SPJ7GJKU"
      },
      "source": [
        "sample['target'] = test_preds\n",
        "\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    sample.to_csv('submission.csv',index=False)\n",
        "else:\n",
        "    sample.to_csv(OUTPUT_DIR/'submission.csv',index=False)"
      ],
      "id": "78Z6SPJ7GJKU",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.193637,
          "end_time": "2021-06-12T04:24:30.230220",
          "exception": false,
          "start_time": "2021-06-12T04:24:30.036583",
          "status": "completed"
        },
        "tags": [],
        "id": "98bEBH-OGJKU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "15ca030f-df12-4ea6-fc7f-6f498326dbb2"
      },
      "source": [
        "print(sample.shape)\n",
        "sample.head()"
      ],
      "id": "98bEBH-OGJKU",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c0f722661</td>\n",
              "      <td>-0.682587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>f0953f0a5</td>\n",
              "      <td>-0.334891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0df072751</td>\n",
              "      <td>-0.658301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>04caf4e0c</td>\n",
              "      <td>-2.001995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0e63f8bea</td>\n",
              "      <td>-1.485477</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id    target\n",
              "0  c0f722661 -0.682587\n",
              "1  f0953f0a5 -0.334891\n",
              "2  0df072751 -0.658301\n",
              "3  04caf4e0c -2.001995\n",
              "4  0e63f8bea -1.485477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "papermill": {
          "duration": 0.185202,
          "end_time": "2021-06-12T04:24:30.601993",
          "exception": false,
          "start_time": "2021-06-12T04:24:30.416791",
          "status": "completed"
        },
        "tags": [],
        "id": "xUGyixX_GJKU"
      },
      "source": [
        "# importance(kaggle環境では描画しない)\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    pass\n",
        "else:\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    sns.barplot(x='gain', y='feature', data=importances.sort_values('gain', ascending=False));\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'feature_importance.png'))"
      ],
      "id": "xUGyixX_GJKU"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "papermill": {
          "duration": 0.185856,
          "end_time": "2021-06-12T04:24:30.965386",
          "exception": false,
          "start_time": "2021-06-12T04:24:30.779530",
          "status": "completed"
        },
        "tags": [],
        "id": "ZjAKzzXyGJKU"
      },
      "source": [
        "# importance_boxen(kaggle環境では描画しない)\n",
        "# 参考: https://www.guruguru.science/competitions/13/discussions/d8f2d66a-aeee-4789-8b3d-d5935c26b1b7/\n",
        "\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    pass\n",
        "else:\n",
        "    order = importances.groupby('feature')\\\n",
        "        .sum()[['gain']]\\\n",
        "        .sort_values('gain', ascending=False).index[:50]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(max(6, len(order) * .4), 7))\n",
        "    sns.boxenplot(data=importances, x='feature', y='gain', order=order, ax=ax, palette='viridis')\n",
        "    ax.tick_params(axis='x', rotation=90)\n",
        "    ax.grid()\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(os.path.join(OUTPUT_DIR, 'feature_importance_boxen.png'))"
      ],
      "id": "ZjAKzzXyGJKU"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "papermill": {
          "duration": 5.683926,
          "end_time": "2021-06-12T04:24:36.838503",
          "exception": false,
          "start_time": "2021-06-12T04:24:31.154577",
          "status": "completed"
        },
        "tags": [],
        "id": "mR9LEnknGJKW"
      },
      "source": [
        "# SHAP(kaggle環境では描画しない)\n",
        "# 参考その1: https://github.com/slundberg/shap/issues/337\n",
        "# 参考その2: https://github.com/slundberg/shap/issues/630\n",
        "import shap\n",
        "\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    pass\n",
        "else:\n",
        "    shap_values = []\n",
        "    for model_ in models:\n",
        "        explainer = shap.TreeExplainer(model_)\n",
        "        shap_values.append(explainer.shap_values(train_feat))\n",
        "\n",
        "    shap_mean = np.mean(shap_values, axis=0)"
      ],
      "id": "mR9LEnknGJKW"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "papermill": {
          "duration": 0.187683,
          "end_time": "2021-06-12T04:24:37.206699",
          "exception": false,
          "start_time": "2021-06-12T04:24:37.019016",
          "status": "completed"
        },
        "tags": [],
        "id": "VCfO7GwPGJKW"
      },
      "source": [
        "# SHAP_summary_plot\n",
        "# 参考_画像の出力について: https://github.com/slundberg/shap/issues/153\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    pass\n",
        "else:\n",
        "    shap.summary_plot(shap_mean, train_feat, show=False)\n",
        "    plt.subplots_adjust(left=0.4, right=1.0)  # 保存画像のラベルが欠けるのを防ぐ\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'shap_summary_plot.png'))"
      ],
      "id": "VCfO7GwPGJKW"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "papermill": {
          "duration": 0.175852,
          "end_time": "2021-06-12T04:24:37.559054",
          "exception": false,
          "start_time": "2021-06-12T04:24:37.383202",
          "status": "completed"
        },
        "tags": [],
        "id": "GCQVA49-GJKW"
      },
      "source": [
        "# SHAP_dependence_plot\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    pass\n",
        "else:\n",
        "    for col_ in train_feat.columns:\n",
        "        shap.dependence_plot(col_, shap_mean, train_feat)"
      ],
      "id": "GCQVA49-GJKW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.185561,
          "end_time": "2021-06-12T04:24:37.921192",
          "exception": false,
          "start_time": "2021-06-12T04:24:37.735631",
          "status": "completed"
        },
        "tags": [],
        "id": "LNX0XmiTGJKW"
      },
      "source": [
        "# 分布(train_vs_oof)\n",
        "if 'kaggle_web_client' in sys.modules:  # kaggle環境\n",
        "    pass\n",
        "else:\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    sns.distplot(train_target, label='Train', ax=ax, color='C1')\n",
        "    sns.distplot(oof_preds, label='Out Of Fold', ax=ax, color='C2')\n",
        "    ax.legend()\n",
        "    ax.grid()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'train_vs_oof.png'))"
      ],
      "id": "LNX0XmiTGJKW",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Z7nq6YFak4"
      },
      "source": [
        ""
      ],
      "id": "n6Z7nq6YFak4",
      "execution_count": 78,
      "outputs": []
    }
  ]
}